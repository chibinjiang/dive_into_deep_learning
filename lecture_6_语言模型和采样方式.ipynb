{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言模型\n",
    "### 1. n元语法\n",
    "- 一元: unigram\n",
    "- 二元: bigram\n",
    "- 三元: trigram\n",
    "- n决定了模型的 复杂度和 准确性\n",
    "\n",
    "### 2. n阶马尔可夫链\n",
    "\n",
    "### 3. 循环神经网络\n",
    "- 记录并使用上一个时间步的隐藏变量/状态, 预测下一个时间步的输出\n",
    "- 不含隐藏状态的 RNN:\n",
    "    - H = activate(X W_xh + b_h)\n",
    "    - O = activate(H W_ho + b_o)\n",
    "- 含隐藏状态的 RNN: \n",
    "    - 时间步 t 的隐藏变量  由 当前时间步的输入和 上一个时间步的隐藏变量共同决定\n",
    "    - H_t = activate(X_t W_xh + H_t-1 W_hh + b_h)\n",
    "    - O_t = H_t W_ho + b_o\n",
    "\n",
    "### 4. 基于字符级循环神经网络的语言模型\n",
    "- 目标: 使用RNN进行歌词创作\n",
    "- 问题建模: 如何使用循环神经网络基于当前和过去的字符来预测下一个字符\n",
    "- 准备数据集:\n",
    "    - 将文本拆分成 单个字\n",
    "    - 随机采样: \n",
    "    - 相邻采样: \n",
    "    - 将样本按vocabulary转为one hot变量\n",
    "- 训练:\n",
    "    - 对每个时间步的输出层输出使用softmax运算\n",
    "    - 使用交叉熵损失函数来计算它与标签的误差\n",
    "- 剪裁梯度:\n",
    "    - why: RNN中容易出现 梯度衰减或者爆炸\n",
    "    - how: 设置裁剪的阈值ø, 将所有梯度参数组成一个向量 g, g的L2范数不能超过ø: $$ min(\\frac {ø} {||g||}, 1) g $$\n",
    "- 模型评估: 困惑度, perplexity, 交叉熵的exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import zipfile\n",
    "import time, math\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet.gluon import loss as gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = mx.optimizer.create('adam', learning_rate=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = mx.optimizer.create('adam', learning_rate=0.1, wd=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam.wd_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, W_xh = nd.random.normal(shape=(3, 1)), nd.random.normal(shape=(1, 4))\n",
    "H, W_hh = nd.random.normal(shape=(3, 4)), nd.random.normal(shape=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 3.1951559  -7.0288424   6.2385654   3.5568771 ]\n",
       " [ 2.8098507  -1.8081223   0.6729959  -0.23211236]\n",
       " [-0.14438549 -2.5961137  -1.1423198  -4.142916  ]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.dot(X, W_xh) + nd.dot(H, W_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 3.1951556  -7.0288424   6.2385654   3.5568771 ]\n",
       " [ 2.8098505  -1.8081224   0.6729959  -0.23211236]\n",
       " [-0.14438546 -2.5961137  -1.1423199  -4.142916  ]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.dot(nd.concat(X, H, dim=1), nd.concat(W_xh, W_hh, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机\\n想要和你飞到宇宙去\\n想要和你融化在一起\\n融化在宇宙里\\n我每天每天每'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取周杰伦歌词\n",
    "with zipfile.ZipFile('DataResources/Chapter_6/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')\n",
    "corpus_chars[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本预处理\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63282"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2582"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立词库索引\n",
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: 想要有直升机 想要和你飞到宇宙去 想要和\n",
      "indices: [887, 307, 1453, 291, 556, 1364, 212, 887, 307, 1378, 375, 865, 2320, 370, 2094, 2178, 212, 887, 307, 1378]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "sample = corpus_indices[:20]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机采样\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    \"\"\"\n",
    "    随机采样: \n",
    "    1. 将 corpus_indices 分成 batch_size 份, 每份 num_steps 个 索引\n",
    "    2. 样本与标签错位:\n",
    "        ++++++++++++++++\n",
    "         ----------------\n",
    "    Sample mini-batches in a random order from sequential data.\n",
    "    :param batch_size, 小批量的样本数\n",
    "    :param num_steps, 每个样本包含的时间步数\n",
    "    \"\"\"\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps  # 为什么要减1: 因为输出的索引是相应输入的索引加1\n",
    "    epoch_size = num_examples // batch_size\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos : pos + num_steps]\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i : i + batch_size]\n",
    "        X = nd.array(\n",
    "            [_data(j * num_steps) for j in batch_indices], ctx=ctx)\n",
    "        Y = nd.array([_data(j * num_steps + 1) for j in batch_indices], ctx=ctx)  # 这里为啥加 1\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 X:  \n",
      "[[160. 161. 162. 163. 164. 165. 166. 167. 168. 169.]\n",
      " [250. 251. 252. 253. 254. 255. 256. 257. 258. 259.]\n",
      " [180. 181. 182. 183. 184. 185. 186. 187. 188. 189.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[161. 162. 163. 164. 165. 166. 167. 168. 169. 170.]\n",
      " [251. 252. 253. 254. 255. 256. 257. 258. 259. 260.]\n",
      " [181. 182. 183. 184. 185. 186. 187. 188. 189. 190.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  1 X:  \n",
      "[[190. 191. 192. 193. 194. 195. 196. 197. 198. 199.]\n",
      " [ 20.  21.  22.  23.  24.  25.  26.  27.  28.  29.]\n",
      " [210. 211. 212. 213. 214. 215. 216. 217. 218. 219.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[191. 192. 193. 194. 195. 196. 197. 198. 199. 200.]\n",
      " [ 21.  22.  23.  24.  25.  26.  27.  28.  29.  30.]\n",
      " [211. 212. 213. 214. 215. 216. 217. 218. 219. 220.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  2 X:  \n",
      "[[200. 201. 202. 203. 204. 205. 206. 207. 208. 209.]\n",
      " [130. 131. 132. 133. 134. 135. 136. 137. 138. 139.]\n",
      " [ 50.  51.  52.  53.  54.  55.  56.  57.  58.  59.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[201. 202. 203. 204. 205. 206. 207. 208. 209. 210.]\n",
      " [131. 132. 133. 134. 135. 136. 137. 138. 139. 140.]\n",
      " [ 51.  52.  53.  54.  55.  56.  57.  58.  59.  60.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  3 X:  \n",
      "[[30. 31. 32. 33. 34. 35. 36. 37. 38. 39.]\n",
      " [80. 81. 82. 83. 84. 85. 86. 87. 88. 89.]\n",
      " [60. 61. 62. 63. 64. 65. 66. 67. 68. 69.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[31. 32. 33. 34. 35. 36. 37. 38. 39. 40.]\n",
      " [81. 82. 83. 84. 85. 86. 87. 88. 89. 90.]\n",
      " [61. 62. 63. 64. 65. 66. 67. 68. 69. 70.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  4 X:  \n",
      "[[220. 221. 222. 223. 224. 225. 226. 227. 228. 229.]\n",
      " [260. 261. 262. 263. 264. 265. 266. 267. 268. 269.]\n",
      " [170. 171. 172. 173. 174. 175. 176. 177. 178. 179.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[221. 222. 223. 224. 225. 226. 227. 228. 229. 230.]\n",
      " [261. 262. 263. 264. 265. 266. 267. 268. 269. 270.]\n",
      " [171. 172. 173. 174. 175. 176. 177. 178. 179. 180.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  5 X:  \n",
      "[[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.]\n",
      " [230. 231. 232. 233. 234. 235. 236. 237. 238. 239.]\n",
      " [ 40.  41.  42.  43.  44.  45.  46.  47.  48.  49.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      " [231. 232. 233. 234. 235. 236. 237. 238. 239. 240.]\n",
      " [ 41.  42.  43.  44.  45.  46.  47.  48.  49.  50.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  6 X:  \n",
      "[[280. 281. 282. 283. 284. 285. 286. 287. 288. 289.]\n",
      " [140. 141. 142. 143. 144. 145. 146. 147. 148. 149.]\n",
      " [120. 121. 122. 123. 124. 125. 126. 127. 128. 129.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[281. 282. 283. 284. 285. 286. 287. 288. 289. 290.]\n",
      " [141. 142. 143. 144. 145. 146. 147. 148. 149. 150.]\n",
      " [121. 122. 123. 124. 125. 126. 127. 128. 129. 130.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  7 X:  \n",
      "[[270. 271. 272. 273. 274. 275. 276. 277. 278. 279.]\n",
      " [110. 111. 112. 113. 114. 115. 116. 117. 118. 119.]\n",
      " [ 90.  91.  92.  93.  94.  95.  96.  97.  98.  99.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[271. 272. 273. 274. 275. 276. 277. 278. 279. 280.]\n",
      " [111. 112. 113. 114. 115. 116. 117. 118. 119. 120.]\n",
      " [ 91.  92.  93.  94.  95.  96.  97.  98.  99. 100.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  8 X:  \n",
      "[[240. 241. 242. 243. 244. 245. 246. 247. 248. 249.]\n",
      " [100. 101. 102. 103. 104. 105. 106. 107. 108. 109.]\n",
      " [ 10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[241. 242. 243. 244. 245. 246. 247. 248. 249. 250.]\n",
      " [101. 102. 103. 104. 105. 106. 107. 108. 109. 110.]\n",
      " [ 11.  12.  13.  14.  15.  16.  17.  18.  19.  20.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(300))\n",
    "for epoch, (X, Y) in enumerate(data_iter_random(my_seq, batch_size=3, num_steps=10)):\n",
    "    print(\"Epoch: \", epoch, 'X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    \"\"\"\n",
    "    相邻采样: 相邻 epoch 的 batch_size 样本是相邻的\n",
    "    Sample mini-batches in a consecutive order from sequential data.\n",
    "    \n",
    "    \"\"\"\n",
    "    corpus_indices = nd.array(corpus_indices, ctx=ctx)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    print(batch_len)\n",
    "    indices = corpus_indices[0 : batch_size * batch_len].reshape((\n",
    "        batch_size, batch_len))  # 只要 前面的batch_size * batch_len 个 \n",
    "    print(indices)\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i : i + num_steps]\n",
    "        Y = indices[:, i + 1 : i + num_steps + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "\n",
      "[[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "   14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "   28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
      "   42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
      "   56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
      "   70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
      "   84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
      "   98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
      "  112. 113. 114. 115. 116. 117. 118. 119. 120.]\n",
      " [121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133. 134.\n",
      "  135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147. 148.\n",
      "  149. 150. 151. 152. 153. 154. 155. 156. 157. 158. 159. 160. 161. 162.\n",
      "  163. 164. 165. 166. 167. 168. 169. 170. 171. 172. 173. 174. 175. 176.\n",
      "  177. 178. 179. 180. 181. 182. 183. 184. 185. 186. 187. 188. 189. 190.\n",
      "  191. 192. 193. 194. 195. 196. 197. 198. 199. 200. 201. 202. 203. 204.\n",
      "  205. 206. 207. 208. 209. 210. 211. 212. 213. 214. 215. 216. 217. 218.\n",
      "  219. 220. 221. 222. 223. 224. 225. 226. 227. 228. 229. 230. 231. 232.\n",
      "  233. 234. 235. 236. 237. 238. 239. 240. 241.]\n",
      " [242. 243. 244. 245. 246. 247. 248. 249. 250. 251. 252. 253. 254. 255.\n",
      "  256. 257. 258. 259. 260. 261. 262. 263. 264. 265. 266. 267. 268. 269.\n",
      "  270. 271. 272. 273. 274. 275. 276. 277. 278. 279. 280. 281. 282. 283.\n",
      "  284. 285. 286. 287. 288. 289. 290. 291. 292. 293. 294. 295. 296. 297.\n",
      "  298. 299. 300. 301. 302. 303. 304. 305. 306. 307. 308. 309. 310. 311.\n",
      "  312. 313. 314. 315. 316. 317. 318. 319. 320. 321. 322. 323. 324. 325.\n",
      "  326. 327. 328. 329. 330. 331. 332. 333. 334. 335. 336. 337. 338. 339.\n",
      "  340. 341. 342. 343. 344. 345. 346. 347. 348. 349. 350. 351. 352. 353.\n",
      "  354. 355. 356. 357. 358. 359. 360. 361. 362.]]\n",
      "<NDArray 3x121 @cpu(0)>\n",
      "Epoch:  1 X:  \n",
      "[[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.]\n",
      " [121. 122. 123. 124. 125. 126. 127. 128. 129. 130.]\n",
      " [242. 243. 244. 245. 246. 247. 248. 249. 250. 251.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      " [122. 123. 124. 125. 126. 127. 128. 129. 130. 131.]\n",
      " [243. 244. 245. 246. 247. 248. 249. 250. 251. 252.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  2 X:  \n",
      "[[ 10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      " [131. 132. 133. 134. 135. 136. 137. 138. 139. 140.]\n",
      " [252. 253. 254. 255. 256. 257. 258. 259. 260. 261.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 11.  12.  13.  14.  15.  16.  17.  18.  19.  20.]\n",
      " [132. 133. 134. 135. 136. 137. 138. 139. 140. 141.]\n",
      " [253. 254. 255. 256. 257. 258. 259. 260. 261. 262.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  3 X:  \n",
      "[[ 20.  21.  22.  23.  24.  25.  26.  27.  28.  29.]\n",
      " [141. 142. 143. 144. 145. 146. 147. 148. 149. 150.]\n",
      " [262. 263. 264. 265. 266. 267. 268. 269. 270. 271.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 21.  22.  23.  24.  25.  26.  27.  28.  29.  30.]\n",
      " [142. 143. 144. 145. 146. 147. 148. 149. 150. 151.]\n",
      " [263. 264. 265. 266. 267. 268. 269. 270. 271. 272.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  4 X:  \n",
      "[[ 30.  31.  32.  33.  34.  35.  36.  37.  38.  39.]\n",
      " [151. 152. 153. 154. 155. 156. 157. 158. 159. 160.]\n",
      " [272. 273. 274. 275. 276. 277. 278. 279. 280. 281.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 31.  32.  33.  34.  35.  36.  37.  38.  39.  40.]\n",
      " [152. 153. 154. 155. 156. 157. 158. 159. 160. 161.]\n",
      " [273. 274. 275. 276. 277. 278. 279. 280. 281. 282.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  5 X:  \n",
      "[[ 40.  41.  42.  43.  44.  45.  46.  47.  48.  49.]\n",
      " [161. 162. 163. 164. 165. 166. 167. 168. 169. 170.]\n",
      " [282. 283. 284. 285. 286. 287. 288. 289. 290. 291.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 41.  42.  43.  44.  45.  46.  47.  48.  49.  50.]\n",
      " [162. 163. 164. 165. 166. 167. 168. 169. 170. 171.]\n",
      " [283. 284. 285. 286. 287. 288. 289. 290. 291. 292.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  6 X:  \n",
      "[[ 50.  51.  52.  53.  54.  55.  56.  57.  58.  59.]\n",
      " [171. 172. 173. 174. 175. 176. 177. 178. 179. 180.]\n",
      " [292. 293. 294. 295. 296. 297. 298. 299. 300. 301.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 51.  52.  53.  54.  55.  56.  57.  58.  59.  60.]\n",
      " [172. 173. 174. 175. 176. 177. 178. 179. 180. 181.]\n",
      " [293. 294. 295. 296. 297. 298. 299. 300. 301. 302.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  7 X:  \n",
      "[[ 60.  61.  62.  63.  64.  65.  66.  67.  68.  69.]\n",
      " [181. 182. 183. 184. 185. 186. 187. 188. 189. 190.]\n",
      " [302. 303. 304. 305. 306. 307. 308. 309. 310. 311.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 61.  62.  63.  64.  65.  66.  67.  68.  69.  70.]\n",
      " [182. 183. 184. 185. 186. 187. 188. 189. 190. 191.]\n",
      " [303. 304. 305. 306. 307. 308. 309. 310. 311. 312.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  8 X:  \n",
      "[[ 70.  71.  72.  73.  74.  75.  76.  77.  78.  79.]\n",
      " [191. 192. 193. 194. 195. 196. 197. 198. 199. 200.]\n",
      " [312. 313. 314. 315. 316. 317. 318. 319. 320. 321.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 71.  72.  73.  74.  75.  76.  77.  78.  79.  80.]\n",
      " [192. 193. 194. 195. 196. 197. 198. 199. 200. 201.]\n",
      " [313. 314. 315. 316. 317. 318. 319. 320. 321. 322.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  9 X:  \n",
      "[[ 80.  81.  82.  83.  84.  85.  86.  87.  88.  89.]\n",
      " [201. 202. 203. 204. 205. 206. 207. 208. 209. 210.]\n",
      " [322. 323. 324. 325. 326. 327. 328. 329. 330. 331.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 81.  82.  83.  84.  85.  86.  87.  88.  89.  90.]\n",
      " [202. 203. 204. 205. 206. 207. 208. 209. 210. 211.]\n",
      " [323. 324. 325. 326. 327. 328. 329. 330. 331. 332.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  10 X:  \n",
      "[[ 90.  91.  92.  93.  94.  95.  96.  97.  98.  99.]\n",
      " [211. 212. 213. 214. 215. 216. 217. 218. 219. 220.]\n",
      " [332. 333. 334. 335. 336. 337. 338. 339. 340. 341.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 91.  92.  93.  94.  95.  96.  97.  98.  99. 100.]\n",
      " [212. 213. 214. 215. 216. 217. 218. 219. 220. 221.]\n",
      " [333. 334. 335. 336. 337. 338. 339. 340. 341. 342.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  11 X:  \n",
      "[[100. 101. 102. 103. 104. 105. 106. 107. 108. 109.]\n",
      " [221. 222. 223. 224. 225. 226. 227. 228. 229. 230.]\n",
      " [342. 343. 344. 345. 346. 347. 348. 349. 350. 351.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[101. 102. 103. 104. 105. 106. 107. 108. 109. 110.]\n",
      " [222. 223. 224. 225. 226. 227. 228. 229. 230. 231.]\n",
      " [343. 344. 345. 346. 347. 348. 349. 350. 351. 352.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  12 X:  \n",
      "[[110. 111. 112. 113. 114. 115. 116. 117. 118. 119.]\n",
      " [231. 232. 233. 234. 235. 236. 237. 238. 239. 240.]\n",
      " [352. 353. 354. 355. 356. 357. 358. 359. 360. 361.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[111. 112. 113. 114. 115. 116. 117. 118. 119. 120.]\n",
      " [232. 233. 234. 235. 236. 237. 238. 239. 240. 241.]\n",
      " [353. 354. 355. 356. 357. 358. 359. 360. 361. 362.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(363))\n",
    "for epoch, (X, Y) in enumerate(data_iter_consecutive(my_seq, batch_size=3, num_steps=10)):\n",
    "    print(\"Epoch: \", epoch + 1, 'X: ', X, '\\nY:', Y, '\\n')\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 1. ... 0. 0. 0.]]\n",
       "<NDArray 2x2582 @cpu(0)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.one_hot(nd.array([0, 2]), vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将输入转为one-hot向量\n",
    "def to_onehot(X, size):\n",
    "    return [nd.one_hot(x, size) for x in X.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 1. 2. 3. 4.]\n",
       " [5. 6. 7. 8. 9.]]\n",
       "<NDArray 2x5 @cpu(0)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_X = nd.arange(10).reshape((2, 5))\n",
    "sample_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " [[1. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 1. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 1. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = to_onehot(X, vocab_size)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cpu(0)\n"
     ]
    }
   ],
   "source": [
    "# 初始化参数\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "ctx = mx.cpu(0)\n",
    "print('will use', ctx)\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return nd.random.normal(scale=0.01, shape=shape, ctx=ctx)\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = nd.zeros(num_hiddens, ctx=ctx)\n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = nd.zeros(num_outputs, ctx=ctx)\n",
    "    # 附上梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取初始化的隐藏状态\n",
    "def init_rnn_state(batch_size, num_hiddens, ctx):\n",
    "    return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)\n",
    "        Y = nd.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\n",
       " [10. 11. 12. 13. 14. 15. 16. 17. 18. 19.]]\n",
       "<NDArray 2x10 @cpu(0)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, (2, 2582), (2, 256))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = init_rnn_state(X.shape[0], num_hiddens, ctx)\n",
    "inputs = to_onehot(X.as_in_context(ctx), vocab_size)\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "len(outputs), outputs[0].shape, state_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n",
    "                num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx):\n",
    "    \"\"\"\n",
    "    :param prefix: 开头\n",
    "    :param num_chars: 生成几个字符\n",
    "    :param rnn: rnn 模型\n",
    "    :param params: 参数\n",
    "    :param init_rnn_state: 初始化隐藏状态的方法\n",
    "    :param num_hiddens: 隐藏层的单元数\n",
    "    :param vocab_size: 字符集的大小, 用于生成ohe-hot变量\n",
    "    :param idx_to_char: 字符id和 字符的匹配\n",
    "    :param idx_to_char: 字符和 字符id的匹配 \n",
    "    \"\"\"\n",
    "    state = init_rnn_state(1, num_hiddens, ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    print(output)\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        print(\"Output: \", output)\n",
    "        # 将上一时间步的输出作为当前时间步的输入\n",
    "        X = to_onehot(nd.array([output[-1]], ctx=ctx), vocab_size)\n",
    "        # 计算输出和更新隐藏状态\n",
    "        (Y, state) = rnn(X, state, params)\n",
    "\n",
    "        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y[0].argmax(axis=1).asscalar()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[563]\n",
      "Output:  [563]\n",
      "Output:  [563, 746]\n",
      "Output:  [563, 746, 224]\n",
      "Output:  [563, 746, 224, 2107]\n",
      "Output:  [563, 746, 224, 2107, 397]\n",
      "Output:  [563, 746, 224, 2107, 397, 1377]\n",
      "Output:  [563, 746, 224, 2107, 397, 1377, 261]\n",
      "Output:  [563, 746, 224, 2107, 397, 1377, 261, 1342]\n",
      "Output:  [563, 746, 224, 2107, 397, 1377, 261, 1342, 2549]\n",
      "Output:  [563, 746, 224, 2107, 397, 1377, 261, 1342, 2549, 161]\n",
      "Output:  [563, 746, 224, 2107, 397, 1377, 261, 1342, 2549, 161, 1224]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'分开建民假愛飛蛇见讓约夺'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n",
    "            ctx, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1804]\n",
      "Output:  [1804]\n",
      "Output:  [1804, 462]\n",
      "Output:  [1804, 462, 307]\n",
      "Output:  [1804, 462, 307, 1804]\n",
      "Output:  [1804, 462, 307, 1804, 462]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490, 1675]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490, 1675, 1400]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490, 1675, 1400, 1358]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490, 1675, 1400, 1358, 702]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490, 1675, 1400, 1358, 702, 1581]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490, 1675, 1400, 1358, 702, 1581, 2378]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我不要我不要帝星声攻雨趁後晓件赛'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn('我不要我不要', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n",
    "            ctx, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剪裁梯度\n",
    "def grad_clipping(params, theta, ctx):\n",
    "    norm = nd.array([0], ctx)\n",
    "    for param in params:\n",
    "        norm += (param.grad ** 2).sum()\n",
    "    norm = norm.sqrt().asscalar()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这种模型, 怎么评估好坏呢 ??\n",
    "# 困惑度(perplexity): 对交叉熵函数的结果做指数运算得到的值\n",
    "# 训练并预测\n",
    "def sgd(params, lr, batch_size):  \n",
    "    \"\"\"\n",
    "    定义优化算法\n",
    "    :param lr: scalar, learning rate\n",
    "    :param params: \n",
    "    :params batch_size: size of mini batch\n",
    "    \"\"\"\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size\n",
    "        \n",
    "        \n",
    "def train_and_predict_rnn(\n",
    "        rnn, get_params, init_rnn_state, num_hiddens, vocab_size, ctx, corpus_indices, \n",
    "        idx_to_char, char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "        lr, clipping_theta, batch_size, pred_period, pred_len, prefixes\n",
    "    ):\n",
    "    \"\"\"\n",
    "    :param rnn: 模型\n",
    "    :param get_params: 获取模型参数变量\n",
    "    :param init_rnn_state: 初始化rnn状态\n",
    "    :param num_hiddens: 隐藏层的单元数\n",
    "    :param vocab_size\n",
    "    :param corpus_indices\n",
    "    :param num_epochs\n",
    "    :param num_streps\n",
    "    :param lr:\n",
    "    :param cliping_theta: 裁剪梯度的阈值\n",
    "    :param pred_period: 预测的时机\n",
    "    :param pred_len: 往后预测的长度\n",
    "    :param prefixed: 前缀输入\n",
    "    \"\"\"\n",
    "    perplexity_hist = list()\n",
    "    data_iter_fn = data_iter_random if is_random_iter else data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        if not is_random_iter:  \n",
    "            # 如使用相邻采样，在epoch开始时初始化隐藏状态\n",
    "            state = init_rnn_state(batch_size, num_hiddens, ctx)\n",
    "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)\n",
    "        for X, Y in data_iter:\n",
    "            if is_random_iter: \n",
    "                # 如使用随机采样，在每个小批量更新前初始化隐藏状态\n",
    "                state = init_rnn_state(batch_size, num_hiddens, ctx)\n",
    "            else:  \n",
    "                # 否则需要使用detach函数从计算图分离隐藏状态\n",
    "                for s in state:\n",
    "                    s.detach()\n",
    "            with autograd.record():\n",
    "                inputs = to_onehot(X, vocab_size)\n",
    "                # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "                (outputs, state) = rnn(inputs, state, params)\n",
    "                # 拼接之后形状为(num_steps * batch_size, vocab_size)\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                # Y的形状是(batch_size, num_steps)，转置后再变成长度为\n",
    "                # batch * num_steps 的向量，这样跟输出的行一一对应\n",
    "                y = Y.T.reshape((-1,))\n",
    "                # 使用交叉熵损失计算平均分类误差\n",
    "                l = loss(outputs, y).mean()\n",
    "            l.backward()\n",
    "            grad_clipping(params, clipping_theta, ctx)  # 裁剪梯度\n",
    "            sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\n",
    "            l_sum += l.asscalar() * y.size\n",
    "            n += y.size\n",
    "        perplexity = math.exp(l_sum / n)\n",
    "        print('epoch %d, perplexity %f, time %.2f sec' % (epoch + 1, perplexity, time.time() - start))\n",
    "        perplexity_hist.append(perplexity)\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn(prefix, pred_len, rnn, params, init_rnn_state, num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))\n",
    "    return perplexity_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 100, 0.01\n",
    "pred_period, pred_len, prefixes = 50, 20, ['分开', '不分开']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, perplexity 859.002742, time 22.07 sec\n",
      "epoch 2, perplexity 512.869748, time 20.75 sec\n",
      "epoch 3, perplexity 461.401658, time 22.28 sec\n",
      "epoch 4, perplexity 430.564968, time 24.65 sec\n",
      "epoch 5, perplexity 406.764540, time 22.20 sec\n",
      "epoch 6, perplexity 385.915595, time 25.43 sec\n",
      "epoch 7, perplexity 364.178734, time 22.64 sec\n",
      "epoch 8, perplexity 343.745503, time 21.73 sec\n",
      "epoch 9, perplexity 322.004675, time 22.14 sec\n",
      "epoch 10, perplexity 301.724242, time 21.87 sec\n",
      "epoch 11, perplexity 284.203075, time 21.28 sec\n",
      "epoch 12, perplexity 268.290758, time 23.67 sec\n"
     ]
    }
   ],
   "source": [
    "# 随机采样\n",
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, ctx, corpus_indices, idx_to_char,\n",
    "                      char_to_idx, True, num_epochs, num_steps, lr,\n",
    "                      clipping_theta, batch_size, pred_period, pred_len,\n",
    "                      prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相邻采样\n",
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, ctx, corpus_indices, idx_to_char,\n",
    "                      char_to_idx, False, num_epochs, num_steps, lr,\n",
    "                      clipping_theta, batch_size, pred_period, pred_len,\n",
    "                      prefixes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
