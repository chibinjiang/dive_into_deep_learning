{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言模型\n",
    "### 1. n元语法\n",
    "- 一元: unigram\n",
    "- 二元: bigram\n",
    "- 三元: trigram\n",
    "- n决定了模型的 复杂度和 准确性\n",
    "\n",
    "### 2. n阶马尔可夫链\n",
    "\n",
    "### 3. 循环神经网络\n",
    "- 记录并使用上一个时间步的隐藏变量/状态, 预测下一个时间步的输出\n",
    "- 不含隐藏状态的 RNN:\n",
    "    - H = activate(X W_xh + b_h)\n",
    "    - O = activate(H W_ho + b_o)\n",
    "- 含隐藏状态的 RNN: \n",
    "    - 时间步 t 的隐藏变量  由 当前时间步的输入和 上一个时间步的隐藏变量共同决定\n",
    "    - H_t = activate(X_t W_xh + H_t-1 W_hh + b_h)\n",
    "    - O_t = H_t W_ho + b_o\n",
    "\n",
    "### 4. 基于字符级循环神经网络的语言模型\n",
    "- 目标: 使用RNN进行歌词创作\n",
    "- 问题建模: 如何使用循环神经网络基于当前和过去的字符来预测下一个字符\n",
    "- 准备数据集:\n",
    "    - 将文本拆分成 单个字\n",
    "    - 随机采样: \n",
    "    - 相邻采样: \n",
    "    - 将样本按vocabulary转为one hot变量\n",
    "- 训练:\n",
    "    - 对每个时间步的输出层输出使用softmax运算\n",
    "    - 使用交叉熵损失函数来计算它与标签的误差\n",
    "- 剪裁梯度:\n",
    "    - why: RNN中容易出现 梯度衰减或者爆炸\n",
    "    - how: 设置裁剪的阈值ø, 将所有梯度参数组成一个向量 g, g的L2范数不能超过ø: $$ min(\\frac {ø} {||g||}, 1) g $$\n",
    "- 模型评估: 困惑度, perplexity, 交叉熵的exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备 Google Colab 环境: 在Runtime中选择 GPU\n",
    "# 拉取数据集\n",
    "! git clone https://github.com/chibinjiang/dive_into_deep_learning.git\n",
    "# 进入到和开发环境相似的工作目录\n",
    "%cd dive_into_deep_learning/\n",
    "# 安装依赖\n",
    "! pip install mxnet-cu101mkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import zipfile\n",
    "import traceback\n",
    "import time, math\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet.gluon import loss as gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, W_xh = nd.random.normal(shape=(3, 1)), nd.random.normal(shape=(1, 4))\n",
    "H, W_hh = nd.random.normal(shape=(3, 4)), nd.random.normal(shape=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 3.1951559  -7.0288424   6.2385654   3.5568771 ]\n",
       " [ 2.8098507  -1.8081223   0.6729959  -0.23211236]\n",
       " [-0.14438549 -2.5961137  -1.1423198  -4.142916  ]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.dot(X, W_xh) + nd.dot(H, W_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 3.1951556  -7.0288424   6.2385654   3.5568771 ]\n",
       " [ 2.8098505  -1.8081224   0.6729959  -0.23211236]\n",
       " [-0.14438546 -2.5961137  -1.1423199  -4.142916  ]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.dot(nd.concat(X, H, dim=1), nd.concat(W_xh, W_hh, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机\\n想要和你飞到宇宙去\\n想要和你融化在一起\\n融化在宇宙里\\n我每天每天每'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取周杰伦歌词\n",
    "with zipfile.ZipFile('DataResources/Chapter_6/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')\n",
    "corpus_chars[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本预处理\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63282"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2582"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立词库索引\n",
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: 想要有直升机 想要和你飞到宇宙去 想要和\n",
      "indices: [887, 307, 1453, 291, 556, 1364, 212, 887, 307, 1378, 375, 865, 2320, 370, 2094, 2178, 212, 887, 307, 1378]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "sample = corpus_indices[:20]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机采样\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    \"\"\"\n",
    "    随机采样: \n",
    "    1. 将 corpus_indices 分成 batch_size 份, 每份 num_steps 个 索引\n",
    "    2. 样本与标签错位:\n",
    "        ++++++++++++++++\n",
    "         ----------------\n",
    "    Sample mini-batches in a random order from sequential data.\n",
    "    :param batch_size, 小批量的样本数\n",
    "    :param num_steps, 每个样本包含的时间步数\n",
    "    \"\"\"\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps  # 为什么要减1: 因为输出的索引是相应输入的索引加1\n",
    "    epoch_size = num_examples // batch_size\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos : pos + num_steps]\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i : i + batch_size]\n",
    "        X = nd.array(\n",
    "            [_data(j * num_steps) for j in batch_indices], ctx=ctx)\n",
    "        Y = nd.array([_data(j * num_steps + 1) for j in batch_indices], ctx=ctx)  # 这里为啥加 1\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 X:  \n",
      "[[160. 161. 162. 163. 164. 165. 166. 167. 168. 169.]\n",
      " [250. 251. 252. 253. 254. 255. 256. 257. 258. 259.]\n",
      " [180. 181. 182. 183. 184. 185. 186. 187. 188. 189.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[161. 162. 163. 164. 165. 166. 167. 168. 169. 170.]\n",
      " [251. 252. 253. 254. 255. 256. 257. 258. 259. 260.]\n",
      " [181. 182. 183. 184. 185. 186. 187. 188. 189. 190.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  1 X:  \n",
      "[[190. 191. 192. 193. 194. 195. 196. 197. 198. 199.]\n",
      " [ 20.  21.  22.  23.  24.  25.  26.  27.  28.  29.]\n",
      " [210. 211. 212. 213. 214. 215. 216. 217. 218. 219.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[191. 192. 193. 194. 195. 196. 197. 198. 199. 200.]\n",
      " [ 21.  22.  23.  24.  25.  26.  27.  28.  29.  30.]\n",
      " [211. 212. 213. 214. 215. 216. 217. 218. 219. 220.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  2 X:  \n",
      "[[200. 201. 202. 203. 204. 205. 206. 207. 208. 209.]\n",
      " [130. 131. 132. 133. 134. 135. 136. 137. 138. 139.]\n",
      " [ 50.  51.  52.  53.  54.  55.  56.  57.  58.  59.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[201. 202. 203. 204. 205. 206. 207. 208. 209. 210.]\n",
      " [131. 132. 133. 134. 135. 136. 137. 138. 139. 140.]\n",
      " [ 51.  52.  53.  54.  55.  56.  57.  58.  59.  60.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  3 X:  \n",
      "[[30. 31. 32. 33. 34. 35. 36. 37. 38. 39.]\n",
      " [80. 81. 82. 83. 84. 85. 86. 87. 88. 89.]\n",
      " [60. 61. 62. 63. 64. 65. 66. 67. 68. 69.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[31. 32. 33. 34. 35. 36. 37. 38. 39. 40.]\n",
      " [81. 82. 83. 84. 85. 86. 87. 88. 89. 90.]\n",
      " [61. 62. 63. 64. 65. 66. 67. 68. 69. 70.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  4 X:  \n",
      "[[220. 221. 222. 223. 224. 225. 226. 227. 228. 229.]\n",
      " [260. 261. 262. 263. 264. 265. 266. 267. 268. 269.]\n",
      " [170. 171. 172. 173. 174. 175. 176. 177. 178. 179.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[221. 222. 223. 224. 225. 226. 227. 228. 229. 230.]\n",
      " [261. 262. 263. 264. 265. 266. 267. 268. 269. 270.]\n",
      " [171. 172. 173. 174. 175. 176. 177. 178. 179. 180.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  5 X:  \n",
      "[[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.]\n",
      " [230. 231. 232. 233. 234. 235. 236. 237. 238. 239.]\n",
      " [ 40.  41.  42.  43.  44.  45.  46.  47.  48.  49.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      " [231. 232. 233. 234. 235. 236. 237. 238. 239. 240.]\n",
      " [ 41.  42.  43.  44.  45.  46.  47.  48.  49.  50.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  6 X:  \n",
      "[[280. 281. 282. 283. 284. 285. 286. 287. 288. 289.]\n",
      " [140. 141. 142. 143. 144. 145. 146. 147. 148. 149.]\n",
      " [120. 121. 122. 123. 124. 125. 126. 127. 128. 129.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[281. 282. 283. 284. 285. 286. 287. 288. 289. 290.]\n",
      " [141. 142. 143. 144. 145. 146. 147. 148. 149. 150.]\n",
      " [121. 122. 123. 124. 125. 126. 127. 128. 129. 130.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  7 X:  \n",
      "[[270. 271. 272. 273. 274. 275. 276. 277. 278. 279.]\n",
      " [110. 111. 112. 113. 114. 115. 116. 117. 118. 119.]\n",
      " [ 90.  91.  92.  93.  94.  95.  96.  97.  98.  99.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[271. 272. 273. 274. 275. 276. 277. 278. 279. 280.]\n",
      " [111. 112. 113. 114. 115. 116. 117. 118. 119. 120.]\n",
      " [ 91.  92.  93.  94.  95.  96.  97.  98.  99. 100.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "Epoch:  8 X:  \n",
      "[[240. 241. 242. 243. 244. 245. 246. 247. 248. 249.]\n",
      " [100. 101. 102. 103. 104. 105. 106. 107. 108. 109.]\n",
      " [ 10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[241. 242. 243. 244. 245. 246. 247. 248. 249. 250.]\n",
      " [101. 102. 103. 104. 105. 106. 107. 108. 109. 110.]\n",
      " [ 11.  12.  13.  14.  15.  16.  17.  18.  19.  20.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(300))\n",
    "for epoch, (X, Y) in enumerate(data_iter_random(my_seq, batch_size=3, num_steps=10)):\n",
    "    print(\"Epoch: \", epoch, 'X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    \"\"\"\n",
    "    相邻采样: 相邻 epoch 的 batch_size 样本是相邻的\n",
    "    Sample mini-batches in a consecutive order from sequential data.\n",
    "    \n",
    "    \"\"\"\n",
    "    corpus_indices = nd.array(corpus_indices, ctx=ctx)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    indices = corpus_indices[0 : batch_size * batch_len].reshape((\n",
    "        batch_size, batch_len))  # 只要 前面的batch_size * batch_len 个 \n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i : i + num_steps]\n",
    "        Y = indices[:, i + 1 : i + num_steps + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "\n",
      "[[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "   14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "   28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
      "   42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
      "   56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
      "   70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
      "   84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
      "   98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
      "  112. 113. 114. 115. 116. 117. 118. 119. 120.]\n",
      " [121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133. 134.\n",
      "  135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147. 148.\n",
      "  149. 150. 151. 152. 153. 154. 155. 156. 157. 158. 159. 160. 161. 162.\n",
      "  163. 164. 165. 166. 167. 168. 169. 170. 171. 172. 173. 174. 175. 176.\n",
      "  177. 178. 179. 180. 181. 182. 183. 184. 185. 186. 187. 188. 189. 190.\n",
      "  191. 192. 193. 194. 195. 196. 197. 198. 199. 200. 201. 202. 203. 204.\n",
      "  205. 206. 207. 208. 209. 210. 211. 212. 213. 214. 215. 216. 217. 218.\n",
      "  219. 220. 221. 222. 223. 224. 225. 226. 227. 228. 229. 230. 231. 232.\n",
      "  233. 234. 235. 236. 237. 238. 239. 240. 241.]\n",
      " [242. 243. 244. 245. 246. 247. 248. 249. 250. 251. 252. 253. 254. 255.\n",
      "  256. 257. 258. 259. 260. 261. 262. 263. 264. 265. 266. 267. 268. 269.\n",
      "  270. 271. 272. 273. 274. 275. 276. 277. 278. 279. 280. 281. 282. 283.\n",
      "  284. 285. 286. 287. 288. 289. 290. 291. 292. 293. 294. 295. 296. 297.\n",
      "  298. 299. 300. 301. 302. 303. 304. 305. 306. 307. 308. 309. 310. 311.\n",
      "  312. 313. 314. 315. 316. 317. 318. 319. 320. 321. 322. 323. 324. 325.\n",
      "  326. 327. 328. 329. 330. 331. 332. 333. 334. 335. 336. 337. 338. 339.\n",
      "  340. 341. 342. 343. 344. 345. 346. 347. 348. 349. 350. 351. 352. 353.\n",
      "  354. 355. 356. 357. 358. 359. 360. 361. 362.]]\n",
      "<NDArray 3x121 @cpu(0)>\n",
      "Epoch:  1 X:  \n",
      "[[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.]\n",
      " [121. 122. 123. 124. 125. 126. 127. 128. 129. 130.]\n",
      " [242. 243. 244. 245. 246. 247. 248. 249. 250. 251.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      " [122. 123. 124. 125. 126. 127. 128. 129. 130. 131.]\n",
      " [243. 244. 245. 246. 247. 248. 249. 250. 251. 252.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  2 X:  \n",
      "[[ 10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      " [131. 132. 133. 134. 135. 136. 137. 138. 139. 140.]\n",
      " [252. 253. 254. 255. 256. 257. 258. 259. 260. 261.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 11.  12.  13.  14.  15.  16.  17.  18.  19.  20.]\n",
      " [132. 133. 134. 135. 136. 137. 138. 139. 140. 141.]\n",
      " [253. 254. 255. 256. 257. 258. 259. 260. 261. 262.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  3 X:  \n",
      "[[ 20.  21.  22.  23.  24.  25.  26.  27.  28.  29.]\n",
      " [141. 142. 143. 144. 145. 146. 147. 148. 149. 150.]\n",
      " [262. 263. 264. 265. 266. 267. 268. 269. 270. 271.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 21.  22.  23.  24.  25.  26.  27.  28.  29.  30.]\n",
      " [142. 143. 144. 145. 146. 147. 148. 149. 150. 151.]\n",
      " [263. 264. 265. 266. 267. 268. 269. 270. 271. 272.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  4 X:  \n",
      "[[ 30.  31.  32.  33.  34.  35.  36.  37.  38.  39.]\n",
      " [151. 152. 153. 154. 155. 156. 157. 158. 159. 160.]\n",
      " [272. 273. 274. 275. 276. 277. 278. 279. 280. 281.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 31.  32.  33.  34.  35.  36.  37.  38.  39.  40.]\n",
      " [152. 153. 154. 155. 156. 157. 158. 159. 160. 161.]\n",
      " [273. 274. 275. 276. 277. 278. 279. 280. 281. 282.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  5 X:  \n",
      "[[ 40.  41.  42.  43.  44.  45.  46.  47.  48.  49.]\n",
      " [161. 162. 163. 164. 165. 166. 167. 168. 169. 170.]\n",
      " [282. 283. 284. 285. 286. 287. 288. 289. 290. 291.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 41.  42.  43.  44.  45.  46.  47.  48.  49.  50.]\n",
      " [162. 163. 164. 165. 166. 167. 168. 169. 170. 171.]\n",
      " [283. 284. 285. 286. 287. 288. 289. 290. 291. 292.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  6 X:  \n",
      "[[ 50.  51.  52.  53.  54.  55.  56.  57.  58.  59.]\n",
      " [171. 172. 173. 174. 175. 176. 177. 178. 179. 180.]\n",
      " [292. 293. 294. 295. 296. 297. 298. 299. 300. 301.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 51.  52.  53.  54.  55.  56.  57.  58.  59.  60.]\n",
      " [172. 173. 174. 175. 176. 177. 178. 179. 180. 181.]\n",
      " [293. 294. 295. 296. 297. 298. 299. 300. 301. 302.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  7 X:  \n",
      "[[ 60.  61.  62.  63.  64.  65.  66.  67.  68.  69.]\n",
      " [181. 182. 183. 184. 185. 186. 187. 188. 189. 190.]\n",
      " [302. 303. 304. 305. 306. 307. 308. 309. 310. 311.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 61.  62.  63.  64.  65.  66.  67.  68.  69.  70.]\n",
      " [182. 183. 184. 185. 186. 187. 188. 189. 190. 191.]\n",
      " [303. 304. 305. 306. 307. 308. 309. 310. 311. 312.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  8 X:  \n",
      "[[ 70.  71.  72.  73.  74.  75.  76.  77.  78.  79.]\n",
      " [191. 192. 193. 194. 195. 196. 197. 198. 199. 200.]\n",
      " [312. 313. 314. 315. 316. 317. 318. 319. 320. 321.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 71.  72.  73.  74.  75.  76.  77.  78.  79.  80.]\n",
      " [192. 193. 194. 195. 196. 197. 198. 199. 200. 201.]\n",
      " [313. 314. 315. 316. 317. 318. 319. 320. 321. 322.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  9 X:  \n",
      "[[ 80.  81.  82.  83.  84.  85.  86.  87.  88.  89.]\n",
      " [201. 202. 203. 204. 205. 206. 207. 208. 209. 210.]\n",
      " [322. 323. 324. 325. 326. 327. 328. 329. 330. 331.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 81.  82.  83.  84.  85.  86.  87.  88.  89.  90.]\n",
      " [202. 203. 204. 205. 206. 207. 208. 209. 210. 211.]\n",
      " [323. 324. 325. 326. 327. 328. 329. 330. 331. 332.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  10 X:  \n",
      "[[ 90.  91.  92.  93.  94.  95.  96.  97.  98.  99.]\n",
      " [211. 212. 213. 214. 215. 216. 217. 218. 219. 220.]\n",
      " [332. 333. 334. 335. 336. 337. 338. 339. 340. 341.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[ 91.  92.  93.  94.  95.  96.  97.  98.  99. 100.]\n",
      " [212. 213. 214. 215. 216. 217. 218. 219. 220. 221.]\n",
      " [333. 334. 335. 336. 337. 338. 339. 340. 341. 342.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  11 X:  \n",
      "[[100. 101. 102. 103. 104. 105. 106. 107. 108. 109.]\n",
      " [221. 222. 223. 224. 225. 226. 227. 228. 229. 230.]\n",
      " [342. 343. 344. 345. 346. 347. 348. 349. 350. 351.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[101. 102. 103. 104. 105. 106. 107. 108. 109. 110.]\n",
      " [222. 223. 224. 225. 226. 227. 228. 229. 230. 231.]\n",
      " [343. 344. 345. 346. 347. 348. 349. 350. 351. 352.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n",
      "Epoch:  12 X:  \n",
      "[[110. 111. 112. 113. 114. 115. 116. 117. 118. 119.]\n",
      " [231. 232. 233. 234. 235. 236. 237. 238. 239. 240.]\n",
      " [352. 353. 354. 355. 356. 357. 358. 359. 360. 361.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "Y: \n",
      "[[111. 112. 113. 114. 115. 116. 117. 118. 119. 120.]\n",
      " [232. 233. 234. 235. 236. 237. 238. 239. 240. 241.]\n",
      " [353. 354. 355. 356. 357. 358. 359. 360. 361. 362.]]\n",
      "<NDArray 3x10 @cpu(0)> \n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(363))\n",
    "for epoch, (X, Y) in enumerate(data_iter_consecutive(my_seq, batch_size=3, num_steps=10)):\n",
    "    print(\"Epoch: \", epoch + 1, 'X: ', X, '\\nY:', Y, '\\n')\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 1. ... 0. 0. 0.]]\n",
       "<NDArray 2x2582 @cpu(0)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.one_hot(nd.array([0, 2]), vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将输入转为one-hot向量\n",
    "def to_onehot(X, size):\n",
    "    return [nd.one_hot(x, size) for x in X.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 1. 2. 3. 4.]\n",
       " [5. 6. 7. 8. 9.]]\n",
       "<NDArray 2x5 @cpu(0)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_X = nd.arange(10).reshape((2, 5))\n",
    "sample_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " [[1. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 1. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 1. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x2582 @cpu(0)>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = to_onehot(X, vocab_size)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cpu(0)\n"
     ]
    }
   ],
   "source": [
    "# 初始化参数\n",
    "def try_gpu(gpu_number=0):\n",
    "    \"\"\"\n",
    "        Return gpu(i) if exists, otherwise return cpu().\n",
    "    \"\"\"\n",
    "    try:\n",
    "        _ = mx.nd.array([1, 2, 3], ctx=mx.gpu(gpu_number))\n",
    "        print(\"Try GPU: {}\".format(gpu_number))\n",
    "    except mx.MXNetError:\n",
    "        traceback.print_exc()\n",
    "        print(\"Try CPU: {}\".format(gpu_number))\n",
    "        return mx.cpu()\n",
    "    return mx.gpu(gpu_number)\n",
    "\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "ctx = try_gpu()\n",
    "print('will use', ctx)\n",
    "\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return nd.random.normal(scale=0.01, shape=shape, ctx=ctx)\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = nd.zeros(num_hiddens, ctx=ctx)\n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = nd.zeros(num_outputs, ctx=ctx)\n",
    "    # 附上梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取初始化的隐藏状态\n",
    "def init_rnn_state(batch_size, num_hiddens, ctx):\n",
    "    return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)\n",
    "        Y = nd.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\n",
       " [10. 11. 12. 13. 14. 15. 16. 17. 18. 19.]]\n",
       "<NDArray 2x10 @cpu(0)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, (2, 2582), (2, 256))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = init_rnn_state(X.shape[0], num_hiddens, ctx)\n",
    "inputs = to_onehot(X.as_in_context(ctx), vocab_size)\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "len(outputs), outputs[0].shape, state_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n",
    "                num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx):\n",
    "    \"\"\"\n",
    "    :param prefix: 开头\n",
    "    :param num_chars: 生成几个字符\n",
    "    :param rnn: rnn 模型\n",
    "    :param params: 参数\n",
    "    :param init_rnn_state: 初始化隐藏状态的方法\n",
    "    :param num_hiddens: 隐藏层的单元数\n",
    "    :param vocab_size: 字符集的大小, 用于生成ohe-hot变量\n",
    "    :param idx_to_char: 字符id和 字符的匹配\n",
    "    :param idx_to_char: 字符和 字符id的匹配 \n",
    "    \"\"\"\n",
    "    state = init_rnn_state(1, num_hiddens, ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        # 将上一时间步的输出作为当前时间步的输入\n",
    "        X = to_onehot(nd.array([output[-1]], ctx=ctx), vocab_size)\n",
    "        # 计算输出和更新隐藏状态\n",
    "        (Y, state) = rnn(X, state, params)\n",
    "\n",
    "        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y[0].argmax(axis=1).asscalar()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[563]\n",
      "Output:  [563]\n",
      "Output:  [563, 746]\n",
      "Output:  [563, 746, 224]\n",
      "Output:  [563, 746, 224, 2107]\n",
      "Output:  [563, 746, 224, 2107, 397]\n",
      "Output:  [563, 746, 224, 2107, 397, 1377]\n",
      "Output:  [563, 746, 224, 2107, 397, 1377, 261]\n",
      "Output:  [563, 746, 224, 2107, 397, 1377, 261, 1342]\n",
      "Output:  [563, 746, 224, 2107, 397, 1377, 261, 1342, 2549]\n",
      "Output:  [563, 746, 224, 2107, 397, 1377, 261, 1342, 2549, 161]\n",
      "Output:  [563, 746, 224, 2107, 397, 1377, 261, 1342, 2549, 161, 1224]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'分开建民假愛飛蛇见讓约夺'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n",
    "            ctx, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1804]\n",
      "Output:  [1804]\n",
      "Output:  [1804, 462]\n",
      "Output:  [1804, 462, 307]\n",
      "Output:  [1804, 462, 307, 1804]\n",
      "Output:  [1804, 462, 307, 1804, 462]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490, 1675]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490, 1675, 1400]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490, 1675, 1400, 1358]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490, 1675, 1400, 1358, 702]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490, 1675, 1400, 1358, 702, 1581]\n",
      "Output:  [1804, 462, 307, 1804, 462, 307, 2339, 1688, 2490, 1675, 1400, 1358, 702, 1581, 2378]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我不要我不要帝星声攻雨趁後晓件赛'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn('我不要我不要', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n",
    "            ctx, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剪裁梯度\n",
    "def grad_clipping(params, theta, ctx):\n",
    "    norm = nd.array([0], ctx)\n",
    "    for param in params:\n",
    "        norm += (param.grad ** 2).sum()\n",
    "    norm = norm.sqrt().asscalar()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这种模型, 怎么评估好坏呢 ??\n",
    "# 困惑度(perplexity): 对交叉熵函数的结果做指数运算得到的值\n",
    "# 训练并预测\n",
    "def sgd(params, lr, batch_size):  \n",
    "    \"\"\"\n",
    "    定义优化算法\n",
    "    :param lr: scalar, learning rate\n",
    "    :param params: \n",
    "    :params batch_size: size of mini batch\n",
    "    \"\"\"\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size\n",
    "        \n",
    "        \n",
    "def train_and_predict_rnn(\n",
    "        rnn, get_params, init_rnn_state, num_hiddens, vocab_size, ctx, corpus_indices, \n",
    "        idx_to_char, char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "        lr, clipping_theta, batch_size, pred_period, pred_len, prefixes\n",
    "    ):\n",
    "    \"\"\"\n",
    "    :param rnn: 模型\n",
    "    :param get_params: 获取模型参数变量\n",
    "    :param init_rnn_state: 初始化rnn状态\n",
    "    :param num_hiddens: 隐藏层的单元数\n",
    "    :param vocab_size\n",
    "    :param corpus_indices\n",
    "    :param num_epochs\n",
    "    :param num_streps\n",
    "    :param lr:\n",
    "    :param cliping_theta: 裁剪梯度的阈值\n",
    "    :param pred_period: 预测的时机\n",
    "    :param pred_len: 往后预测的长度\n",
    "    :param prefixed: 前缀输入\n",
    "    \"\"\"\n",
    "    perplexity_hist = list()\n",
    "    data_iter_fn = data_iter_random if is_random_iter else data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        if not is_random_iter:  \n",
    "            # 如使用相邻采样，在epoch开始时初始化隐藏状态\n",
    "            state = init_rnn_state(batch_size, num_hiddens, ctx)\n",
    "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)\n",
    "        for X, Y in data_iter:\n",
    "            if is_random_iter: \n",
    "                # 如使用随机采样，在每个小批量更新前初始化隐藏状态\n",
    "                state = init_rnn_state(batch_size, num_hiddens, ctx)\n",
    "            else:  \n",
    "                # 否则需要使用detach函数从计算图分离隐藏状态\n",
    "                for s in state:\n",
    "                    s.detach()\n",
    "            with autograd.record():\n",
    "                inputs = to_onehot(X, vocab_size)\n",
    "                # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "                (outputs, state) = rnn(inputs, state, params)\n",
    "                # 拼接之后形状为(num_steps * batch_size, vocab_size)\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                # Y的形状是(batch_size, num_steps)，转置后再变成长度为\n",
    "                # batch * num_steps 的向量，这样跟输出的行一一对应\n",
    "                y = Y.T.reshape((-1,))\n",
    "                # 使用交叉熵损失计算平均分类误差\n",
    "                l = loss(outputs, y).mean()\n",
    "            l.backward()\n",
    "            grad_clipping(params, clipping_theta, ctx)  # 裁剪梯度\n",
    "            sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\n",
    "            l_sum += l.asscalar() * y.size\n",
    "            n += y.size\n",
    "        perplexity = math.exp(l_sum / n)\n",
    "        print('epoch %d, perplexity %f, time %.2f sec' % (epoch + 1, perplexity, time.time() - start))\n",
    "        perplexity_hist.append(perplexity)\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn(prefix, pred_len, rnn, params, init_rnn_state, num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))\n",
    "    return perplexity_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 300, 50, 32, 100, 0.01\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开', '我静静地']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, perplexity 859.002742, time 22.07 sec\n",
      "epoch 2, perplexity 512.869748, time 20.75 sec\n",
      "epoch 3, perplexity 461.401658, time 22.28 sec\n",
      "epoch 4, perplexity 430.564968, time 24.65 sec\n",
      "epoch 5, perplexity 406.764540, time 22.20 sec\n",
      "epoch 6, perplexity 385.915595, time 25.43 sec\n",
      "epoch 7, perplexity 364.178734, time 22.64 sec\n",
      "epoch 8, perplexity 343.745503, time 21.73 sec\n",
      "epoch 9, perplexity 322.004675, time 22.14 sec\n",
      "epoch 10, perplexity 301.724242, time 21.87 sec\n",
      "epoch 11, perplexity 284.203075, time 21.28 sec\n",
      "epoch 12, perplexity 268.290758, time 23.67 sec\n",
      "epoch 13, perplexity 251.984735, time 21.79 sec\n",
      "epoch 14, perplexity 237.229979, time 21.98 sec\n",
      "epoch 15, perplexity 225.284144, time 21.21 sec\n",
      "epoch 16, perplexity 212.430445, time 22.82 sec\n",
      "epoch 17, perplexity 202.587929, time 27.30 sec\n",
      "epoch 18, perplexity 190.318045, time 21.82 sec\n",
      "epoch 19, perplexity 181.743100, time 22.51 sec\n",
      "epoch 20, perplexity 171.356619, time 23.20 sec\n",
      "epoch 21, perplexity 162.569793, time 22.18 sec\n",
      "epoch 22, perplexity 154.174038, time 25.94 sec\n",
      "epoch 23, perplexity 145.504197, time 22.69 sec\n",
      "epoch 24, perplexity 138.086992, time 23.73 sec\n",
      "epoch 25, perplexity 130.169149, time 23.35 sec\n",
      "epoch 26, perplexity 123.528149, time 25.35 sec\n",
      "epoch 27, perplexity 116.561717, time 22.20 sec\n",
      "epoch 28, perplexity 110.798553, time 21.34 sec\n",
      "epoch 29, perplexity 104.388070, time 22.63 sec\n",
      "epoch 30, perplexity 98.525000, time 23.61 sec\n",
      "epoch 31, perplexity 93.570467, time 22.09 sec\n",
      "epoch 32, perplexity 88.869091, time 22.36 sec\n",
      "epoch 33, perplexity 84.243959, time 24.72 sec\n",
      "epoch 34, perplexity 80.166715, time 22.26 sec\n",
      "epoch 35, perplexity 75.360276, time 21.69 sec\n",
      "epoch 36, perplexity 71.914566, time 21.27 sec\n",
      "epoch 37, perplexity 68.750983, time 21.28 sec\n",
      "epoch 38, perplexity 65.583859, time 22.61 sec\n",
      "epoch 39, perplexity 62.991997, time 21.18 sec\n",
      "epoch 40, perplexity 59.923220, time 21.64 sec\n",
      "epoch 41, perplexity 57.499057, time 21.75 sec\n",
      "epoch 42, perplexity 54.972602, time 21.99 sec\n",
      "epoch 43, perplexity 52.515460, time 22.92 sec\n",
      "epoch 44, perplexity 50.336256, time 22.10 sec\n",
      "epoch 45, perplexity 48.294130, time 22.80 sec\n",
      "epoch 46, perplexity 46.073719, time 23.21 sec\n",
      "epoch 47, perplexity 44.599892, time 21.99 sec\n",
      "epoch 48, perplexity 42.988284, time 21.87 sec\n",
      "epoch 49, perplexity 41.381844, time 22.18 sec\n",
      "epoch 50, perplexity 40.064879, time 22.41 sec\n",
      "[563]\n",
      "Output:  [563]\n",
      "Output:  [563, 746]\n",
      "Output:  [563, 746, 212]\n",
      "Output:  [563, 746, 212, 2083]\n",
      "Output:  [563, 746, 212, 2083, 33]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746, 1804]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746, 1804, 212]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746, 1804, 212, 212]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746, 1804, 212, 212, 375]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746, 1804, 212, 212, 375, 2467]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746, 1804, 212, 212, 375, 2467, 212]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746, 1804, 212, 212, 375, 2467, 212, 462]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746, 1804, 212, 212, 375, 2467, 212, 462, 1307]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746, 1804, 212, 212, 375, 2467, 212, 462, 1307, 212]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746, 1804, 212, 212, 375, 2467, 212, 462, 1307, 212, 212]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746, 1804, 212, 212, 375, 2467, 212, 462, 1307, 212, 212, 212]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746, 1804, 212, 212, 375, 2467, 212, 462, 1307, 212, 212, 212, 212]\n",
      "Output:  [563, 746, 212, 2083, 33, 1565, 869, 746, 1804, 212, 212, 375, 2467, 212, 462, 1307, 212, 212, 212, 212, 212]\n",
      " - 分开 为什么停开我  你说 不多      \n",
      "[462]\n",
      "Output:  [462]\n",
      "Output:  [462, 563]\n",
      "Output:  [462, 563, 746]\n",
      "Output:  [462, 563, 746, 212]\n",
      "Output:  [462, 563, 746, 212, 2125]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212, 1730]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212, 1730, 1622]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212, 1730, 1622, 2423]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212, 1730, 1622, 2423, 1740]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212, 1730, 1622, 2423, 1740, 212]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212, 1730, 1622, 2423, 1740, 212, 1804]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212, 1730, 1622, 2423, 1740, 212, 1804, 2480]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212, 1730, 1622, 2423, 1740, 212, 1804, 2480, 228]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212, 1730, 1622, 2423, 1740, 212, 1804, 2480, 228, 1617]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212, 1730, 1622, 2423, 1740, 212, 1804, 2480, 228, 1617, 212]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212, 1730, 1622, 2423, 1740, 212, 1804, 2480, 228, 1617, 212, 936]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212, 1730, 1622, 2423, 1740, 212, 1804, 2480, 228, 1617, 212, 936, 1549]\n",
      "Output:  [462, 563, 746, 212, 2125, 1753, 1453, 1386, 212, 1730, 1622, 2423, 1740, 212, 1804, 2480, 228, 1617, 212, 936, 1549, 936]\n",
      " - 不分开 也没有理 只能回忆 我们都无 一生一行\n",
      "epoch 51, perplexity 38.861432, time 21.09 sec\n",
      "epoch 52, perplexity 37.274983, time 22.68 sec\n",
      "epoch 53, perplexity 36.531774, time 21.66 sec\n",
      "epoch 54, perplexity 35.471144, time 22.80 sec\n",
      "epoch 55, perplexity 34.280594, time 22.23 sec\n",
      "epoch 56, perplexity 33.351706, time 23.74 sec\n",
      "epoch 57, perplexity 32.326877, time 21.80 sec\n",
      "epoch 58, perplexity 31.445691, time 20.85 sec\n",
      "epoch 59, perplexity 30.659224, time 21.64 sec\n",
      "epoch 60, perplexity 29.980278, time 22.14 sec\n",
      "epoch 61, perplexity 29.123541, time 22.84 sec\n",
      "epoch 62, perplexity 28.438842, time 21.88 sec\n",
      "epoch 63, perplexity 27.550862, time 22.95 sec\n",
      "epoch 64, perplexity 26.921332, time 22.20 sec\n",
      "epoch 65, perplexity 26.442106, time 21.81 sec\n",
      "epoch 66, perplexity 25.958904, time 23.38 sec\n",
      "epoch 67, perplexity 25.265796, time 24.53 sec\n",
      "epoch 68, perplexity 24.858271, time 22.88 sec\n",
      "epoch 69, perplexity 24.212892, time 22.20 sec\n",
      "epoch 70, perplexity 23.977566, time 22.17 sec\n",
      "epoch 71, perplexity 23.485184, time 22.63 sec\n",
      "epoch 72, perplexity 22.813388, time 22.49 sec\n",
      "epoch 73, perplexity 22.332012, time 22.54 sec\n",
      "epoch 74, perplexity 22.360827, time 21.89 sec\n",
      "epoch 75, perplexity 21.823130, time 23.16 sec\n",
      "epoch 76, perplexity 21.585805, time 23.74 sec\n",
      "epoch 77, perplexity 20.989314, time 23.43 sec\n",
      "epoch 78, perplexity 20.600502, time 23.13 sec\n",
      "epoch 79, perplexity 20.311441, time 24.04 sec\n",
      "epoch 80, perplexity 20.049993, time 22.71 sec\n",
      "epoch 81, perplexity 19.677232, time 24.00 sec\n",
      "epoch 82, perplexity 19.475646, time 22.34 sec\n",
      "epoch 83, perplexity 19.069171, time 21.57 sec\n",
      "epoch 84, perplexity 18.666661, time 22.11 sec\n",
      "epoch 85, perplexity 18.586972, time 23.70 sec\n",
      "epoch 86, perplexity 18.224874, time 21.78 sec\n",
      "epoch 87, perplexity 18.197313, time 23.94 sec\n",
      "epoch 88, perplexity 17.778614, time 21.80 sec\n",
      "epoch 89, perplexity 17.488861, time 21.92 sec\n",
      "epoch 90, perplexity 17.406819, time 21.57 sec\n",
      "epoch 91, perplexity 17.218524, time 22.57 sec\n",
      "epoch 92, perplexity 16.845870, time 23.46 sec\n",
      "epoch 93, perplexity 16.538065, time 21.80 sec\n",
      "epoch 94, perplexity 16.456744, time 22.66 sec\n",
      "epoch 95, perplexity 16.343119, time 21.63 sec\n",
      "epoch 96, perplexity 16.373744, time 21.12 sec\n",
      "epoch 97, perplexity 15.795339, time 20.97 sec\n",
      "epoch 98, perplexity 15.553331, time 20.74 sec\n",
      "epoch 99, perplexity 15.491383, time 21.33 sec\n",
      "epoch 100, perplexity 15.255605, time 21.02 sec\n",
      "[563]\n",
      "Output:  [563]\n",
      "Output:  [563, 746]\n",
      "Output:  [563, 746, 1394]\n",
      "Output:  [563, 746, 1394, 600]\n",
      "Output:  [563, 746, 1394, 600, 1075]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51, 1751]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51, 1751, 375]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51, 1751, 375, 212]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51, 1751, 375, 212, 1583]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51, 1751, 375, 212, 1583, 1753]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51, 1751, 375, 212, 1583, 1753, 1453]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51, 1751, 375, 212, 1583, 1753, 1453, 936]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51, 1751, 375, 212, 1583, 1753, 1453, 936, 275]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51, 1751, 375, 212, 1583, 1753, 1453, 936, 275, 212]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51, 1751, 375, 212, 1583, 1753, 1453, 936, 275, 212, 2459]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51, 1751, 375, 212, 1583, 1753, 1453, 936, 275, 212, 2459, 1832]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51, 1751, 375, 212, 1583, 1753, 1453, 936, 275, 212, 2459, 1832, 212]\n",
      "Output:  [563, 746, 1394, 600, 1075, 212, 1804, 51, 1751, 375, 212, 1583, 1753, 1453, 936, 275, 212, 2459, 1832, 212, 212]\n",
      " - 分开始知道 我用本你 脑没有一天 几谁   \n",
      "[462]\n",
      "Output:  [462]\n",
      "Output:  [462, 563]\n",
      "Output:  [462, 563, 746]\n",
      "Output:  [462, 563, 746, 212]\n",
      "Output:  [462, 563, 746, 212, 2381]\n",
      "Output:  [462, 563, 746, 212, 2381, 56]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548, 212]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548, 212, 462]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548, 212, 462, 51]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548, 212, 462, 51, 212]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548, 212, 462, 51, 212, 462]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548, 212, 462, 51, 212, 462, 1729]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548, 212, 462, 51, 212, 462, 1729, 212]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548, 212, 462, 51, 212, 462, 1729, 212, 1804]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548, 212, 462, 51, 212, 462, 1729, 212, 1804, 2143]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548, 212, 462, 51, 212, 462, 1729, 212, 1804, 2143, 1819]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548, 212, 462, 51, 212, 462, 1729, 212, 1804, 2143, 1819, 1966]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548, 212, 462, 51, 212, 462, 1729, 212, 1804, 2143, 1819, 1966, 212]\n",
      "Output:  [462, 563, 746, 212, 2381, 56, 2143, 367, 1548, 212, 462, 51, 212, 462, 1729, 212, 1804, 2143, 1819, 1966, 212, 462]\n",
      " - 不分开 徒快的珊度 不用 不得 我的指悔 不断\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, perplexity 15.321791, time 22.81 sec\n",
      "epoch 102, perplexity 15.167743, time 21.59 sec\n",
      "epoch 103, perplexity 14.721244, time 21.29 sec\n",
      "epoch 104, perplexity 14.686179, time 22.83 sec\n",
      "epoch 105, perplexity 14.555502, time 24.33 sec\n",
      "epoch 106, perplexity 14.349756, time 23.22 sec\n",
      "epoch 107, perplexity 14.285166, time 23.72 sec\n",
      "epoch 108, perplexity 13.934395, time 21.30 sec\n",
      "epoch 109, perplexity 13.838401, time 21.71 sec\n",
      "epoch 110, perplexity 13.638564, time 21.37 sec\n",
      "epoch 111, perplexity 13.513872, time 24.86 sec\n",
      "epoch 112, perplexity 13.306742, time 22.81 sec\n",
      "epoch 113, perplexity 13.203319, time 23.22 sec\n",
      "epoch 114, perplexity 13.118956, time 24.29 sec\n",
      "epoch 115, perplexity 12.965246, time 21.75 sec\n",
      "epoch 116, perplexity 12.734135, time 22.03 sec\n",
      "epoch 117, perplexity 12.698126, time 21.99 sec\n",
      "epoch 118, perplexity 12.418688, time 24.09 sec\n",
      "epoch 119, perplexity 12.448622, time 23.58 sec\n",
      "epoch 120, perplexity 12.342053, time 24.09 sec\n",
      "epoch 121, perplexity 12.312460, time 23.07 sec\n",
      "epoch 122, perplexity 12.200912, time 21.23 sec\n",
      "epoch 123, perplexity 12.024494, time 21.92 sec\n",
      "epoch 124, perplexity 11.732717, time 21.42 sec\n",
      "epoch 125, perplexity 11.882559, time 21.44 sec\n",
      "epoch 126, perplexity 11.632693, time 21.41 sec\n",
      "epoch 127, perplexity 11.664144, time 21.55 sec\n",
      "epoch 128, perplexity 11.476297, time 21.34 sec\n",
      "epoch 129, perplexity 11.408752, time 21.53 sec\n",
      "epoch 130, perplexity 11.249515, time 21.45 sec\n",
      "epoch 131, perplexity 11.277292, time 21.40 sec\n",
      "epoch 132, perplexity 11.083677, time 21.37 sec\n",
      "epoch 133, perplexity 11.087863, time 21.38 sec\n",
      "epoch 134, perplexity 10.964455, time 21.53 sec\n",
      "epoch 135, perplexity 10.808327, time 21.46 sec\n",
      "epoch 136, perplexity 10.622302, time 22.04 sec\n",
      "epoch 137, perplexity 10.642623, time 21.49 sec\n",
      "epoch 138, perplexity 10.599895, time 21.46 sec\n",
      "epoch 139, perplexity 10.459954, time 21.46 sec\n",
      "epoch 140, perplexity 10.357280, time 21.55 sec\n",
      "epoch 141, perplexity 10.339805, time 21.52 sec\n",
      "epoch 142, perplexity 10.259607, time 21.70 sec\n",
      "epoch 143, perplexity 10.184064, time 21.67 sec\n",
      "epoch 144, perplexity 10.098200, time 21.57 sec\n",
      "epoch 145, perplexity 10.057249, time 20.71 sec\n",
      "epoch 146, perplexity 9.833639, time 20.66 sec\n",
      "epoch 147, perplexity 9.848422, time 20.55 sec\n",
      "epoch 148, perplexity 9.800568, time 20.57 sec\n",
      "epoch 149, perplexity 9.703112, time 21.13 sec\n",
      "epoch 150, perplexity 9.688134, time 20.55 sec\n",
      "[563]\n",
      "Output:  [563]\n",
      "Output:  [563, 746]\n",
      "Output:  [563, 746, 1394]\n",
      "Output:  [563, 746, 1394, 1670]\n",
      "Output:  [563, 746, 1394, 1670, 2320]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462, 887]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462, 887, 2083]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462, 887, 2083, 375]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462, 887, 2083, 375, 212]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462, 887, 2083, 375, 212, 695]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462, 887, 2083, 375, 212, 695, 705]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462, 887, 2083, 375, 212, 695, 705, 2123]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462, 887, 2083, 375, 212, 695, 705, 2123, 212]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462, 887, 2083, 375, 212, 695, 705, 2123, 212, 563]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462, 887, 2083, 375, 212, 695, 705, 2123, 212, 563, 746]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462, 887, 2083, 375, 212, 695, 705, 2123, 212, 563, 746, 212]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462, 887, 2083, 375, 212, 695, 705, 2123, 212, 563, 746, 212, 1400]\n",
      "Output:  [563, 746, 1394, 1670, 2320, 212, 1804, 462, 887, 2083, 375, 212, 695, 705, 2123, 212, 563, 746, 212, 1400, 212]\n",
      " - 分开始边到 我不想为你 那童手 分开 雨 Y\n",
      "[462]\n",
      "Output:  [462]\n",
      "Output:  [462, 563]\n",
      "Output:  [462, 563, 746]\n",
      "Output:  [462, 563, 746, 212]\n",
      "Output:  [462, 563, 746, 212, 2381]\n",
      "Output:  [462, 563, 746, 212, 2381, 936]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167, 212]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167, 212, 11]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167, 212, 11, 1032]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167, 212, 11, 1032, 2143]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167, 212, 11, 1032, 2143, 2123]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167, 212, 11, 1032, 2143, 2123, 465]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167, 212, 11, 1032, 2143, 2123, 465, 212]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167, 212, 11, 1032, 2143, 2123, 465, 212, 1804]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167, 212, 11, 1032, 2143, 2123, 465, 212, 1804, 1978]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167, 212, 11, 1032, 2143, 2123, 465, 212, 1804, 1978, 547]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167, 212, 11, 1032, 2143, 2123, 465, 212, 1804, 1978, 547, 414]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167, 212, 11, 1032, 2143, 2123, 465, 212, 1804, 1978, 547, 414, 212]\n",
      "Output:  [462, 563, 746, 212, 2381, 936, 2265, 508, 1167, 212, 11, 1032, 2143, 2123, 465, 212, 1804, 1978, 547, 414, 212, 212]\n",
      " - 不分开 徒一定爱过 温色的手绪 我在等待   \n",
      "epoch 151, perplexity 9.569247, time 20.55 sec\n",
      "epoch 152, perplexity 9.344489, time 20.67 sec\n",
      "epoch 153, perplexity 9.419790, time 20.56 sec\n",
      "epoch 154, perplexity 9.357773, time 20.51 sec\n",
      "epoch 155, perplexity 9.244020, time 20.58 sec\n",
      "epoch 156, perplexity 9.131642, time 20.91 sec\n",
      "epoch 157, perplexity 9.112670, time 20.59 sec\n",
      "epoch 158, perplexity 8.990282, time 20.72 sec\n",
      "epoch 159, perplexity 8.967238, time 20.55 sec\n",
      "epoch 160, perplexity 8.973012, time 20.83 sec\n",
      "epoch 161, perplexity 8.806418, time 23.72 sec\n",
      "epoch 162, perplexity 8.736914, time 23.00 sec\n",
      "epoch 163, perplexity 8.767984, time 25.46 sec\n",
      "epoch 164, perplexity 8.619625, time 21.69 sec\n",
      "epoch 165, perplexity 8.597391, time 23.50 sec\n",
      "epoch 166, perplexity 8.580597, time 23.50 sec\n",
      "epoch 167, perplexity 8.548623, time 23.29 sec\n",
      "epoch 168, perplexity 8.503984, time 22.28 sec\n",
      "epoch 169, perplexity 8.314569, time 26.41 sec\n",
      "epoch 170, perplexity 8.227705, time 21.87 sec\n",
      "epoch 171, perplexity 8.207390, time 24.51 sec\n",
      "epoch 172, perplexity 8.150074, time 23.62 sec\n",
      "epoch 173, perplexity 8.081417, time 21.69 sec\n",
      "epoch 174, perplexity 7.997490, time 20.58 sec\n",
      "epoch 175, perplexity 7.995003, time 21.73 sec\n",
      "epoch 176, perplexity 7.968852, time 24.12 sec\n",
      "epoch 177, perplexity 7.932434, time 25.84 sec\n",
      "epoch 178, perplexity 7.929008, time 23.50 sec\n",
      "epoch 179, perplexity 7.774959, time 24.03 sec\n",
      "epoch 180, perplexity 7.778458, time 22.78 sec\n",
      "epoch 181, perplexity 7.648170, time 20.94 sec\n",
      "epoch 182, perplexity 7.688853, time 23.34 sec\n",
      "epoch 183, perplexity 7.631673, time 23.69 sec\n",
      "epoch 184, perplexity 7.666391, time 23.27 sec\n",
      "epoch 185, perplexity 7.516147, time 23.15 sec\n",
      "epoch 186, perplexity 7.428493, time 25.06 sec\n",
      "epoch 187, perplexity 7.410017, time 23.77 sec\n",
      "epoch 188, perplexity 7.293262, time 21.81 sec\n"
     ]
    }
   ],
   "source": [
    "# 随机采样\n",
    "perplexity_hist_random = train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, ctx, corpus_indices, idx_to_char,\n",
    "                      char_to_idx, True, num_epochs, num_steps, lr,\n",
    "                      clipping_theta, batch_size, pred_period, pred_len,\n",
    "                      prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相邻采样\n",
    "perplexity_hist_adjacency = train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, ctx, corpus_indices, idx_to_char,\n",
    "                      char_to_idx, False, num_epochs, num_steps, lr,\n",
    "                      clipping_theta, batch_size, pred_period, pred_len,\n",
    "                      prefixes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
