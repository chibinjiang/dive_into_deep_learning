{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 丢弃法(倒置丢弃法)\n",
    "- dropout(inverted dropout)\n",
    "- 操作: 隐藏层的神经元 有p的概率被丢弃, 并且有 1-p的概率做除以 1-p的拉伸:  $$ h^{'} = \\frac{ξ}{1-p}h $$, 其中 随机变量 ξ 是0和1的概率分别是 p和1-p, 即符合伯努利分布, 则 随机变量ξ的期望值是 1-p, 那么, 拉伸后的新神经元的期望值是, $$ E(h^{'}) = \\frac{E(ξ)}{1-p}h = h $$, 所以丢弃法不改变输入的期望值\n",
    "- 丢弃概率 p, 是超参数\n",
    "- 因为隐藏层的单元都有可能被清零, 输出层的计算不能过度依赖隐藏层中的任一个, 从而起到正则化的作用\n",
    "- 为了确定的结果, 测试模型时不使用丢弃法\n",
    "- 论文:\n",
    "    - Hiton老爷子的合作论文: [Dropout: a simple way to prevent neural networks from overfitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf), 多伦多大学好牛!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob):\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # 这种情况下把全部元素都丢弃\n",
    "    if keep_prob == 0:\n",
    "        return X.zeros_like()\n",
    "    mask = nd.random.uniform(0, 1, X.shape) < keep_prob  # 为什么是均匀分布\n",
    "    return mask * X / keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
       " [ 8.  9. 10. 11. 12. 13. 14. 15.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.arange(16).reshape((2, 8))\n",
    "dropout(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.96366274 0.2726563  0.3834415  0.47766513 0.79172504 0.8121687\n",
       "  0.5288949  0.47997716]\n",
       " [0.56804454 0.3927848  0.92559665 0.83607876 0.07103606 0.33739617\n",
       "  0.08712929 0.6481719 ]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.random.uniform(0, 1, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  2.  0.  0.  0. 10.  0.  0.]\n",
       " [ 0. 18.  0.  0. 24.  0.  0.  0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens1))\n",
    "b1 = nd.zeros(num_hiddens1)\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens1, num_hiddens2))\n",
    "b2 = nd.zeros(num_hiddens2)\n",
    "W3 = nd.random.normal(scale=0.01, shape=(num_hiddens2, num_outputs))\n",
    "b3 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H1 = (nd.dot(X, W1) + b1).relu()\n",
    "    if autograd.is_training():  # 只在训练模型时使用丢弃法\n",
    "        H1 = dropout(H1, drop_prob1)  # 在第一层全连接后添加丢弃层\n",
    "    H2 = (nd.dot(H1, W2) + b2).relu()\n",
    "    if autograd.is_training():\n",
    "        H2 = dropout(H2, drop_prob2)  # 在第二层全连接后添加丢弃层\n",
    "    return nd.dot(H2, W3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_workers(num_workers=4):\n",
    "    # 0 means no additional process is used to speed up the reading of data.\n",
    "    if sys.platform.startswith('win'):\n",
    "        return 0\n",
    "    else:\n",
    "        return num_workers\n",
    "    \n",
    "    \n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n",
    "    dataset = gluon.data.vision\n",
    "    trans = [dataset.transforms.Resize(resize)] if resize else []\n",
    "    trans.append(dataset.transforms.ToTensor())\n",
    "    trans = dataset.transforms.Compose(trans)\n",
    "    mnist_train = dataset.FashionMNIST(train=True).transform_first(trans)\n",
    "    mnist_test = dataset.FashionMNIST(train=False).transform_first(trans)\n",
    "    return (gluon.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                                  num_workers=get_dataloader_workers()),\n",
    "            gluon.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                                  num_workers=get_dataloader_workers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算预测准确率\n",
    "def accuracy(y_hat, y):\n",
    "#     return (nd.argmax(y_hat) == y).sum()/y.shape[0]\n",
    "    return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()\n",
    "\n",
    "\n",
    "# 评估模型\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        y = y.astype('float32')\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum().asscalar()\n",
    "        n += y.size\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"\n",
    "    参数还是照常更新, dropout是在 优化模型中\n",
    "    \"\"\"\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size\n",
    "        \n",
    "        \n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, trainer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            if trainer is None:\n",
    "                sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1172, train acc 0.568, test acc 0.754\n",
      "epoch 2, loss 0.5759, train acc 0.787, test acc 0.831\n",
      "epoch 3, loss 0.4917, train acc 0.820, test acc 0.842\n",
      "epoch 4, loss 0.4448, train acc 0.838, test acc 0.853\n",
      "epoch 5, loss 0.4198, train acc 0.848, test acc 0.860\n",
      "epoch 6, loss 0.3970, train acc 0.856, test acc 0.872\n",
      "epoch 7, loss 0.3796, train acc 0.862, test acc 0.870\n",
      "epoch 8, loss 0.3687, train acc 0.867, test acc 0.875\n",
      "epoch 9, loss 0.3537, train acc 0.871, test acc 0.871\n",
      "epoch 10, loss 0.3458, train acc 0.873, test acc 0.879\n",
      "epoch 11, loss 0.3365, train acc 0.877, test acc 0.880\n",
      "epoch 12, loss 0.3357, train acc 0.878, test acc 0.883\n",
      "epoch 13, loss 0.3184, train acc 0.884, test acc 0.881\n",
      "epoch 14, loss 0.3155, train acc 0.884, test acc 0.886\n",
      "epoch 15, loss 0.3077, train acc 0.886, test acc 0.887\n",
      "epoch 16, loss 0.3024, train acc 0.889, test acc 0.884\n",
      "epoch 17, loss 0.2975, train acc 0.891, test acc 0.888\n",
      "epoch 18, loss 0.2956, train acc 0.891, test acc 0.886\n",
      "epoch 19, loss 0.2900, train acc 0.892, test acc 0.889\n",
      "epoch 20, loss 0.2840, train acc 0.896, test acc 0.891\n",
      "epoch 21, loss 0.2818, train acc 0.896, test acc 0.891\n",
      "epoch 22, loss 0.2776, train acc 0.896, test acc 0.890\n",
      "epoch 23, loss 0.2742, train acc 0.899, test acc 0.892\n",
      "epoch 24, loss 0.2683, train acc 0.901, test acc 0.895\n",
      "epoch 25, loss 0.2672, train acc 0.900, test acc 0.890\n",
      "epoch 26, loss 0.2627, train acc 0.902, test acc 0.888\n",
      "epoch 27, loss 0.2624, train acc 0.902, test acc 0.894\n",
      "epoch 28, loss 0.2576, train acc 0.904, test acc 0.890\n",
      "epoch 29, loss 0.2560, train acc 0.904, test acc 0.898\n",
      "epoch 30, loss 0.2522, train acc 0.906, test acc 0.892\n",
      "epoch 31, loss 0.2503, train acc 0.905, test acc 0.891\n",
      "epoch 32, loss 0.2456, train acc 0.909, test acc 0.890\n",
      "epoch 33, loss 0.2453, train acc 0.909, test acc 0.896\n",
      "epoch 34, loss 0.2424, train acc 0.909, test acc 0.897\n",
      "epoch 35, loss 0.2374, train acc 0.911, test acc 0.890\n",
      "epoch 36, loss 0.2365, train acc 0.911, test acc 0.899\n",
      "epoch 37, loss 0.2336, train acc 0.912, test acc 0.894\n",
      "epoch 38, loss 0.2331, train acc 0.913, test acc 0.892\n",
      "epoch 39, loss 0.2335, train acc 0.912, test acc 0.897\n",
      "epoch 40, loss 0.2306, train acc 0.914, test acc 0.899\n",
      "epoch 41, loss 0.2274, train acc 0.914, test acc 0.896\n",
      "epoch 42, loss 0.2223, train acc 0.917, test acc 0.900\n",
      "epoch 43, loss 0.2235, train acc 0.916, test acc 0.901\n",
      "epoch 44, loss 0.2222, train acc 0.915, test acc 0.898\n",
      "epoch 45, loss 0.2199, train acc 0.917, test acc 0.896\n",
      "epoch 46, loss 0.2186, train acc 0.917, test acc 0.897\n",
      "epoch 47, loss 0.2182, train acc 0.918, test acc 0.900\n",
      "epoch 48, loss 0.2122, train acc 0.920, test acc 0.900\n",
      "epoch 49, loss 0.2147, train acc 0.919, test acc 0.899\n",
      "epoch 50, loss 0.2109, train acc 0.921, test acc 0.900\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr, batch_size = 50, 0.5, 256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_net = nn.Sequential()\n",
    "seq_net.add(nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob1),  # 在第一个全连接层后添加丢弃层\n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob2),  # 在第二个全连接层后添加丢弃层\n",
    "        nn.Dense(10))\n",
    "seq_net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1917, train acc 0.538, test acc 0.769\n",
      "epoch 2, loss 0.5946, train acc 0.779, test acc 0.830\n",
      "epoch 3, loss 0.4965, train acc 0.818, test acc 0.794\n",
      "epoch 4, loss 0.4547, train acc 0.836, test acc 0.861\n",
      "epoch 5, loss 0.4270, train acc 0.845, test acc 0.863\n",
      "epoch 6, loss 0.4016, train acc 0.854, test acc 0.866\n",
      "epoch 7, loss 0.3845, train acc 0.860, test acc 0.873\n",
      "epoch 8, loss 0.3712, train acc 0.864, test acc 0.875\n",
      "epoch 9, loss 0.3635, train acc 0.867, test acc 0.873\n",
      "epoch 10, loss 0.3460, train acc 0.874, test acc 0.877\n",
      "epoch 11, loss 0.3419, train acc 0.875, test acc 0.883\n",
      "epoch 12, loss 0.3327, train acc 0.878, test acc 0.882\n",
      "epoch 13, loss 0.3295, train acc 0.879, test acc 0.881\n",
      "epoch 14, loss 0.3204, train acc 0.883, test acc 0.885\n",
      "epoch 15, loss 0.3169, train acc 0.884, test acc 0.882\n",
      "epoch 16, loss 0.3118, train acc 0.884, test acc 0.880\n",
      "epoch 17, loss 0.3011, train acc 0.889, test acc 0.883\n",
      "epoch 18, loss 0.2974, train acc 0.890, test acc 0.883\n",
      "epoch 19, loss 0.2942, train acc 0.892, test acc 0.892\n",
      "epoch 20, loss 0.2906, train acc 0.892, test acc 0.882\n",
      "epoch 21, loss 0.2827, train acc 0.895, test acc 0.894\n",
      "epoch 22, loss 0.2821, train acc 0.895, test acc 0.890\n",
      "epoch 23, loss 0.2771, train acc 0.896, test acc 0.889\n",
      "epoch 24, loss 0.2796, train acc 0.896, test acc 0.888\n",
      "epoch 25, loss 0.2726, train acc 0.898, test acc 0.892\n",
      "epoch 26, loss 0.2702, train acc 0.900, test acc 0.891\n",
      "epoch 27, loss 0.2650, train acc 0.900, test acc 0.893\n",
      "epoch 28, loss 0.2678, train acc 0.900, test acc 0.895\n",
      "epoch 29, loss 0.2585, train acc 0.902, test acc 0.892\n",
      "epoch 30, loss 0.2581, train acc 0.903, test acc 0.889\n",
      "epoch 31, loss 0.2546, train acc 0.903, test acc 0.897\n",
      "epoch 32, loss 0.2499, train acc 0.906, test acc 0.893\n",
      "epoch 33, loss 0.2493, train acc 0.907, test acc 0.893\n",
      "epoch 34, loss 0.2479, train acc 0.908, test acc 0.895\n",
      "epoch 35, loss 0.2443, train acc 0.908, test acc 0.884\n",
      "epoch 36, loss 0.2429, train acc 0.909, test acc 0.894\n",
      "epoch 37, loss 0.2426, train acc 0.909, test acc 0.895\n",
      "epoch 38, loss 0.2375, train acc 0.912, test acc 0.896\n",
      "epoch 39, loss 0.2359, train acc 0.910, test acc 0.898\n",
      "epoch 40, loss 0.2354, train acc 0.912, test acc 0.894\n",
      "epoch 41, loss 0.2327, train acc 0.912, test acc 0.897\n",
      "epoch 42, loss 0.2290, train acc 0.912, test acc 0.897\n",
      "epoch 43, loss 0.2282, train acc 0.914, test acc 0.894\n",
      "epoch 44, loss 0.2281, train acc 0.914, test acc 0.895\n",
      "epoch 45, loss 0.2262, train acc 0.914, test acc 0.895\n",
      "epoch 46, loss 0.2232, train acc 0.915, test acc 0.896\n",
      "epoch 47, loss 0.2257, train acc 0.914, test acc 0.896\n",
      "epoch 48, loss 0.2199, train acc 0.917, test acc 0.898\n",
      "epoch 49, loss 0.2152, train acc 0.918, test acc 0.896\n",
      "epoch 50, loss 0.2197, train acc 0.917, test acc 0.897\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(seq_net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_ch3(seq_net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正向传播/反向传播和 计算图\n",
    "1. Frobenius范数: 每个元素的平方和, 再开方, 用于计算矩阵的L2范数\n",
    "2. 多层神经网络中, 正则化的惩罚项是 各隐藏层 的 Frobenius范数的平均\n",
    "3. 计算图: \n",
    "    - 左下角是输入\n",
    "    - 右上角的输出\n",
    "    - 变量是方框, 操作符是圆圈\n",
    "    - 大部分箭头向右或向上\n",
    "4. 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数值稳定性和 模型初始化\n",
    "- 随着神经网络层数增多, 数值稳定性可能会变差, 出现输出层的衰减或者爆炸\n",
    "- 初始化: 不可将隐藏单元的值初始化成相等的值, 因为在反向传播中, 参数梯度值也会相等, 导致隐藏单元的值一直相等(可以试试)\n",
    "    - Solution: 随机初始化\n",
    "    - 打破对称性(symmetry breaking): When some machine learning models have weights all initialized to the same value, it can be difficult or impossible for the weights to differ as the model is trained. Initializing the model to small random values breaks the symmetry and allows different weights to learn independently of each other.\n",
    "    - 相同的参数, WX的乘积求和就导致隐藏层的单元的值都一样: https://stackoverflow.com/a/20029817/5141613\n",
    "    - MXNet默认初始化: 权重参数每个元素随机采样于-0.07到0.07之间的均匀分布，bias全部清零\n",
    "    - Xavier随机初始化: 在full connected 网络中, a是输入层个数, b是输出层个数, 那么权重参数随机采样于 均匀分布:  $$ U(-\\sqrt{\\frac{6}{a+b}}, \\sqrt{\\frac{6}{a+b}}) $$, 优点: 每层输出的方差不受输入个数的影响, 每层梯度的方差不受输出个数的影响\n",
    "- 避免衰减和爆炸:\n",
    "- 避免参数的初始值过大或者过小:\n",
    "    - He-et-al Initialization: 令a=上一层的维度, 则 随机初始化后, 再乘以 $$ \\sqrt{\\frac{2}{a}} $$\n",
    "    - Sigmoid的Xavier Initialization: $$ \\sqrt{\\frac{1}{a}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习\n",
    "1. 调参:\n",
    "    - 对调 两个丢弃参数, 有何结果\n",
    "    - 比较 使用丢弃法和不使用丢弃法的结果\n",
    "    - 增加隐藏层单元 或者 增加隐藏层数, 使用丢弃法处理过拟合的效果好不好更明显\n",
    "    - 比较丢弃法和权重衰减的效果, 如果同时使用 权重衰减和丢弃法, 效果如何\n",
    "2. Symmetry breaking中的对称如何理解?\n",
    "    - 正向转播和反向传播中, 只有一个神经元在起作用\n",
    "3. 能否将线性回归或者softmax回归中的 权重参数初始化成相同的值?\n",
    "    - 不能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.random.uniform?\n",
    "# 采样区间: 左闭右开, 默认区间: -0.07~0.07"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
