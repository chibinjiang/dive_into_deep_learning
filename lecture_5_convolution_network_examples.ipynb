{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 那些年, 大牛的卷积网络\n",
    "1. 卷积网络:\n",
    "    - 尝试解决全连接层的局限:\n",
    "        - 因为全连接层按行展开, 难于识别相距较远的 模式 -> 卷积层保留输入形状, 使得图像的像素在高和宽两个方向上的相关性可能被识别;\n",
    "        - 全连接层模型容易过大 -> 卷积层通过滑动窗口, 和不同位置的输入做卷积计算, 避免参数尺寸过大\n",
    "\n",
    "### Lenet\n",
    "0. 1998年\n",
    "1. 交叉使用两个模块: 卷积层 + 全连接层\n",
    "2. 卷积层基本单位 = 卷积层 + 最大池化层, 卷积层识别空间模式, 池化层降低卷积层对位置的敏感性\n",
    "3. 结构:\n",
    "    - 卷积层块:\n",
    "        - 卷积层: 5x5 -> 输出 -> sigmoid\n",
    "        - 1st 卷积层 通道为 5, 2nd 为16 <- 增加通道使两个卷积层的参数尺寸类似\n",
    "        - 两个 2x2 的max-pooling窗口, 步幅是2\n",
    "    - 输出: (批量大小, 通道数, 高, 宽)\n",
    "    - 进入全连接层: flatten 每个样本, 输入形状变成二维 -> 样本数 * flatten vectors\n",
    "    - 三个全连接层: 120, 84, 10(输出类别个数, 因为是MNIST)\n",
    "4. 缺点:\n",
    "    - 多通道, 多层, 大量参数, 如今有GPU就不是问题\n",
    "    \n",
    "### AlexNet\n",
    "1. end-to-end: 神经网络可以直接基于图像的原始像素进行分类\n",
    "2. 那些年图像分类的特征工程, 流程:\n",
    "    - 获取图像数据集\n",
    "    - 使用特征提取函数生成图像的特征\n",
    "    - 使用机器学习模型对图像的特征分类\n",
    "3. 学习特征表示:\n",
    "    - 多层神经网络可能可以学习到数据的 多级表征, 并逐级表示逐渐抽象的概念或模式\n",
    "4. AlexNet 首次证明了学习到的特征可以超越手工设计的特征\n",
    "5. 与leNet的不用:\n",
    "    - 8层变换:\n",
    "        - 5层卷积\n",
    "        - 2层全连接隐藏层\n",
    "        - 1层全连接输出层\n",
    "    - 将sigmoid激活函数改成relu激活函数, 后者计算更简单\n",
    "        - sigmoid 的缺点: 当输出极限接近0或者1时, 梯度几乎为0, 导致反向传播无法更新部分模型参数\n",
    "    - 通过丢弃法控制全连接层的模型复杂福\n",
    "    - 引入大量图像增广: 翻转/裁剪/颜色变化, 进一步扩大数据集来缓解过拟合\n",
    "6. 结构:\n",
    "    - 第一层: 11x11的卷积窗口 -> 需要大窗口捕获物体\n",
    "    - 第二层: 5x5的卷积窗口\n",
    "    - 第三及之后层: 3x3的卷积窗口\n",
    "    - 第1/2/5层都使用了3x3, 步幅为2的最大池化层\n",
    "    - 卷积层之后是两个 输出个数为4096的全连接层\n",
    "9. 结构图: ![](./images/alexnet_structure.png)\n",
    "\n",
    "### VGG\n",
    "1. 实验室: Visual Geometry Group\n",
    "2. 提出了通过重复使用简单的基础块来构建深度模型的思路\n",
    "3. 结构:\n",
    "    - 连续使用数个相同的填充为1、窗口形状为 3×3 的卷积层后接一个步幅为2、窗口形状为 2×2 的最大池化层\n",
    "    - 卷积层保持输入的高和宽不变\n",
    "    - 池化层则对宽高减半\n",
    "    - 卷积模块后接全连接层\n",
    "\n",
    "### NiN, 网中网\n",
    "1. LeNet/AlexNet/VGG 的共同之处, 是 先使用卷积层构成的模块充分抽取空间特征, 再以 全连接层 构成的模块来输出分类 结果\n",
    "2. AlexNet/VGG的改进之处在于, 将两个模块加厚(增加通道数), 加深\n",
    "3. NiN的思路: 串联多个 卷积层+全连接层 的小网络来构建 一个深层网络\n",
    "4. NiN使用 1x1卷积层来代替全连接层, 从而将空间信息传递到后面的层, 其中, 1x1卷积层空间维度(宽+高)上的每个元素相当于样本, 通道相当于特征\n",
    "5. NiN块: 1个卷积层 + 2个 1x1卷积层\n",
    "6. NiN结构:\n",
    "    - 卷积层窗口形状 分别是 11, 5, 3\n",
    "    - 通道数 和 alexnet 相同\n",
    "    - 每个 NiN块 后接一个 步幅为2, 窗口形状为3x3的最大池化层\n",
    "7. 和 AlexNet相比, 去掉了 最后三个全连接层, 最后的是 输出通道数 等于 标签类别数的 NiN 块 + 全局平均池化层\n",
    "8. 优点: 显著建雄模型参数尺寸, 缓解过拟合\n",
    "9. 缺点: 可能会导致获取有效模型的训练时间增加\n",
    "10. <b>这是一个  参数数量 和 训练时间 的博弈</b>\n",
    "11. 结构图: ![](./images/nin.png)\n",
    "\n",
    "### GoogLeNet(含并行连接)\n",
    "1. GoogLeNet吸收了NiN中网络串联网络的思想\n",
    "2. Inception 块: 4条并行线路\n",
    "    - 1x1 卷积层\n",
    "    - 1x1 卷积层 + 3x3 卷积层(3和5抽取不同空间尺寸的信息)\n",
    "    - 1x1 卷积层 + 5x5 卷积层\n",
    "    - 3x3 最大池化层 + 1x1 卷积层(减少通道数, 降低复杂度)\n",
    "    - 输出: 通道合并层\n",
    "3. Inception 块结构: ![](./images/inception.png)\n",
    "4. GoogLeNet 结构:\n",
    "    - 主体卷积部分:\n",
    "        - 模块1: 卷积层(64通道, 7x7) + 最大池化层(步幅=2, 3x3)\n",
    "        - 模块2: 卷积层(64通道, 1x1) + 卷积层(192通道, 3x3) + 最大池化层(步幅=2, 充填=1, 3x3) , 对应 Inception的第二条线路\n",
    "        - 模块3: 串联两个 Inception块 + 最大池化层(步幅=2, 填充=1, 3x3):\n",
    "            - 第一个Inception, 输出通道数为256: \n",
    "                - 64\n",
    "                - 96, 128\n",
    "                - 16, 32\n",
    "                - 32\n",
    "            - 第二个Inception, 输出通道数为480:\n",
    "                - 128\n",
    "                - 128, 192\n",
    "                - 32, 96\n",
    "                - 64\n",
    "        - 模块4: 5个Inception块 + 最大池化层(步幅=2, 填充=1, 3x3)\n",
    "            - 第1个Inception, 输出通道数为512\n",
    "                - 192\n",
    "                - 96, 208\n",
    "                - 16, 48\n",
    "                - 64\n",
    "            - 第2个Inception, 输出通道数为512\n",
    "                - 160\n",
    "                - 112, 224\n",
    "                - 24, 64\n",
    "                - 64\n",
    "            - 第3个Inception, 输出通道数为512\n",
    "                - 128\n",
    "                - 128, 256\n",
    "                - 24, 64\n",
    "                - 64\n",
    "            - 第4个Inception, 输出通道数为528\n",
    "                - 112\n",
    "                - 144, 288\n",
    "                - 32, 64\n",
    "                - 64\n",
    "            - 第5个Inception, 输出通道数为832\n",
    "                - 256\n",
    "                - 160, 320\n",
    "                - 32, 128\n",
    "                - 128\n",
    "5. Inception块的通道数分配之比 需要通过大量实验获取\n",
    "\n",
    "        \n",
    "### ResNet, 残差网络\n",
    "1. 加入BN依旧未完全解决: more 层, 训练稳定的深度模型越困难, 训练误差反而可能上升\n",
    "2. 加入残差网络: 输入可以跨层向前传播\n",
    "3. 之前的映射: f(x); 新的残差映射: f(x) - x\n",
    "4. 残差块 Residual Block, 结构:\n",
    "    - 两个相同输出通道数的 3x3 卷积层 (输出形状同输入)\n",
    "    - 每个卷积层后 跟着 BN\n",
    "    - 加上 输入\n",
    "    - 加上ReLU\n",
    "    - 如果要改变 输出通道数, 则需要使用1x1 卷积层resize输入再做相加运算\n",
    "5. ResNet模型\n",
    "    - 卷积层(64通道, 步幅为2, 7x7) + BN + 最大池化层(步幅为2, 3x3)\n",
    "    - 4个残差块\n",
    "6. 相比 GoogLeNet, 结构更简单\n",
    "### DenseNet\n",
    "1. 从ResNet引申出来, 但是 ResNet加上输入, 而DenseNet连结输入\n",
    "2. DenseNet结构:\n",
    "    - 稠密块: 定义输出和输入如何连结(在通道维)\n",
    "        - 卷积层 + BN + ReLU\n",
    "    - 过渡层: 控制通道数(控制模型复杂度), 避免过大\n",
    "        - 1x1卷积层, 减少 通道数\n",
    "        - 平均池化层(步幅=2), 减半宽和高\n",
    "3. 增长率(Grow Rate): 输出通道数 相对 输入通道数的增长\n",
    "4. 卷积块的通道数 控制 增长率\n",
    "5. DenseNet模型结构:\n",
    "    - 单卷积层\n",
    "    - 最大池化层\n",
    "    - 4个稠密块\n",
    "    - 全局平均池化层\n",
    "    - 全连接层\n",
    "    \n",
    "### 批量归一化\n",
    "1. 如果没有BN: 每层参数的更新难以导致靠近输出层的输出有剧烈变化, 这种数值的不稳定性会导致难以训练处有效的深度模型\n",
    "2. 如果加了BN: \n",
    "    - BN利用小批量上的均值和标准差, 不断调整神经网络的中间输出, 使用整个神经网络在各层的中间输出的数值更稳定\n",
    "    - 更快地收敛\n",
    "3. 对全连接层进行BN\n",
    "    - 在激活函数之前\n",
    "    - 对 Wx+b 求均值u和方差σ: \n",
    "    - 归一化时, 分母添加小常数, 避免 分母为0: y = (y - u) / sqrt(σ ^ 2, ϵ)\n",
    "    - 拉伸: 按元素乘一个参数, 按元素加一个参数, 这两个参数可以训练, 可以使得BN效果消失\n",
    "4. 对卷积层进行BN\n",
    "    - 独立对每个通道做BN: 每个通道有独立的拉伸和偏移参数\n",
    "5. 预测时的BN(用已有模型预测测试集)\n",
    "\n",
    "### 结语\n",
    "#### 吃透这几个Net\n",
    "\n",
    "\n",
    "### 参考\n",
    "- [Gradient-based learning applied to document recognition](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)\n",
    "- [Imagenet classification with deep convolutional neural networks.](http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf)\n",
    "- [Very deep convolutional networks for large-scale image recognition.](https://arxiv.org/abs/1409.1556)\n",
    "- 新加坡国立大学, [Network in Network](https://arxiv.org/pdf/1312.4400.pdf)\n",
    "- [Going deeper with convolutions](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf)\n",
    "- 批量归一化, [Batch normalization: Accelerating deep network training by reducing internal covariate shift](https://arxiv.org/pdf/1502.03167.pdf)\n",
    "- [Identity mappings in deep residual networks](https://arxiv.org/pdf/1603.05027.pdf)\n",
    "- [Deep residual learning for image recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "import mxnet as mx\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn, data as gdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = nn.Sequential()\n",
    "lenet.add(nn.Conv2D(channels=6, kernel_size=5, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        # Dense会默认将(批量大小, 通道, 高, 宽)形状的输入转换成\n",
    "        # (批量大小, 通道 * 高 * 宽)形状的输入\n",
    "        nn.Dense(120, activation='sigmoid'),\n",
    "        nn.Dense(84, activation='sigmoid'),\n",
    "        nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0 output shape:\t (1, 6, 24, 24)\n",
      "pool0 output shape:\t (1, 6, 12, 12)\n",
      "conv1 output shape:\t (1, 16, 8, 8)\n",
      "pool1 output shape:\t (1, 16, 4, 4)\n",
      "dense0 output shape:\t (1, 120)\n",
      "dense1 output shape:\t (1, 84)\n",
      "dense2 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 28, 28))\n",
    "lenet.initialize()\n",
    "for layer in lenet:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctx():\n",
    "    \"\"\"\n",
    "    GPU first\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.zeros((1,), ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, ctx):\n",
    "    acc_sum, n = nd.array([0], ctx=ctx), 0\n",
    "    for X, y in data_iter:\n",
    "        # 如果ctx代表GPU及相应的显存，将数据复制到显存上\n",
    "        X, y = X.as_in_context(ctx), y.as_in_context(ctx).astype('float32')\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.size\n",
    "    return acc_sum.asscalar() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs):\n",
    "    print('training on', ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec' % (\n",
    "            epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_workers(num_workers=4):\n",
    "    # 0 means no additional process is used to speed up the reading of data.\n",
    "    if sys.platform.startswith('win'):\n",
    "        return 0\n",
    "    else:\n",
    "        return num_workers\n",
    "    \n",
    "    \n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n",
    "    dataset = gluon.data.vision\n",
    "    trans = [dataset.transforms.Resize(resize)] if resize else []\n",
    "    trans.append(dataset.transforms.ToTensor())\n",
    "    trans = dataset.transforms.Compose(trans)\n",
    "    mnist_train = dataset.FashionMNIST(train=True).transform_first(trans)\n",
    "    mnist_test = dataset.FashionMNIST(train=False).transform_first(trans)\n",
    "    return (gluon.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                                  num_workers=get_dataloader_workers()),\n",
    "            gluon.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                                  num_workers=get_dataloader_workers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gluon.data.vision.FashionMNIST?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "\n",
    "\n",
    "def get_fashion_mnist_label(label):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return text_labels[int(label)]\n",
    "\n",
    "\n",
    "def show_fashion_mnist(images, labels):\n",
    "    display.set_matplotlib_formats('svg')  # 用grey 就不行\n",
    "    _, figs = plt.subplots(1, len(images), figsize=(20, 20))\n",
    "    for f, img, lbl in zip(figs, images, labels):\n",
    "        f.imshow(img.reshape((28, 28)).asnumpy())\n",
    "        f.set_title(lbl)\n",
    "        f.axes.get_xaxis().set_visible(False)\n",
    "        f.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x1728 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize image set\n",
    "show_num = 10\n",
    "i = 0\n",
    "_, figs = plt.subplots(1, show_num, figsize=(24, 24))\n",
    "for sample_X, sample_y in train_iter:\n",
    "    break\n",
    "for image_pixel, image_label in zip(sample_X, sample_y):\n",
    "    # 不会自动换行\n",
    "    if i >= show_num:\n",
    "        break\n",
    "    f = figs[i]\n",
    "    f.imshow(image_pixel.reshape((28, 28)).asnumpy())\n",
    "    f.set_title(get_fashion_mnist_label(image_label.asscalar()))\n",
    "    f.axes.get_xaxis().set_visible(False)\n",
    "    f.axes.get_yaxis().set_visible(False)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n",
      "epoch 1, loss 2.3159, train acc 0.105, test acc 0.100, time 16.6 sec\n",
      "epoch 2, loss 1.4244, train acc 0.443, test acc 0.609, time 15.1 sec\n",
      "epoch 3, loss 0.8718, train acc 0.658, test acc 0.719, time 15.4 sec\n",
      "epoch 4, loss 0.7152, train acc 0.718, test acc 0.728, time 25.9 sec\n",
      "epoch 5, loss 0.6367, train acc 0.745, test acc 0.754, time 25.8 sec\n",
      "epoch 6, loss 0.5756, train acc 0.772, test acc 0.793, time 17.0 sec\n",
      "epoch 7, loss 0.5285, train acc 0.792, test acc 0.809, time 18.4 sec\n",
      "epoch 8, loss 0.4928, train acc 0.808, test acc 0.830, time 17.8 sec\n",
      "epoch 9, loss 0.4648, train acc 0.823, test acc 0.834, time 17.2 sec\n",
      "epoch 10, loss 0.4454, train acc 0.832, test acc 0.847, time 16.0 sec\n",
      "epoch 11, loss 0.4247, train acc 0.840, test acc 0.841, time 18.4 sec\n",
      "epoch 12, loss 0.4121, train acc 0.846, test acc 0.850, time 21.8 sec\n",
      "epoch 13, loss 0.3994, train acc 0.852, test acc 0.858, time 25.0 sec\n",
      "epoch 14, loss 0.3861, train acc 0.858, test acc 0.867, time 26.0 sec\n",
      "epoch 15, loss 0.3746, train acc 0.862, test acc 0.867, time 18.1 sec\n",
      "epoch 16, loss 0.3638, train acc 0.866, test acc 0.871, time 15.4 sec\n",
      "epoch 17, loss 0.3560, train acc 0.869, test acc 0.874, time 17.4 sec\n",
      "epoch 18, loss 0.3489, train acc 0.871, test acc 0.867, time 18.1 sec\n",
      "epoch 19, loss 0.3420, train acc 0.874, test acc 0.877, time 14.9 sec\n",
      "epoch 20, loss 0.3361, train acc 0.875, test acc 0.880, time 15.0 sec\n",
      "epoch 21, loss 0.3292, train acc 0.877, test acc 0.880, time 15.8 sec\n",
      "epoch 22, loss 0.3241, train acc 0.880, test acc 0.884, time 14.8 sec\n",
      "epoch 23, loss 0.3179, train acc 0.882, test acc 0.880, time 14.6 sec\n",
      "epoch 24, loss 0.3156, train acc 0.883, test acc 0.878, time 16.3 sec\n",
      "epoch 25, loss 0.3108, train acc 0.885, test acc 0.881, time 15.7 sec\n",
      "epoch 26, loss 0.3080, train acc 0.885, test acc 0.888, time 18.8 sec\n",
      "epoch 27, loss 0.3017, train acc 0.889, test acc 0.881, time 20.5 sec\n",
      "epoch 28, loss 0.2976, train acc 0.890, test acc 0.887, time 17.4 sec\n",
      "epoch 29, loss 0.2954, train acc 0.891, test acc 0.888, time 17.2 sec\n",
      "epoch 30, loss 0.2933, train acc 0.891, test acc 0.873, time 17.3 sec\n",
      "epoch 31, loss 0.2912, train acc 0.892, test acc 0.889, time 16.9 sec\n",
      "epoch 32, loss 0.2870, train acc 0.894, test acc 0.893, time 15.9 sec\n",
      "epoch 33, loss 0.2821, train acc 0.896, test acc 0.886, time 16.0 sec\n",
      "epoch 34, loss 0.2810, train acc 0.897, test acc 0.893, time 21.6 sec\n",
      "epoch 35, loss 0.2788, train acc 0.897, test acc 0.896, time 15.8 sec\n",
      "epoch 36, loss 0.2758, train acc 0.898, test acc 0.898, time 19.4 sec\n",
      "epoch 37, loss 0.2724, train acc 0.899, test acc 0.897, time 17.0 sec\n",
      "epoch 38, loss 0.2718, train acc 0.899, test acc 0.894, time 17.3 sec\n",
      "epoch 39, loss 0.2678, train acc 0.901, test acc 0.893, time 15.9 sec\n",
      "epoch 40, loss 0.2659, train acc 0.901, test acc 0.899, time 15.4 sec\n",
      "epoch 41, loss 0.2623, train acc 0.903, test acc 0.891, time 16.6 sec\n",
      "epoch 42, loss 0.2598, train acc 0.905, test acc 0.894, time 18.4 sec\n",
      "epoch 43, loss 0.2587, train acc 0.905, test acc 0.896, time 16.4 sec\n",
      "epoch 44, loss 0.2575, train acc 0.905, test acc 0.899, time 16.8 sec\n",
      "epoch 45, loss 0.2531, train acc 0.906, test acc 0.898, time 17.6 sec\n",
      "epoch 46, loss 0.2522, train acc 0.906, test acc 0.895, time 17.5 sec\n",
      "epoch 47, loss 0.2515, train acc 0.906, test acc 0.901, time 15.8 sec\n",
      "epoch 48, loss 0.2487, train acc 0.908, test acc 0.900, time 15.3 sec\n",
      "epoch 49, loss 0.2465, train acc 0.909, test acc 0.898, time 15.0 sec\n",
      "epoch 50, loss 0.2444, train acc 0.910, test acc 0.898, time 18.5 sec\n",
      "epoch 51, loss 0.2432, train acc 0.910, test acc 0.904, time 18.2 sec\n",
      "epoch 52, loss 0.2411, train acc 0.910, test acc 0.902, time 19.7 sec\n",
      "epoch 53, loss 0.2397, train acc 0.910, test acc 0.903, time 15.6 sec\n",
      "epoch 54, loss 0.2363, train acc 0.912, test acc 0.903, time 15.5 sec\n",
      "epoch 55, loss 0.2362, train acc 0.912, test acc 0.899, time 15.5 sec\n",
      "epoch 56, loss 0.2323, train acc 0.914, test acc 0.899, time 15.4 sec\n",
      "epoch 57, loss 0.2316, train acc 0.914, test acc 0.904, time 16.5 sec\n",
      "epoch 58, loss 0.2280, train acc 0.916, test acc 0.905, time 15.4 sec\n",
      "epoch 59, loss 0.2296, train acc 0.915, test acc 0.898, time 15.7 sec\n",
      "epoch 60, loss 0.2279, train acc 0.916, test acc 0.892, time 16.0 sec\n",
      "epoch 61, loss 0.2254, train acc 0.917, test acc 0.905, time 15.3 sec\n",
      "epoch 62, loss 0.2240, train acc 0.917, test acc 0.897, time 19.4 sec\n",
      "epoch 63, loss 0.2223, train acc 0.918, test acc 0.907, time 18.5 sec\n",
      "epoch 64, loss 0.2222, train acc 0.916, test acc 0.907, time 15.5 sec\n",
      "epoch 65, loss 0.2203, train acc 0.919, test acc 0.906, time 23.3 sec\n",
      "epoch 66, loss 0.2175, train acc 0.920, test acc 0.907, time 16.1 sec\n",
      "epoch 67, loss 0.2174, train acc 0.920, test acc 0.906, time 16.9 sec\n",
      "epoch 68, loss 0.2142, train acc 0.921, test acc 0.904, time 14.9 sec\n",
      "epoch 69, loss 0.2138, train acc 0.920, test acc 0.902, time 15.0 sec\n",
      "epoch 70, loss 0.2112, train acc 0.922, test acc 0.906, time 15.0 sec\n",
      "epoch 71, loss 0.2110, train acc 0.921, test acc 0.907, time 15.0 sec\n",
      "epoch 72, loss 0.2089, train acc 0.923, test acc 0.906, time 15.5 sec\n",
      "epoch 73, loss 0.2078, train acc 0.922, test acc 0.908, time 15.0 sec\n",
      "epoch 74, loss 0.2056, train acc 0.924, test acc 0.909, time 15.2 sec\n",
      "epoch 75, loss 0.2065, train acc 0.923, test acc 0.906, time 15.8 sec\n",
      "epoch 76, loss 0.2031, train acc 0.924, test acc 0.905, time 17.3 sec\n",
      "epoch 77, loss 0.2029, train acc 0.925, test acc 0.907, time 15.1 sec\n",
      "epoch 78, loss 0.2009, train acc 0.926, test acc 0.908, time 15.6 sec\n",
      "epoch 79, loss 0.2000, train acc 0.925, test acc 0.909, time 18.0 sec\n",
      "epoch 80, loss 0.1983, train acc 0.926, test acc 0.906, time 16.5 sec\n",
      "epoch 81, loss 0.1966, train acc 0.927, test acc 0.907, time 15.7 sec\n",
      "epoch 82, loss 0.1957, train acc 0.928, test acc 0.909, time 16.6 sec\n",
      "epoch 83, loss 0.1955, train acc 0.928, test acc 0.908, time 18.2 sec\n",
      "epoch 84, loss 0.1925, train acc 0.928, test acc 0.904, time 16.3 sec\n",
      "epoch 85, loss 0.1899, train acc 0.930, test acc 0.910, time 17.1 sec\n",
      "epoch 86, loss 0.1904, train acc 0.928, test acc 0.911, time 16.4 sec\n",
      "epoch 87, loss 0.1898, train acc 0.929, test acc 0.910, time 16.1 sec\n",
      "epoch 88, loss 0.1870, train acc 0.930, test acc 0.905, time 17.2 sec\n",
      "epoch 89, loss 0.1869, train acc 0.929, test acc 0.908, time 16.3 sec\n",
      "epoch 90, loss 0.1851, train acc 0.931, test acc 0.909, time 15.9 sec\n",
      "epoch 91, loss 0.1839, train acc 0.931, test acc 0.905, time 15.0 sec\n",
      "epoch 92, loss 0.1827, train acc 0.932, test acc 0.908, time 15.0 sec\n",
      "epoch 93, loss 0.1840, train acc 0.932, test acc 0.909, time 15.6 sec\n",
      "epoch 94, loss 0.1815, train acc 0.932, test acc 0.908, time 15.7 sec\n",
      "epoch 95, loss 0.1791, train acc 0.934, test acc 0.910, time 16.2 sec\n",
      "epoch 96, loss 0.1791, train acc 0.932, test acc 0.904, time 16.5 sec\n",
      "epoch 97, loss 0.1773, train acc 0.934, test acc 0.909, time 14.9 sec\n",
      "epoch 98, loss 0.1774, train acc 0.933, test acc 0.908, time 15.0 sec\n",
      "epoch 99, loss 0.1736, train acc 0.935, test acc 0.909, time 15.6 sec\n",
      "epoch 100, loss 0.1743, train acc 0.934, test acc 0.911, time 15.3 sec\n"
     ]
    }
   ],
   "source": [
    "context = get_ctx()\n",
    "lr, num_epochs = 0.9, 100\n",
    "lenet.initialize(force_reinit=True, ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(lenet.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_cnn(lenet, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = nn.Sequential()\n",
    "alexnet.add(nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "        nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。\n",
    "        # 前两个卷积层后不使用池化层来减小输入的高和宽\n",
    "        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
    "        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
    "        nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合\n",
    "        nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "        nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "        # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "        nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv4 output shape:\t (1, 96, 54, 54)\n",
      "pool4 output shape:\t (1, 96, 26, 26)\n",
      "conv5 output shape:\t (1, 256, 26, 26)\n",
      "pool5 output shape:\t (1, 256, 12, 12)\n",
      "conv6 output shape:\t (1, 384, 12, 12)\n",
      "conv7 output shape:\t (1, 384, 12, 12)\n",
      "conv8 output shape:\t (1, 256, 12, 12)\n",
      "pool6 output shape:\t (1, 256, 5, 5)\n",
      "dense6 output shape:\t (1, 4096)\n",
      "dropout0 output shape:\t (1, 4096)\n",
      "dense7 output shape:\t (1, 4096)\n",
      "dropout1 output shape:\t (1, 4096)\n",
      "dense8 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 224, 224))\n",
    "alexnet.initialize(force_reinit=True)\n",
    "for layer in alexnet:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-21:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# 如出现“out of memory”的报错信息，可减小batch_size或resize\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6c90964f7fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0malexnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_reinit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXavier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malexnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malexnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-58459d6bc941>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[0;34m(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtrain_acc_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Learning/EnvLearning/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2014\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Learning/EnvLearning/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1994\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1996\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1997\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "alexnet.initialize(force_reinit=True, ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(alexnet.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_cnn(alexnet, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG\n",
    "def vgg_block(num_convs, num_channels):\n",
    "    block = nn.Sequential()\n",
    "    for _ in range(num_convs):\n",
    "        block.add(nn.Conv2D(\n",
    "            num_channels, kernel_size=3, padding=1, activation='relu'))\n",
    "    block.add(nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    net = nn.Sequential()\n",
    "    # 卷积层部分\n",
    "    for (num_convs, num_channels) in conv_arch:\n",
    "        net.add(vgg_block(num_convs, num_channels))\n",
    "    # 全连接层部分\n",
    "    net.add(nn.Dense(4096, activation='relu'), nn.Dropout(0.5),\n",
    "            nn.Dense(4096, activation='relu'), nn.Dropout(0.5),\n",
    "            nn.Dense(10))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n",
    "vggnet = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential7 output shape:\t (1, 64, 112, 112)\n",
      "sequential8 output shape:\t (1, 128, 56, 56)\n",
      "sequential9 output shape:\t (1, 256, 28, 28)\n",
      "sequential10 output shape:\t (1, 512, 14, 14)\n",
      "sequential11 output shape:\t (1, 512, 7, 7)\n",
      "dense9 output shape:\t (1, 4096)\n",
      "dropout2 output shape:\t (1, 4096)\n",
      "dense10 output shape:\t (1, 4096)\n",
      "dropout3 output shape:\t (1, 4096)\n",
      "dense11 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "vggnet.initialize(force_reinit=True)\n",
    "X = nd.random.uniform(shape=(1, 1, 224, 224))\n",
    "for blk in vggnet:\n",
    "    X = blk(X)\n",
    "    print(blk.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(num_channels, kernel_size, strides, padding):\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.Conv2D(num_channels, kernel_size,\n",
    "                      strides, padding, activation='relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1, activation='relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1, activation='relu'))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "netinet = nn.Sequential()\n",
    "netinet.add(nin_block(96, kernel_size=11, strides=4, padding=0),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        nin_block(256, kernel_size=5, strides=1, padding=2),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        nin_block(384, kernel_size=3, strides=1, padding=1),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2), nn.Dropout(0.5),\n",
    "        # 标签类别数是10\n",
    "        nin_block(10, kernel_size=3, strides=1, padding=1),\n",
    "        # 全局平均池化层将窗口形状自动设置成输入的高和宽\n",
    "        nn.GlobalAvgPool2D(),\n",
    "        # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n",
    "        nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential13 output shape:\t (1, 96, 54, 54)\n",
      "pool12 output shape:\t (1, 96, 26, 26)\n",
      "sequential14 output shape:\t (1, 256, 26, 26)\n",
      "pool13 output shape:\t (1, 256, 12, 12)\n",
      "sequential15 output shape:\t (1, 384, 12, 12)\n",
      "pool14 output shape:\t (1, 384, 5, 5)\n",
      "dropout4 output shape:\t (1, 384, 5, 5)\n",
      "sequential16 output shape:\t (1, 10, 5, 5)\n",
      "pool15 output shape:\t (1, 10, 1, 1)\n",
      "flatten0 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 224, 224))\n",
    "netinet.initialize(force_reinit=True)\n",
    "for layer in netinet:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 5, 128\n",
    "net.initialize(force_reinit=True, ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "train_cnn(netinet, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Block):\n",
    "    \"\"\"\n",
    "    c1 - c4为每条线路里的层的输出通道数\n",
    "    这种forward 怎么反向计算\n",
    "    \"\"\"\n",
    "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # 线路1，单1 x 1卷积层\n",
    "        self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')\n",
    "        # 线路2，1 x 1卷积层后接3 x 3卷积层\n",
    "        self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')\n",
    "        self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1,\n",
    "                              activation='relu')\n",
    "        # 线路3，1 x 1卷积层后接5 x 5卷积层\n",
    "        self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')\n",
    "        self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2,\n",
    "                              activation='relu')\n",
    "        # 线路4，3 x 3最大池化层后接1 x 1卷积层\n",
    "        self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1_1(x)\n",
    "        p2 = self.p2_2(self.p2_1(x))\n",
    "        p3 = self.p3_2(self.p3_1(x))\n",
    "        p4 = self.p4_2(self.p4_1(x))\n",
    "        return nd.concat(p1, p2, p3, p4, dim=1)  # 在通道维上连结输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GoogleNet: 主体卷积部分有5个 模块\n",
    "block_1 = nn.Sequential()\n",
    "block_1.add(\n",
    "    nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_2 = nn.Sequential()\n",
    "block_2.add(\n",
    "    nn.Conv2D(64, kernel_size=1, activation='relu'),\n",
    "    nn.Conv2D(192, kernel_size=3, padding=1, activation='relu'),  # 通道数增大3倍\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三模块: 2个inception\n",
    "block_3 = nn.Sequential()\n",
    "block_3.add(\n",
    "    Inception(64, (96, 128), (16, 32), 32),\n",
    "    Inception(128, (128, 192), (32, 96), 64),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_4 = nn.Sequential()\n",
    "block_4.add(\n",
    "    Inception(192, (96, 208), (16, 48), 64),\n",
    "    Inception(160, (112, 224), (24, 64), 64),\n",
    "    Inception(128, (128, 256), (24, 64), 64),\n",
    "    Inception(112, (144, 288), (32, 64), 64),\n",
    "    Inception(256, (160, 320), (32, 128), 128),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_5 = nn.Sequential()\n",
    "block_5.add(\n",
    "    Inception(256, (160, 320), (32, 128), 128),\n",
    "    Inception(384, (192, 384), (48, 128), 128),\n",
    "    nn.GlobalAvgPool2D()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_net = nn.Sequential()\n",
    "google_net.add(\n",
    "    block_1,\n",
    "    block_2,\n",
    "    block_3,\n",
    "    block_4,\n",
    "    block_5,\n",
    "    nn.Dense(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential26 output shape:\t (1, 64, 24, 24)\n",
      "sequential27 output shape:\t (1, 192, 12, 12)\n",
      "sequential28 output shape:\t (1, 480, 6, 6)\n",
      "sequential29 output shape:\t (1, 832, 3, 3)\n",
      "sequential30 output shape:\t (1, 1024, 1, 1)\n",
      "dense14 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 96, 96))\n",
    "google_net.initialize(force_reinit=True)\n",
    "for layer in google_net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不敢执行, 悲哀\n",
    "lr, num_epochs, batch_size = 0.1, 5, 128\n",
    "google_net.initialize(force_reinit=True, ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(google_net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)\n",
    "train_cnn(net, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch normalization\n",
    "# 批量归一化 的 backward怎么算\n",
    "# BN from scratch, without mxnet BacthNorm Class\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    \"\"\"\n",
    "    通过autograd来判断当前模式是训练模式还是预测模式\n",
    "    在预测模式下, 使用移动平均和 移动方差, 保证模型对于任意输入都有稳定的输出\n",
    "    :param X: \n",
    "    :param gamma: 拉伸系数\n",
    "    :param beta: 偏移系数\n",
    "    :param moving_mean: 移动平均, 估算整个训练数据集的样本均值\n",
    "    :param moving_var: 移动方差\n",
    "    :param eps: 小数, 避免分母为0\n",
    "    :parma momentum: \n",
    "    \"\"\"\n",
    "    if not autograd.is_training():\n",
    "        # 在 预测 测试集时(预测模式), 直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / nd.sqrt(moving_var + eps)\n",
    "        return gamma * X_hat, moving_mean, moving_var   # 拉伸和偏移\n",
    "    if len(X.shape) == 2:\n",
    "        # 全连接层\n",
    "        mean = X.mean(axis=0)  # 列\n",
    "        var = ((X - mean) ** 2).mean(axis=0)\n",
    "    elif len(X.shape) == 4:\n",
    "        mean = X.mean(axis=(0,2,3), keepdims=True)\n",
    "        var = ((X - mean) ** 2).mean(axis=(0,2,3), keepdims=True)\n",
    "    else:\n",
    "        raise Exception(\"Unkown X shape: {}\".format(X.shape))\n",
    "    X_hat = (X - mean) / nd.sqrt(var + eps)\n",
    "    # 更新移动平均的均值和方差做标准化\n",
    "    moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "    moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    return gamma * X_hat, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Block):\n",
    "    def __init__(self, num_features, num_dims, **kwargs):\n",
    "        super(BatchNorm, self).__init__(**kwargs)\n",
    "        if num_dims == 2:  # 全连接层\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            # 卷积层\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0\n",
    "        self.gamma = self.params.get('gamma', shape=shape, init=init.One())\n",
    "        self.beta = self.params.get('beta', shape=shape, init=init.Zero())\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = nd.zeros(shape)\n",
    "        self.moving_var = nd.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.context != X.context:\n",
    "            self.moving_mean = self.moving_mean.copyto(X.context)\n",
    "            self.moving_var = self.moving_var.copyto(X.context)\n",
    "        # 保存更新过的moving_mean和moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma.data(), self.beta.data(), self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_with_bn = nn.Sequential()\n",
    "lenet_with_bn.add(\n",
    "    # 卷积层\n",
    "    nn.Conv2D(6, kernel_size=5),\n",
    "    BatchNorm(6, num_dims=4),  # 也可以使用 nn.BatchNorm()\n",
    "    nn.Activation('sigmoid'),\n",
    "    nn.MaxPool2D(pool_size=2, strides=2),\n",
    "    \n",
    "    nn.Conv2D(16, kernel_size=5),\n",
    "    BatchNorm(16, num_dims=4),\n",
    "    nn.Activation('sigmoid'),\n",
    "    nn.MaxPool2D(pool_size=2, strides=2),\n",
    "    # 全连接层\n",
    "    nn.Dense(120),\n",
    "    BatchNorm(120, num_dims=2),\n",
    "    nn.Activation('sigmoid'),\n",
    "    nn.Dense(84),\n",
    "    BatchNorm(84, num_dims=2),\n",
    "    nn.Activation('sigmoid'),\n",
    "    nn.Dense(10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 1.0, 5, 256\n",
    "net.initialize(ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "train_cnn(lenet_with_bn, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Block):\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,\n",
    "                               strides=strides)\n",
    "        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\n",
    "        self.conv3 = None\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2D(num_channels, kernel_size=1, strides=strides)\n",
    "        self.bn1 = nn.BatchNorm()\n",
    "        self.bn2 = nn.BatchNorm()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = nd.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return nd.relu(Y + X)\n",
    "\n",
    "    \n",
    "def resnet_block(num_channels, num_residuals, first_block=False):\n",
    "    blk = nn.Sequential()\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.add(Residual(num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_block = Residual(3)\n",
    "res_block.initialize()\n",
    "X = nd.random.uniform(shape=(4, 3, 6, 6))\n",
    "res_block(X).shape == X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6, 3, 3)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 增加输出通道数, 并且减半宽和高, 使用1x1卷积层resize\n",
    "res_block = Residual(6, use_1x1conv=True, strides=2)\n",
    "res_block.initialize()\n",
    "res_block(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet模型\n",
    "res_net = nn.Sequential()\n",
    "res_net.add(\n",
    "    nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n",
    "    nn.BatchNorm(), \n",
    "    nn.Activation('relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1),\n",
    "    resnet_block(64, 2, first_block=True),\n",
    "    resnet_block(128, 2),\n",
    "    resnet_block(256, 2),\n",
    "    resnet_block(512, 2),\n",
    "    nn.GlobalAvgPool2D(),  # 全局平均层\n",
    "    nn.Dense(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.05, 5, 256\n",
    "res_net.initialize(force_reinit=True, ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(res_net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "train_cnn(res_net, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 稠密连接网络\n",
    "def conv_block(num_channels):\n",
    "    \"\"\"\n",
    "    改良版的 \"卷积 + BN + 激活\"\n",
    "    \"\"\"\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.BatchNorm(), nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=3, padding=1))\n",
    "    return blk\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Block):\n",
    "    \"\"\"\n",
    "    稠密块\n",
    "    \"\"\"\n",
    "    def __init__(self, num_convs, num_channels, **kwargs):\n",
    "        super(DenseBlock, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "        for _ in range(num_convs):\n",
    "            self.net.add(conv_block(num_channels))\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            X = nd.concat(X, Y, dim=1)  # 在通道维上将输入和输出连结\n",
    "        return X\n",
    "\n",
    "    \n",
    "def transition_block(num_channels):\n",
    "    \"\"\"\n",
    "    过渡层\n",
    "    \"\"\"\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.BatchNorm(), nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1),\n",
    "            nn.AvgPool2D(pool_size=2, strides=2))\n",
    "    return blk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 23, 8, 8)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_block = DenseBlock(2, 10)\n",
    "dense_block.initialize()\n",
    "X = nd.random.uniform(shape=(4, 3, 8, 8))\n",
    "Y = dense_block(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10, 4, 4)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tran_block = transition_block(10)\n",
    "tran_block.initialize()\n",
    "tran_block(Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels, growth_rate = 64, 32  # num_channels为当前的通道数\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "\n",
    "dense_net = nn.Sequential()\n",
    "dense_net.add(\n",
    "    nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n",
    "    nn.BatchNorm(), \n",
    "    nn.Activation('relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    dense_net.add(DenseBlock(num_convs, growth_rate))\n",
    "    # 上一个稠密块的输出通道数\n",
    "    num_channels += num_convs * growth_rate\n",
    "    # 在稠密块之间加入通道数减半的过渡层\n",
    "    if i != len(num_convs_in_dense_blocks) - 1:\n",
    "        num_channels //= 2\n",
    "        dense_net.add(transition_block(num_channels))\n",
    "dense_net.add(\n",
    "    nn.BatchNorm(), \n",
    "    nn.Activation('relu'), \n",
    "    nn.GlobalAvgPool2D(),\n",
    "    nn.Dense(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 5, 256\n",
    "dense_net.initialize(ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(dense_net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)\n",
    "train_cnn(dense_net, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习\n",
    "1. 尝试基于LeNet构造更复杂的网络来提高分类准确率:\n",
    "    - 调整卷积窗口大小\n",
    "    - 输出通道数\n",
    "    - 激活函数和全连接层输出个数\n",
    "    - 在优化方面:\n",
    "        - 尝试不同的学习率、\n",
    "        - 初始化方法\n",
    "        - 增加迭代周期\n",
    "2. 尝试增加迭代周期。跟LeNet的结果相比，AlexNet的结果有什么区别？为什么？\n",
    "    - Mac跑不动\n",
    "3. AlexNet对Fashion-MNIST数据集来说可能过于复杂。试着简化模型来使训练更快，同时保证准确率不明显下降\n",
    "4. 修改批量大小, 观察准确率和内存或显存的变化\n",
    "5. 为什么NiN块里要有两个 1×1 卷积层？去除其中的一个，观察并分析实验现象\n",
    "6. 对比AlexNet、VGG和NiN、GoogLeNet的模型参数尺寸。为什么后两个网络可以显著减小模型参数尺寸？\n",
    "7. GoogLeNet有数个后续版本。尝试实现并运行它们，然后观察实验结果。后续版本:\n",
    "    - 加入批量归一化层\n",
    "    - 对Inception块做调整\n",
    "    - 加入残差连接\n",
    "8. 能否将批量归一化前的 全连接层/卷积层中的bias参数去掉? 为什么?\n",
    "    - 去掉bias, 会导致 均值减少bias, 方差不变\n",
    "9. 和LeNet 相比, 加了BN是不是可以调大learning rate?\n",
    "10. 尝试将BN层加到LeNet的其他地方, 看看有什么变化\n",
    "11. 如果不学习拉伸参数和偏移参数, 传参`grad_req='null'`可以避免计算梯度, 结果有什么变化?\n",
    "12. 如何在训练时使用基于全局平均的均值和方差?\n",
    "13. 对于比较深的网络， ResNet论文中介绍了一个“瓶颈”架构来降低模型复杂度。尝试实现它.\n",
    "14. 在ResNet的后续版本里，作者将残差块里的“卷积、批量归一化和激活”结构改成了“批量归一化、激活和卷积”，实现这个改进.\n",
    "15. DenseNet论文中提到的一个优点是模型参数比ResNet的更小，这是为什么 ?\n",
    "16. DenseNet被人诟病的一个问题是内存或显存消耗过多。真的会这样吗？可以把输入形状换成 224×224 ，来看看实际的消耗。\n",
    "17. 实现DenseNet论文中的表1提出的不同版本的DenseNet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
