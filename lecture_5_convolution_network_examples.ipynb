{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 那些年, 大牛的卷积网络\n",
    "1. 卷积网络:\n",
    "    - 尝试解决全连接层的局限:\n",
    "        - 因为全连接层按行展开, 难于识别相距较远的 模式 -> 卷积层保留输入形状, 使得图像的像素在高和宽两个方向上的相关性可能被识别;\n",
    "        - 全连接层模型容易过大 -> 卷积层通过滑动窗口, 和不同位置的输入做卷积计算, 避免参数尺寸过大\n",
    "\n",
    "### Lenet\n",
    "0. 1998年\n",
    "1. 交叉使用两个模块: 卷积层 + 全连接层\n",
    "2. 卷积层基本单位 = 卷积层 + 最大池化层, 卷积层识别空间模式, 池化层降低卷积层对位置的敏感性\n",
    "3. 结构:\n",
    "    - 卷积层块:\n",
    "        - 卷积层: 5x5 -> 输出 -> sigmoid\n",
    "        - 1st 卷积层 通道为 5, 2nd 为16 <- 增加通道使两个卷积层的参数尺寸类似\n",
    "        - 两个 2x2 的max-pooling窗口, 步幅是2\n",
    "    - 输出: (批量大小, 通道数, 高, 宽)\n",
    "    - 进入全连接层: flatten 每个样本, 输入形状变成二维 -> 样本数 * flatten vectors\n",
    "    - 三个全连接层: 120, 84, 10(输出类别个数, 因为是MNIST)\n",
    "4. 缺点:\n",
    "    - 多通道, 多层, 大量参数, 如今有GPU就不是问题\n",
    "    \n",
    "### AlexNet\n",
    "1. end-to-end: 神经网络可以直接基于图像的原始像素进行分类\n",
    "2. 那些年图像分类的特征工程, 流程:\n",
    "    - 获取图像数据集\n",
    "    - 使用特征提取函数生成图像的特征\n",
    "    - 使用机器学习模型对图像的特征分类\n",
    "3. 学习特征表示:\n",
    "    - 多层神经网络可能可以学习到数据的 多级表征, 并逐级表示逐渐抽象的概念或模式\n",
    "4. AlexNet 首次证明了学习到的特征可以超越手工设计的特征\n",
    "5. 与leNet的不用:\n",
    "    - 8层变换:\n",
    "        - 5层卷积\n",
    "        - 2层全连接隐藏层\n",
    "        - 1层全连接输出层\n",
    "    - 将sigmoid激活函数改成relu激活函数, 后者计算更简单\n",
    "        - sigmoid 的缺点: 当输出极限接近0或者1时, 梯度几乎为0, 导致反向传播无法更新部分模型参数\n",
    "    - 通过丢弃法控制全连接层的模型复杂福\n",
    "    - 引入大量图像增广: 翻转/裁剪/颜色变化, 进一步扩大数据集来缓解过拟合\n",
    "6. 结构:\n",
    "    - 第一层: 11x11的卷积窗口 -> 需要大窗口捕获物体\n",
    "    - 第二层: 5x5的卷积窗口\n",
    "    - 第三及之后层: 3x3的卷积窗口\n",
    "    - 第1/2/5层都使用了3x3, 步幅为2的最大池化层\n",
    "    - 卷积层之后是两个 输出个数为4096的全连接层\n",
    "9. 结构图: ![](./images/alexnet_structure.png)\n",
    "\n",
    "### VGG\n",
    "1. 实验室: Visual Geometry Group\n",
    "2. 提出了通过重复使用简单的基础块来构建深度模型的思路\n",
    "3. 结构:\n",
    "    - 连续使用数个相同的填充为1、窗口形状为 3×3 的卷积层后接一个步幅为2、窗口形状为 2×2 的最大池化层\n",
    "    - 卷积层保持输入的高和宽不变\n",
    "    - 池化层则对宽高减半\n",
    "    - 卷积模块后接全连接层\n",
    "\n",
    "### NiN, 网中网\n",
    "1. LeNet/AlexNet/VGG 的共同之处, 是 先使用卷积层构成的模块充分抽取空间特征, 再以 全连接层 构成的模块来输出分类 结果\n",
    "2. AlexNet/VGG的改进之处在于, 将两个模块加厚(增加通道数), 加深\n",
    "3. NiN的思路: 串联多个 卷积层+全连接层 的小网络来构建 一个深层网络\n",
    "4. NiN使用 1x1卷积层来代替全连接层, 从而将空间信息传递到后面的层, 其中, 1x1卷积层空间维度(宽+高)上的每个元素相当于样本, 通道相当于特征\n",
    "5. NiN块: 1个卷积层 + 2个 1x1卷积层\n",
    "6. NiN结构:\n",
    "    - 卷积层窗口形状 分别是 11, 5, 3\n",
    "    - 通道数 和 alexnet 相同\n",
    "    - 每个 NiN块 后接一个 步幅为2, 窗口形状为3x3的最大池化层\n",
    "7. 和 AlexNet相比, 去掉了 最后三个全连接层, 最后的是 输出通道数 等于 标签类别数的 NiN 块 + 全局平均池化层\n",
    "8. 优点: 显著建雄模型参数尺寸, 缓解过拟合\n",
    "9. 缺点: 可能会导致获取有效模型的训练时间增加\n",
    "10. <b>这是一个  参数数量 和 训练时间 的博弈</b>\n",
    "11. 结构图: ![](./images/nin.png)\n",
    "\n",
    "### GoogLeNet(含并行连接)\n",
    "1. GoogLeNet吸收了NiN中网络串联网络的思想\n",
    "2. Inception 块: 4条并行线路\n",
    "    - 1x1 卷积层\n",
    "    - 1x1 卷积层 + 3x3 卷积层(3和5抽取不同空间尺寸的信息)\n",
    "    - 1x1 卷积层 + 5x5 卷积层\n",
    "    - 3x3 最大池化层 + 1x1 卷积层(减少通道数, 降低复杂度)\n",
    "    - 输出: 通道合并层\n",
    "3. Inception 块结构: ![](./images/inception.png)\n",
    "4. GoogLeNet 结构:\n",
    "    - 主体卷积部分:\n",
    "        - 模块1: 卷积层(64通道, 7x7) + 最大池化层(步幅=2, 3x3)\n",
    "        - 模块2: 卷积层(64通道, 1x1) + 卷积层(192通道, 3x3) + 最大池化层(步幅=2, 充填=1, 3x3) , 对应 Inception的第二条线路\n",
    "        - 模块3: 串联两个 Inception块 + 最大池化层(步幅=2, 填充=1, 3x3):\n",
    "            - 第一个Inception, 输出通道数为256: \n",
    "                - 64\n",
    "                - 96, 128\n",
    "                - 16, 32\n",
    "                - 32\n",
    "            - 第二个Inception, 输出通道数为480:\n",
    "                - 128\n",
    "                - 128, 192\n",
    "                - 32, 96\n",
    "                - 64\n",
    "        - 模块4: 5个Inception块 + 最大池化层(步幅=2, 填充=1, 3x3)\n",
    "            - 第1个Inception, 输出通道数为512\n",
    "                - 192\n",
    "                - 96, 208\n",
    "                - 16, 48\n",
    "                - 64\n",
    "            - 第2个Inception, 输出通道数为512\n",
    "                - 160\n",
    "                - 112, 224\n",
    "                - 24, 64\n",
    "                - 64\n",
    "            - 第3个Inception, 输出通道数为512\n",
    "                - 128\n",
    "                - 128, 256\n",
    "                - 24, 64\n",
    "                - 64\n",
    "            - 第4个Inception, 输出通道数为528\n",
    "                - 112\n",
    "                - 144, 288\n",
    "                - 32, 64\n",
    "                - 64\n",
    "            - 第5个Inception, 输出通道数为832\n",
    "                - 256\n",
    "                - 160, 320\n",
    "                - 32, 128\n",
    "                - 128\n",
    "5. Inception块的通道数分配之比 需要通过大量实验获取\n",
    "\n",
    "        \n",
    "### ResNet, 残差网络\n",
    "1. 加入BN依旧未完全解决: more 层, 训练稳定的深度模型越困难, 训练误差反而可能上升\n",
    "2. 加入残差网络: 输入可以跨层向前传播\n",
    "3. 之前的映射: f(x); 新的残差映射: f(x) - x\n",
    "4. 残差块 Residual Block, 结构:\n",
    "    - 两个相同输出通道数的 3x3 卷积层 (输出形状同输入)\n",
    "    - 每个卷积层后 跟着 BN\n",
    "    - 加上 输入\n",
    "    - 加上ReLU\n",
    "    - 如果要改变 输出通道数, 则需要使用1x1 卷积层resize输入再做相加运算\n",
    "5. ResNet模型\n",
    "    - 卷积层(64通道, 步幅为2, 7x7) + BN + 最大池化层(步幅为2, 3x3)\n",
    "    - 4个残差块\n",
    "6. 相比 GoogLeNet, 结构更简单\n",
    "### DenseNet\n",
    "1. 从ResNet引申出来, 但是 ResNet加上输入, 而DenseNet连结输入\n",
    "2. DenseNet结构:\n",
    "    - 稠密块: 定义输出和输入如何连结(在通道维)\n",
    "        - 卷积层 + BN + ReLU\n",
    "    - 过渡层: 控制通道数(控制模型复杂度), 避免过大\n",
    "        - 1x1卷积层, 减少 通道数\n",
    "        - 平均池化层(步幅=2), 减半宽和高\n",
    "3. 增长率(Grow Rate): 输出通道数 相对 输入通道数的增长\n",
    "4. 卷积块的通道数 控制 增长率\n",
    "5. DenseNet模型结构:\n",
    "    - 单卷积层\n",
    "    - 最大池化层\n",
    "    - 4个稠密块\n",
    "    - 全局平均池化层\n",
    "    - 全连接层\n",
    "    \n",
    "### 批量归一化\n",
    "1. 如果没有BN: 每层参数的更新难以导致靠近输出层的输出有剧烈变化, 这种数值的不稳定性会导致难以训练处有效的深度模型\n",
    "2. 如果加了BN: \n",
    "    - BN利用小批量上的均值和标准差, 不断调整神经网络的中间输出, 使用整个神经网络在各层的中间输出的数值更稳定\n",
    "    - 更快地收敛\n",
    "3. 对全连接层进行BN\n",
    "    - 在激活函数之前\n",
    "    - 对 Wx+b 求均值u和方差σ: \n",
    "    - 归一化时, 分母添加小常数, 避免 分母为0: y = (y - u) / sqrt(σ ^ 2, ϵ)\n",
    "    - 拉伸: 按元素乘一个参数, 按元素加一个参数, 这两个参数可以训练, 可以使得BN效果消失\n",
    "4. 对卷积层进行BN\n",
    "    - 独立对每个通道做BN: 每个通道有独立的拉伸和偏移参数\n",
    "5. 预测时的BN(用已有模型预测测试集)\n",
    "\n",
    "### 结语\n",
    "#### 吃透这几个Net\n",
    "\n",
    "\n",
    "### 参考\n",
    "- [Gradient-based learning applied to document recognition](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)\n",
    "- [Imagenet classification with deep convolutional neural networks.](http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf)\n",
    "- [Very deep convolutional networks for large-scale image recognition.](https://arxiv.org/abs/1409.1556)\n",
    "- 新加坡国立大学, [Network in Network](https://arxiv.org/pdf/1312.4400.pdf)\n",
    "- [Going deeper with convolutions](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf)\n",
    "- 批量归一化, [Batch normalization: Accelerating deep network training by reducing internal covariate shift](https://arxiv.org/pdf/1502.03167.pdf)\n",
    "- [Identity mappings in deep residual networks](https://arxiv.org/pdf/1603.05027.pdf)\n",
    "- [Deep residual learning for image recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "import mxnet as mx\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn, data as gdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = nn.Sequential()\n",
    "lenet.add(nn.Conv2D(channels=6, kernel_size=5, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        # Dense会默认将(批量大小, 通道, 高, 宽)形状的输入转换成\n",
    "        # (批量大小, 通道 * 高 * 宽)形状的输入\n",
    "        nn.Dense(120, activation='sigmoid'),\n",
    "        nn.Dense(84, activation='sigmoid'),\n",
    "        nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2 output shape:\t (1, 6, 24, 24)\n",
      "pool2 output shape:\t (1, 6, 12, 12)\n",
      "conv3 output shape:\t (1, 16, 8, 8)\n",
      "pool3 output shape:\t (1, 16, 4, 4)\n",
      "dense3 output shape:\t (1, 120)\n",
      "dense4 output shape:\t (1, 84)\n",
      "dense5 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 28, 28))\n",
    "lenet.initialize()\n",
    "for layer in lenet:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctx():\n",
    "    \"\"\"\n",
    "    GPU first\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.zeros((1,), ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, ctx):\n",
    "    acc_sum, n = nd.array([0], ctx=ctx), 0\n",
    "    for X, y in data_iter:\n",
    "        # 如果ctx代表GPU及相应的显存，将数据复制到显存上\n",
    "        X, y = X.as_in_context(ctx), y.as_in_context(ctx).astype('float32')\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.size\n",
    "    return acc_sum.asscalar() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs):\n",
    "    print('training on', ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec' % (\n",
    "            epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_workers(num_workers=4):\n",
    "    # 0 means no additional process is used to speed up the reading of data.\n",
    "    if sys.platform.startswith('win'):\n",
    "        return 0\n",
    "    else:\n",
    "        return num_workers\n",
    "    \n",
    "    \n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n",
    "    dataset = gluon.data.vision\n",
    "    trans = [dataset.transforms.Resize(resize)] if resize else []\n",
    "    trans.append(dataset.transforms.ToTensor())\n",
    "    trans = dataset.transforms.Compose(trans)\n",
    "    mnist_train = dataset.FashionMNIST(train=True).transform_first(trans)\n",
    "    mnist_test = dataset.FashionMNIST(train=False).transform_first(trans)\n",
    "    return (gluon.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                                  num_workers=get_dataloader_workers()),\n",
    "            gluon.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                                  num_workers=get_dataloader_workers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gluon.data.vision.FashionMNIST?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "\n",
    "\n",
    "def get_fashion_mnist_label(label):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return text_labels[int(label)]\n",
    "\n",
    "\n",
    "def show_fashion_mnist(images, labels):\n",
    "    display.set_matplotlib_formats('svg')  # 用grey 就不行\n",
    "    _, figs = plt.subplots(1, len(images), figsize=(20, 20))\n",
    "    for f, img, lbl in zip(figs, images, labels):\n",
    "        f.imshow(img.reshape((28, 28)).asnumpy())\n",
    "        f.set_title(lbl)\n",
    "        f.axes.get_xaxis().set_visible(False)\n",
    "        f.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"146.50965pt\" version=\"1.1\" viewBox=\"0 0 1357.1 146.50965\" width=\"1357.1pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 146.50965 \n",
       "L 1357.1 146.50965 \n",
       "L 1357.1 -0 \n",
       "L 0 -0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 10.7 135.80965 \n",
       "L 124.191525 135.80965 \n",
       "L 124.191525 22.318125 \n",
       "L 10.7 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pc50bd7c825)\">\n",
       "    <image height=\"114\" id=\"image1514861231\" transform=\"scale(1 -1)translate(0 -114)\" width=\"114\" x=\"10.7\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-21.80965\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 10.7 135.80965 \n",
       "L 10.7 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 124.191525 135.80965 \n",
       "L 124.191525 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 10.7 135.80965 \n",
       "L 124.191525 135.80965 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 10.7 22.318125 \n",
       "L 124.191525 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    <!-- ankle boot -->\n",
       "    <defs>\n",
       "     <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "     <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "     <path d=\"M 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 31.109375 \n",
       "L 44.921875 54.6875 \n",
       "L 56.390625 54.6875 \n",
       "L 27.390625 29.109375 \n",
       "L 57.625 0 \n",
       "L 45.90625 0 \n",
       "L 18.109375 26.703125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-107\"/>\n",
       "     <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "     <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "     <path id=\"DejaVuSans-32\"/>\n",
       "     <path d=\"M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "M 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "z\n",
       "\" id=\"DejaVuSans-98\"/>\n",
       "     <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "     <path d=\"M 18.3125 70.21875 \n",
       "L 18.3125 54.6875 \n",
       "L 36.8125 54.6875 \n",
       "L 36.8125 47.703125 \n",
       "L 18.3125 47.703125 \n",
       "L 18.3125 18.015625 \n",
       "Q 18.3125 11.328125 20.140625 9.421875 \n",
       "Q 21.96875 7.515625 27.59375 7.515625 \n",
       "L 36.8125 7.515625 \n",
       "L 36.8125 0 \n",
       "L 27.59375 0 \n",
       "Q 17.1875 0 13.234375 3.875 \n",
       "Q 9.28125 7.765625 9.28125 18.015625 \n",
       "L 9.28125 47.703125 \n",
       "L 2.6875 47.703125 \n",
       "L 2.6875 54.6875 \n",
       "L 9.28125 54.6875 \n",
       "L 9.28125 70.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-116\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(35.722638 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"61.279297\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-107\"/>\n",
       "     <use x=\"182.568359\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"210.351562\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"271.875\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"303.662109\" xlink:href=\"#DejaVuSans-98\"/>\n",
       "     <use x=\"367.138672\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"428.320312\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"489.501953\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 146.889831 135.80965 \n",
       "L 260.381356 135.80965 \n",
       "L 260.381356 22.318125 \n",
       "L 146.889831 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p3c0728fb39)\">\n",
       "    <image height=\"114\" id=\"image5d373d8f02\" transform=\"scale(1 -1)translate(0 -114)\" width=\"114\" x=\"146.889831\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-21.80965\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 146.889831 135.80965 \n",
       "L 146.889831 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 260.381356 135.80965 \n",
       "L 260.381356 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 146.889831 135.80965 \n",
       "L 260.381356 135.80965 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 146.889831 22.318125 \n",
       "L 260.381356 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_2\">\n",
       "    <!-- t-shirt -->\n",
       "    <defs>\n",
       "     <path d=\"M 4.890625 31.390625 \n",
       "L 31.203125 31.390625 \n",
       "L 31.203125 23.390625 \n",
       "L 4.890625 23.390625 \n",
       "z\n",
       "\" id=\"DejaVuSans-45\"/>\n",
       "     <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "     <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-104\"/>\n",
       "     <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "     <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(185.704968 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-45\"/>\n",
       "     <use x=\"75.292969\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"127.392578\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "     <use x=\"190.771484\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"218.554688\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"259.667969\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_3\">\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 283.079661 135.80965 \n",
       "L 396.571186 135.80965 \n",
       "L 396.571186 22.318125 \n",
       "L 283.079661 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p6b517d99eb)\">\n",
       "    <image height=\"114\" id=\"imagea8db3e5a5b\" transform=\"scale(1 -1)translate(0 -114)\" width=\"114\" x=\"283.079661\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAHIAAAByCAYAAACP3YV9AAAABHNCSVQICAgIfAhkiAAAA6BJREFUeJztnU1Ik3Ecx5//8zxTZ1uGmtqkBZWI9KKHPATdtCgPERlYEF2qU0FBL5dOea4uQbegiyevFRF2yqwOkUFQFtiLJQSymXPOue1Z177/hevFqH35fm4f92w8ez7778fzODbTaw4VHVHxuP96B8TKoJAkKCQJCkmCQpKgkCQoJAkKSYJCkqCQJCgkCQpJgkKSoJAkKCQJCkmCQpKgkCQoJAkKSYJCkqCQJCgkCQpJgkKSoJAkKCQJCkmCQpKgkCQoJAkKSYJCkqCQJCgkCQpJgkKSoJAkKCQJCkmC/6cPYEJV4G5kFW5QFbK2R3ccxylml9BTKfR8flmvRIxf5tCb5ddYMYfHTCuSBIUkQSFJ8HN7dsAfpnqsmbYhDb69dRq8q+4T+GKQAd8axtujHt7uOI6TDqrBU4UweKKAc3cmF0XPRsCzAc6f/Y3PwatMAbxQxNfzveQ26/E88GoX77/az1i3l87wiJcFr7U8ZO1TjcmBN/jz4COzW8C1IklQSBIUkgQzMHYSvnj31LoHsMGdr13gkwuN4G8T6PvWvwIferwTfE1srmQn5tM14E31uM3aMM7pg83PwB/NtYEfaXgKni7iua49E1MBzmR7Zjd4OJ9sbic6wb9koiXbzCzgnE8v4j5lLQ/yBty4+P3I7jQeM61IEhSSBIUkwezddB7efAtTeJ5oX9Mrx82Po+DH47t+c9d+nmxfN/jhK3fBr47vBq8J43PKLOBMjDXOgve0TICPdeI8+x/QiiRBIUlQSBL8/OT7FX3A/ksXwC++GQIfvHG05D6xkQR48PI1uNeB54nJawH45bZb4KeHT4D7i3hOlm7CmRhtxfPWA60vwO931Vt7/Bf+H+p65bf5DuPic9KKJEEhSVBIEkyvNwDnkfZ7b9nPxxjc3iniNUG/pRl84tzGkoc404fnfbFQErw/gjOs/eEx8Ph1fD26o+Pg9ox9N4jXKZc+43XQzWeflOzjL2Efkx9RXNnfltOKJEEhSVBIEkwl/BCo/dnZoLsDPNleC75UhzPK+giPEx+eAs9/QK8IjM4jKVFIEhSShIqYkaI8WpEkKCQJCkmCQpKgkCQoJAkKSYJCkqCQJCgkCQpJgkKSoJAkKCQJCkmCQpKgkCQoJAkKSYJCkqCQJCgkCQpJgkKSoJAkKCQJCkmCQpKgkCQoJAkKSYJCkqCQJCgkCQpJgkKSoJAkKCQJCkmCQpKgkCQoJAkKSYJCkvANpt22xxA330UAAAAASUVORK5CYII=\" y=\"-21.80965\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 283.079661 135.80965 \n",
       "L 283.079661 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 396.571186 135.80965 \n",
       "L 396.571186 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 283.079661 135.80965 \n",
       "L 396.571186 135.80965 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 283.079661 22.318125 \n",
       "L 396.571186 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_3\">\n",
       "    <!-- sandal -->\n",
       "    <defs>\n",
       "     <path d=\"M 45.40625 46.390625 \n",
       "L 45.40625 75.984375 \n",
       "L 54.390625 75.984375 \n",
       "L 54.390625 0 \n",
       "L 45.40625 0 \n",
       "L 45.40625 8.203125 \n",
       "Q 42.578125 3.328125 38.25 0.953125 \n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \n",
       "Q 5.515625 14.40625 5.515625 27.296875 \n",
       "Q 5.515625 40.1875 11.734375 48.09375 \n",
       "Q 17.96875 56 27.875 56 \n",
       "Q 33.9375 56 38.25 53.625 \n",
       "Q 42.578125 51.265625 45.40625 46.390625 \n",
       "z\n",
       "M 14.796875 27.296875 \n",
       "Q 14.796875 17.390625 18.875 11.75 \n",
       "Q 22.953125 6.109375 30.078125 6.109375 \n",
       "Q 37.203125 6.109375 41.296875 11.75 \n",
       "Q 45.40625 17.390625 45.40625 27.296875 \n",
       "Q 45.40625 37.203125 41.296875 42.84375 \n",
       "Q 37.203125 48.484375 30.078125 48.484375 \n",
       "Q 22.953125 48.484375 18.875 42.84375 \n",
       "Q 14.796875 37.203125 14.796875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-100\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(320.067611 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"113.378906\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"176.757812\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "     <use x=\"240.234375\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"301.513672\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_4\">\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 419.269492 135.80965 \n",
       "L 532.761017 135.80965 \n",
       "L 532.761017 22.318125 \n",
       "L 419.269492 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pf7600a7b78)\">\n",
       "    <image height=\"114\" id=\"image0087ab383b\" transform=\"scale(1 -1)translate(0 -114)\" width=\"114\" x=\"419.269492\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-21.80965\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 419.269492 135.80965 \n",
       "L 419.269492 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_19\">\n",
       "    <path d=\"M 532.761017 135.80965 \n",
       "L 532.761017 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 419.269492 135.80965 \n",
       "L 532.761017 135.80965 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 419.269492 22.318125 \n",
       "L 532.761017 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_4\">\n",
       "    <!-- ankle boot -->\n",
       "    <g transform=\"translate(444.292129 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"61.279297\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-107\"/>\n",
       "     <use x=\"182.568359\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"210.351562\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"271.875\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"303.662109\" xlink:href=\"#DejaVuSans-98\"/>\n",
       "     <use x=\"367.138672\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"428.320312\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"489.501953\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_5\">\n",
       "   <g id=\"patch_22\">\n",
       "    <path d=\"M 555.459322 135.80965 \n",
       "L 668.950847 135.80965 \n",
       "L 668.950847 22.318125 \n",
       "L 555.459322 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p6ab47dcc66)\">\n",
       "    <image height=\"114\" id=\"image403efdfefb\" transform=\"scale(1 -1)translate(0 -114)\" width=\"114\" x=\"555.459322\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAHIAAAByCAYAAACP3YV9AAAABHNCSVQICAgIfAhkiAAACGBJREFUeJztnW2IVFUYx8+9M7M7O/umq7vZ6rJZ7oYbom35UhYaSQhFQUQfIggCoaSCCCUICrRPSp/qU/XFyAyikFKSIgyyF6lM7UUj1HV1121fXWfdnbd7p6/9/2e4Z8fdWZaH5/ftv3Pn3nGee/zP85xznutt8Z4omgrixeOgi4WC8z25rWtBj3QlQF/vngJ9R9sV0Hct6AW9OoU65WVBn8+1gP433wj61PhS1L3LQMcuJUHfdmDMMOHps9bfZhO/omdX5gwNpBA0kELwKu2Rfn096DCdto4ZfOFe0F/s3AP6cqEG9NpqD/TvuTyeL6gD3eBnQLfH0WMn6RtIh+jJvocHDBTw3/RADZ7/mZ4thhnZaPvmbKIjUggaSCFoIIUQdx8yM4qZrPMYP48eNBrix/pushP0r5kc6IQXgK7yMFeNJUbo/Pj+oaAWdF9hIWjOK/l6Sf8i6LMjmJcaY0yzUY9UpoEGUggaSCFU3CO9GN4rxXyJY9ByTJL+wJ7UHL8GOh1gnpn08CJBMfp+9b0w8v1N8YnI11lPZasir1cJdEQKQQMpBA2kECqfRxbdpdxCjRf5eszgOdgTmYDuzxh5YGDwejGDr6dDnF9kj80YrMWG9Ho+V/Gv1UJHpBA0kELQQAqh8v+Zh+VPd7Incp7nIih6pKPvV/ZUPj5BtdvQcf97XkWneEuiI1IIGkghaCCFMPcJTwnYwjjPc2HljaTzJoaaLmjlgUU8nj1yPqIjUggaSCFoIIWggRTCvPixw7cTJ/Rc1LaK4MXohN7S9GOKdb6IX0vMwwXIrhqHFgSUG0YDKQQNpBAq75G+O7nPLoz2FFdRO0ubbnixFMMJv309/Mz1Pm76GSgsAM1F/XiCVpOVwqPvZRoT8FHoiBSCBlIIGkghVH7xVd5dcM42o6fkrSI4ehAXsX0PPZLhiWCX5zKctzJV9Pl8fxp+N0NPZHRECkEDKQQNpBDmYPGVO6dqWnYVdMaR5zHsoVyrzdH5eCI5U4xecMyEvLiL8k7fL2+x2GygI1IIGkghaCCFMC/mI9sbsVGCqxYa83gBM2rOG3l+kT3NzjPx9UyIG1dd5wuCuR8fOiKFoIEUggZSCPPCI+9rOgea87phauLH843ZEJsI8u2ZofnKZmpQwa+nfGyoxLVWrvXypqOGFK7xmQt0RApBAykEDaQQ5oVHbqrFxu09+cWg62LoOa5mELU+NjK8HlaD5k09rlosz1eOFtCTM5RXrm/BJoPGGHMm4vPOBjoihaCBFIIGUgjle6RPddAizb3dwFqUBOVp9kZTnNPkvK4+hutOnbVayvuSPjYFzBfZY7HWmorh6+kQPbslYTfgP2OifX2m61x1RApBAykEDaQQyvfIaazB+T/+6pWgL7xmXzIsHgfNtVbG3u+IeqyAzeZHSfOaG4Y9k+crrTyT5iO7Uz3WOfe9+Szo5btPgC5m3U38o9ARKQQNpBA0kEKY8UPOBrfjA8qef/Eg6DVJrDt2xO3u9J9MrADND2DhWinXUnk+sZ5qs/wAltYErhHiWuskXc9aE+Tw2JRv+91jdZdAN/qYVy4/vA1057afI6/B6IgUggZSCBpIITg90k+lQE8dxAd47VqBnjhQQD/iubsliXHrGuezeE5ek5Oj+T6upbbRQ8w+HlwP+uQRzGVr1g2D3rXyc9BDhQbQ7MHsmQz3NDDG9k32+e7kZdB7Bx4C3f9kE+jCRfRcHZFC0EAKQQMpBHetNYZ+9GjradDHJm4HzXN1TbGJSG2MMf+Ym0DzQ8PYI7nWyceHVPvk6cFrafT9EfJxn/db0v3uG5of5R4CJXoOsCfyOQ+lV4F+ruUo6L0fbQU9thHPryNSCBpIIWggheD0yDCNBvP2sS2g39nyAei/MktB817GUutpXHv6Xc3i++nh1psX/g069zi+f1VjP+gqWhPEeWS1X+LppRGwZxpj14t5zpNz53P5ZtDpp3BO1ZhRUDoihaCBFIIGUghlr9np2o01waHN6Ce8ZpQp1feNPYjzRvYcaw9/yA8lQ8+riqEey2MeGSSj1+CUS6laLK/FTQf4sNGuZB/o/cP3gObaqnXNcj6gMn/RQApBAykEDaQQyv6xU+jDZHp/H07i7mg/ArqHEtukZ29m4YIAF5iHqBkET9y2V+FE8TdXu0Cf6sciRV0Nnr+jZpA+Y3kFAPtpeXZBgCenXe85t3MlvY4LmhkdkULQQApBAymEGTeD8F5fBDq/DwvU49S4odTiXk7grQIzeVYqjg2NrgaY4N/fiEXz3iYsqi+rxUa/vFGW/Yw/Myf8VuPfEuODJwI6qgdAHx5fg+f8NtoTGR2RQtBACkEDKYSZe+T3J0G/17cJ9I62L0EfncAczxhjfEehnRczBY5tR7wJ5+X2r0GzB48EuPiKPc5aOOXYaFtq8pwnExb510H/sv1O0J45ZZ0jCh2RQtBACkEDKQS3R3IjH49iT80hMg/ihpqjJ9AT16WwyW4p/si0gZ4MuCHRNdDsSbxx6HION8BwXrg4gQufGmOToDmvZA/mifFStdoNqQugH/nqJdCdP5a3sZXRESkEDaQQNJBCcDeDmOWHOhc3rrH+NvIqetKOTsz7lsRxc+yJqVtAc555a1X0/CIvSOZGv1y7bY7jIm3eWMvnPz6JzS2MMebdfQ+Dbt3zg3UMUOb3riNSCBpIIWgghTD7HsmNectsQmiMMfGlraBH38c88o2OQ6B5ro83+Xw2fDfo7gZs4rSA8saOKpwr5Prwp700d/ghzsk2HPjJlM0MvzcdkULQQApBAymE8psKsmc6j5/GvVKmH4w/vQF002/YJDD4E9fslMuVV7BR4s1vOXK+eYCOSCFoIIWggRTCfxLossm7FojyAAAAAElFTkSuQmCC\" y=\"-21.80965\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_23\">\n",
       "    <path d=\"M 555.459322 135.80965 \n",
       "L 555.459322 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_24\">\n",
       "    <path d=\"M 668.950847 135.80965 \n",
       "L 668.950847 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_25\">\n",
       "    <path d=\"M 555.459322 135.80965 \n",
       "L 668.950847 135.80965 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_26\">\n",
       "    <path d=\"M 555.459322 22.318125 \n",
       "L 668.950847 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_5\">\n",
       "    <!-- t-shirt -->\n",
       "    <g transform=\"translate(594.27446 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-45\"/>\n",
       "     <use x=\"75.292969\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"127.392578\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "     <use x=\"190.771484\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"218.554688\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"259.667969\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_6\">\n",
       "   <g id=\"patch_27\">\n",
       "    <path d=\"M 691.649153 135.80965 \n",
       "L 805.140678 135.80965 \n",
       "L 805.140678 22.318125 \n",
       "L 691.649153 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pc514565187)\">\n",
       "    <image height=\"114\" id=\"image6fc8c388d2\" transform=\"scale(1 -1)translate(0 -114)\" width=\"114\" x=\"691.649153\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-21.80965\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_28\">\n",
       "    <path d=\"M 691.649153 135.80965 \n",
       "L 691.649153 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_29\">\n",
       "    <path d=\"M 805.140678 135.80965 \n",
       "L 805.140678 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_30\">\n",
       "    <path d=\"M 691.649153 135.80965 \n",
       "L 805.140678 135.80965 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_31\">\n",
       "    <path d=\"M 691.649153 22.318125 \n",
       "L 805.140678 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_6\">\n",
       "    <!-- ankle boot -->\n",
       "    <g transform=\"translate(716.67179 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"61.279297\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-107\"/>\n",
       "     <use x=\"182.568359\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"210.351562\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"271.875\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"303.662109\" xlink:href=\"#DejaVuSans-98\"/>\n",
       "     <use x=\"367.138672\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"428.320312\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"489.501953\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_7\">\n",
       "   <g id=\"patch_32\">\n",
       "    <path d=\"M 827.838983 135.80965 \n",
       "L 941.330508 135.80965 \n",
       "L 941.330508 22.318125 \n",
       "L 827.838983 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p2e7e7a4dc9)\">\n",
       "    <image height=\"114\" id=\"imageb8eebb26f8\" transform=\"scale(1 -1)translate(0 -114)\" width=\"114\" x=\"827.838983\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAHIAAAByCAYAAACP3YV9AAAABHNCSVQICAgIfAhkiAAACd9JREFUeJztXV1sHFcVvjOz6/Xu+nfjNGlMqdskUClESeXabSC0KJTSPlCpoqhSgYc+IIEQQkIIhITKEw9I9JUipCoNEpRKhZQiKiBSRasETENJS0saO+DUdbATu3bWXnv/ZmeGh774+647l5GbPpyc7+3z3L337hzf/eace84d727vwcR8gPBG91l/u/iZfuBdVZxSYRV51OUBbw0ibw5R+5EmcD+IgOdyMfA4pv6XisAHXwuAVyZbwLvOXjSM6PKC9bf3E/5V7V3xgUENKQRqSCHIbbUDL4ddJJ0OcL9UAv6pJ1+2+ritNA08SvD/69NF1CAXYoMaOR81UtsPB6XU642kDbx4fxfwEw3U0HtL9nzvu/kOnGMTddt1H13QFSkEakghUEMKwZY1MonT3VB/53XAr89PWW1eWrsF+FBuDfhzV3YCLwaoWXkP/cKCn64vgYd+4+GyPaeNeHHtY6nXwwT9yoXOvNXG37EdeDwziw28ra0pXZFCoIYUAjWkEGzdjwxQH5IY9SrJ4fUwsYcc7rqSOsaNxXeA9/oUO/VQp1mz6jH6fd0eauiOADU5Nhhr7Q1wPNbk5U4ZeBddN8aYpCfdVzW+l37dAV2RQqCGFAI1pBBk10gPf8uTsP0eDd9FdB7jqA/1vmW1Oba6N7WPmGKvzSQPPG9QkwKDfiJrWpN0ukb9BRSrbcZ43Q9o/5I09YEee+/x6L8mrb9tRNLC+GzW2KuuSCFQQwqBGlIIsmtkkh5bje+6Fbj/4hng1dj+rWdN28zXfD/BGsh+pyFNZQ2sRwXgPeRn1uL05wZjjEk+cRCHPPUqXtf9yGsTakghUEMKgVOMcsO7gM/9pA/4l3djDs63KkeBj3/va8B/sbJujXGg+Dbw/7Svs9psBGscayyD/VCf9iPZL2Ve8EPgLfIrb+5aBP5EFfXPGGOWHzkE/PQPHwf+oyX0pY/PHgBeeWQVOOfJ6ooUAjWkEKghhUANKQTOh53pr4wAnxh9DPjTtd3A12J0jt8ZxweLY5O3W2P8aRyDBn9d3wO85KODHZGDjo8exkT0/2m354BA+m3g8a+EuJF8sDAH/NHXP2f10RjF+7AQ4UPfYA75c/uPAf/kN74NfOT7+rAjEmpIIVBDCoFTI0eeWQb+x4cxWZgDzs2Eiki3YQFNNNlrjbH9EAaheSOXg9Ls4If0NXhOHEBgzaxGmBjFAQNGK8bxdgQ0/iQGTYwxxh/B+1CghOQ4wTn9uYGBmN1PoR04vUtXpBCoIYVADSkETo305tFfeaCMv9VHV9GnqlFRzw9u/T3wR+e+YI0x00kvyskK1kDWSLs9BdUpCL/SQQ0dzNeBL9Nm+WZu6Y/HngF+OcIxSj4mX/F9fnIBOUNXpBCoIYVADSkETo2MlvC3eYoSksv02z5HPtmR4gzwnx+wDxM6H24DPpSvAV+LuoHzRq9v0gtgWDOjhAuL0nmeCmc5+WqiOQx87xh+Z2OMuasbnzUmw26rzUZc7JD/vbj4Hi3fha5IIVBDCoEaUggyZwJPNG4Cvj2HSUHVGDVyLkJ9aceUDGyMWY9RcziWyrCuO/xOV6yVwQc2MbpJo3k+9RALa40xZrqDt3opRv+b+zxR/0jqHBi6IoVADSkEakghyKyRv13A5NtvfugE8FpcJI7+UjuyNZJ9UYZLs/yECk8tDcXrnGDM+5+cwxPG6bdpIMDYaz3kLCJbl2sR3qcKHUjxm8VR6kFjrdcE1JBCoIYUgswa+cYM5pKUb8DY62yEcdMBH/XjwDbMATXGmKVOD3DXfqRLM/m6i1ua6Ii1ssayH/zxnResOdWpDRcK9dEhUKenbwS+RzXy2oAaUgjUkEKQWSP7/k77aIeRsn/UzKEW7Cv/1+qTNYp9Lj6MgcF+o6u9nYuL47MGsmb7fKAS6d3+kr3nym1YVzmXtu9veB9d0BUpBGpIIVBDCkF2jXwbfar1xN572wjOEd0MHGtdiRyH1G4RXGfBGst+JsdiKzn7QIuN4PiyMfZ94MOAWUPLl7Ll9uqKFAI1pBCoIYUgs0b2voKxUt5/ZNQT9Je4jsIYd94qI6S8n9BLz0tl5H3UH6ue0tq/xNvEdRqXQ3yRaS2yNTIspNdsLnawprL/lUvAXUcM6ooUAjWkEKghhSCzRnZmMY54T5HqJcNB4G+1h4B/d9ubVp8/XcG9N9ZRjm26wH4g+438whfu36Wx7PNV6KVsXx04Z33mqdqHgfO5CJ8tYW3Izy7Y9SNp0BUpBGpIIVBDCsGWT4E/vn49cKteso2auRBhDo8x7lqPrYI1MSt6AvxO3R76ufwdl2N8aY0xxlxsV4B/tBtfFvpCA69nha5IIVBDCoEaUgi2rJH/WEMfcKwHczrZJ3uezmI1xpj5dr/1t43gWCfn1PAY3N7KueEXh3ocB+UXyvCLRTF+3KKcn5ONGwyD6x/5bJ9Ta9nqIRm6IoVADSkEakgh2LJG/m5qP/AjY2dT2+c9e2eN9WKhhWe6FgPUF96PdO1fMlgjOzGNH+L45QDrW86H+IKZkdIScM5ZNcaO/w4EmPdz/By+sOUm85rVRxp0RQqBGlII1JBCoIYUAvfDjk+brDEdPv8GHvxTHscHA3amX6/bzjKjN4ebro0oPQmaAwTWdXrQaDlypvnhilHpSk9QPlvfZf0t5+OgZQ/vU+EM3kcLDjvoihQCNaQQqCGFwH04fYC/zQn9NlfOcUA6PeC82rGTdwfyeMgsb+QW6DAGTqZicPvhwhXg8+2B1DlygIADFvzSM9d8jNksaI6fGTifXrTj+diezojSFSkFakghUEMKgVMjkyj9t7t0CfWMi1FYP/KbvECM9cMCfYQ3cqshJjQf7J0F/tjL9wD/zvgfgP9zHX3bImkgay4H3fk7bubX+lF60U7xMvrOjCROTyDTFSkEakghUEMKgTvWyg4LwTv1KvA3mxhn5ESocJNk5DrFUvtz6FeGBv08fskY81159Bv9Reyfi25YE7k/F/iwCP7Om2G6hZvT3l+ybSQzdEUKgRpSCNSQQvB/aGS2ApgLdSxsvb0fC1peWL7F+gwX8bBf2EeaWaVC2FaEX+NweQo4fdyMFfBgw5dWcE4zDTw8uEjJV6790c4mzwGjZSwQPlndSy2qqX3y/iNDV6QQqCGFQA0pBN7d3oPZRNCRO8KYeuI24E8fedxq8+vqGPDPD5wG/svlQ8Cfn9qHU+BDAts4x8pJ1LT4fjzAYrCEInrn9n8D5xeTjhVR95+t4stW7uydNIyvTzwMfM+XzlhtAB7tcTqeVXRFCoEaUgjUkEKQXSOvAla+eAfwOI/6MPQsFgZF1ZWrPqeNCAbxsIdLD6Hf2V3FW9j7q4mrPieGrkghUEMKgRpSCP4HFNxZ0nhGsTIAAAAASUVORK5CYII=\" y=\"-21.80965\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_33\">\n",
       "    <path d=\"M 827.838983 135.80965 \n",
       "L 827.838983 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_34\">\n",
       "    <path d=\"M 941.330508 135.80965 \n",
       "L 941.330508 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_35\">\n",
       "    <path d=\"M 827.838983 135.80965 \n",
       "L 941.330508 135.80965 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_36\">\n",
       "    <path d=\"M 827.838983 22.318125 \n",
       "L 941.330508 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_7\">\n",
       "    <!-- shirt -->\n",
       "    <g transform=\"translate(871.170996 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "     <use x=\"115.478516\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"143.261719\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"184.375\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_8\">\n",
       "   <g id=\"patch_37\">\n",
       "    <path d=\"M 964.028814 135.80965 \n",
       "L 1077.520339 135.80965 \n",
       "L 1077.520339 22.318125 \n",
       "L 964.028814 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pc07581dc71)\">\n",
       "    <image height=\"114\" id=\"imagedccca15e47\" transform=\"scale(1 -1)translate(0 -114)\" width=\"114\" x=\"964.028814\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-21.80965\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_38\">\n",
       "    <path d=\"M 964.028814 135.80965 \n",
       "L 964.028814 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_39\">\n",
       "    <path d=\"M 1077.520339 135.80965 \n",
       "L 1077.520339 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_40\">\n",
       "    <path d=\"M 964.028814 135.80965 \n",
       "L 1077.520339 135.80965 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_41\">\n",
       "    <path d=\"M 964.028814 22.318125 \n",
       "L 1077.520339 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_8\">\n",
       "    <!-- coat -->\n",
       "    <defs>\n",
       "     <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(1007.775201 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"116.162109\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"177.441406\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_9\">\n",
       "   <g id=\"patch_42\">\n",
       "    <path d=\"M 1100.218644 135.80965 \n",
       "L 1213.710169 135.80965 \n",
       "L 1213.710169 22.318125 \n",
       "L 1100.218644 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p54382c4d66)\">\n",
       "    <image height=\"114\" id=\"image79440df729\" transform=\"scale(1 -1)translate(0 -114)\" width=\"114\" x=\"1100.218644\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-21.80965\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_43\">\n",
       "    <path d=\"M 1100.218644 135.80965 \n",
       "L 1100.218644 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_44\">\n",
       "    <path d=\"M 1213.710169 135.80965 \n",
       "L 1213.710169 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_45\">\n",
       "    <path d=\"M 1100.218644 135.80965 \n",
       "L 1213.710169 135.80965 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_46\">\n",
       "    <path d=\"M 1100.218644 22.318125 \n",
       "L 1213.710169 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_9\">\n",
       "    <!-- pullover -->\n",
       "    <defs>\n",
       "     <path d=\"M 18.109375 8.203125 \n",
       "L 18.109375 -20.796875 \n",
       "L 9.078125 -20.796875 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "z\n",
       "M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-112\"/>\n",
       "     <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-117\"/>\n",
       "     <path d=\"M 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 8.796875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "L 35.6875 0 \n",
       "L 23.484375 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-118\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(1132.638157 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-112\"/>\n",
       "     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "     <use x=\"126.855469\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"154.638672\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"182.421875\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"243.603516\" xlink:href=\"#DejaVuSans-118\"/>\n",
       "     <use x=\"302.783203\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"364.306641\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_10\">\n",
       "   <g id=\"patch_47\">\n",
       "    <path d=\"M 1236.408475 135.80965 \n",
       "L 1349.9 135.80965 \n",
       "L 1349.9 22.318125 \n",
       "L 1236.408475 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pe607b73d66)\">\n",
       "    <image height=\"114\" id=\"imagea336973ac4\" transform=\"scale(1 -1)translate(0 -114)\" width=\"114\" x=\"1236.408475\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-21.80965\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_48\">\n",
       "    <path d=\"M 1236.408475 135.80965 \n",
       "L 1236.408475 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_49\">\n",
       "    <path d=\"M 1349.9 135.80965 \n",
       "L 1349.9 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_50\">\n",
       "    <path d=\"M 1236.408475 135.80965 \n",
       "L 1349.9 135.80965 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_51\">\n",
       "    <path d=\"M 1236.408475 22.318125 \n",
       "L 1349.9 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_10\">\n",
       "    <!-- coat -->\n",
       "    <g transform=\"translate(1280.154862 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"116.162109\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"177.441406\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pc50bd7c825\">\n",
       "   <rect height=\"113.491525\" width=\"113.491525\" x=\"10.7\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p3c0728fb39\">\n",
       "   <rect height=\"113.491525\" width=\"113.491525\" x=\"146.889831\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p6b517d99eb\">\n",
       "   <rect height=\"113.491525\" width=\"113.491525\" x=\"283.079661\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pf7600a7b78\">\n",
       "   <rect height=\"113.491525\" width=\"113.491525\" x=\"419.269492\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p6ab47dcc66\">\n",
       "   <rect height=\"113.491525\" width=\"113.491525\" x=\"555.459322\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pc514565187\">\n",
       "   <rect height=\"113.491525\" width=\"113.491525\" x=\"691.649153\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p2e7e7a4dc9\">\n",
       "   <rect height=\"113.491525\" width=\"113.491525\" x=\"827.838983\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pc07581dc71\">\n",
       "   <rect height=\"113.491525\" width=\"113.491525\" x=\"964.028814\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p54382c4d66\">\n",
       "   <rect height=\"113.491525\" width=\"113.491525\" x=\"1100.218644\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pe607b73d66\">\n",
       "   <rect height=\"113.491525\" width=\"113.491525\" x=\"1236.408475\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 1728x1728 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize image set\n",
    "show_num = 10\n",
    "i = 0\n",
    "_, figs = plt.subplots(1, show_num, figsize=(24, 24))\n",
    "for sample_X, sample_y in train_iter:\n",
    "    break\n",
    "for image_pixel, image_label in zip(sample_X, sample_y):\n",
    "    # 不会自动换行\n",
    "    if i >= show_num:\n",
    "        break\n",
    "    f = figs[i]\n",
    "    f.imshow(image_pixel.reshape((28, 28)).asnumpy())\n",
    "    f.set_title(get_fashion_mnist_label(image_label.asscalar()))\n",
    "    f.axes.get_xaxis().set_visible(False)\n",
    "    f.axes.get_yaxis().set_visible(False)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.         0.3529412  0.\n",
       "  0.         0.         0.         0.         0.2784314  0.\n",
       "  0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.         0.5019608  0.54509807\n",
       "  0.06666667 0.         0.01960784 0.47843137 0.5372549  0.\n",
       "  0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.1764706  0.73333335 0.7607843\n",
       "  0.7372549  0.7019608  0.7372549  0.76862746 0.7529412  0.42745098\n",
       "  0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.58431375 0.7254902  0.6862745\n",
       "  0.63529414 0.6666667  0.67058825 0.65882355 0.69411767 0.7254902\n",
       "  0.04313726 0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.78431374 0.6784314  0.6784314\n",
       "  0.63529414 0.6431373  0.67058825 0.6784314  0.6666667  0.8039216\n",
       "  0.2509804  0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.7254902  0.73333335 0.65882355\n",
       "  0.67058825 0.6509804  0.6784314  0.6666667  0.69411767 0.81960785\n",
       "  0.21176471 0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.65882355 0.76862746 0.6431373\n",
       "  0.65882355 0.6666667  0.63529414 0.6431373  0.69411767 0.78431374\n",
       "  0.22745098 0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.7764706  0.8039216  0.6784314\n",
       "  0.6784314  0.6509804  0.63529414 0.6431373  0.7019608  0.7921569\n",
       "  0.24313726 0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.6666667  0.78431374 0.6862745\n",
       "  0.7019608  0.61960787 0.67058825 0.6431373  0.7019608  0.8\n",
       "  0.25882354 0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.74509805 0.8117647  0.6862745\n",
       "  0.70980394 0.6784314  0.69411767 0.6862745  0.6862745  0.8\n",
       "  0.30980393 0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.7372549  0.78431374 0.7176471\n",
       "  0.7019608  0.7019608  0.6862745  0.7254902  0.7019608  0.8117647\n",
       "  0.31764707 0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.7607843  0.8039216  0.76862746\n",
       "  0.6862745  0.7254902  0.7019608  0.73333335 0.7176471  0.84313726\n",
       "  0.3019608  0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.7372549  0.7529412  0.6784314\n",
       "  0.6784314  0.7254902  0.6666667  0.6666667  0.6784314  0.8039216\n",
       "  0.28627452 0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.06666667 0.9019608  0.67058825 0.7372549\n",
       "  0.63529414 0.65882355 0.6509804  0.7176471  0.69411767 0.7372549\n",
       "  0.6039216  0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.22745098 0.9529412  0.6666667  0.7372549\n",
       "  0.67058825 0.73333335 0.7019608  0.70980394 0.7019608  0.7176471\n",
       "  0.78431374 0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.40392157 0.91764706 0.63529414 0.73333335\n",
       "  0.69411767 0.7254902  0.7019608  0.73333335 0.7176471  0.69411767\n",
       "  0.8784314  0.         0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.45882353 0.8666667  0.6431373  0.74509805\n",
       "  0.73333335 0.70980394 0.73333335 0.7254902  0.74509805 0.6666667\n",
       "  0.9372549  0.02745098 0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.5254902  0.8352941  0.6784314  0.7607843\n",
       "  0.7176471  0.6862745  0.74509805 0.7372549  0.7372549  0.6509804\n",
       "  0.93333334 0.1764706  0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.6        0.8117647  0.7019608  0.76862746\n",
       "  0.7254902  0.67058825 0.70980394 0.74509805 0.73333335 0.6117647\n",
       "  0.91764706 0.30980393 0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.6509804  0.8039216  0.7372549  0.7764706\n",
       "  0.73333335 0.70980394 0.7529412  0.78431374 0.70980394 0.627451\n",
       "  0.9098039  0.39215687 0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.7019608  0.7764706  0.7607843  0.7607843\n",
       "  0.7372549  0.70980394 0.7764706  0.7764706  0.69411767 0.6666667\n",
       "  0.84313726 0.4509804  0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.6784314  0.76862746 0.7607843  0.7372549\n",
       "  0.7372549  0.69411767 0.7764706  0.7764706  0.73333335 0.6862745\n",
       "  0.8        0.5176471  0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.7019608  0.7529412  0.827451   0.7529412\n",
       "  0.7764706  0.7019608  0.7764706  0.78431374 0.7176471  0.67058825\n",
       "  0.8        0.6        0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.69411767 0.7254902  0.8039216  0.74509805\n",
       "  0.7764706  0.67058825 0.8117647  0.76862746 0.7176471  0.70980394\n",
       "  0.7607843  0.6039216  0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.6784314  0.7019608  0.8352941  0.7372549\n",
       "  0.7607843  0.70980394 0.81960785 0.7764706  0.76862746 0.7372549\n",
       "  0.78431374 0.63529414 0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.7254902  0.70980394 0.81960785 0.7607843\n",
       "  0.7764706  0.69411767 0.8        0.76862746 0.74509805 0.7372549\n",
       "  0.7529412  0.6431373  0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.8666667  0.8784314  0.85882354 0.8117647\n",
       "  0.96862745 0.84313726 1.         0.93333334 0.9019608  0.87058824\n",
       "  0.9254902  0.8666667  0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]\n",
       " [0.         0.         0.         0.         0.         0.\n",
       "  0.         0.         0.29411766 0.36078432 0.43529412 0.5764706\n",
       "  0.3764706  0.1254902  0.39215687 0.4        0.29411766 0.3019608\n",
       "  0.36078432 0.23529412 0.         0.         0.         0.\n",
       "  0.         0.         0.         0.        ]]\n",
       "<NDArray 28x28 @cpu_shared(0)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_X.reshape((28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n",
      "epoch 1, loss 2.3159, train acc 0.105, test acc 0.100, time 16.6 sec\n",
      "epoch 2, loss 1.4244, train acc 0.443, test acc 0.609, time 15.1 sec\n",
      "epoch 3, loss 0.8718, train acc 0.658, test acc 0.719, time 15.4 sec\n",
      "epoch 4, loss 0.7152, train acc 0.718, test acc 0.728, time 25.9 sec\n",
      "epoch 5, loss 0.6367, train acc 0.745, test acc 0.754, time 25.8 sec\n",
      "epoch 6, loss 0.5756, train acc 0.772, test acc 0.793, time 17.0 sec\n",
      "epoch 7, loss 0.5285, train acc 0.792, test acc 0.809, time 18.4 sec\n",
      "epoch 8, loss 0.4928, train acc 0.808, test acc 0.830, time 17.8 sec\n",
      "epoch 9, loss 0.4648, train acc 0.823, test acc 0.834, time 17.2 sec\n",
      "epoch 10, loss 0.4454, train acc 0.832, test acc 0.847, time 16.0 sec\n",
      "epoch 11, loss 0.4247, train acc 0.840, test acc 0.841, time 18.4 sec\n",
      "epoch 12, loss 0.4121, train acc 0.846, test acc 0.850, time 21.8 sec\n",
      "epoch 13, loss 0.3994, train acc 0.852, test acc 0.858, time 25.0 sec\n",
      "epoch 14, loss 0.3861, train acc 0.858, test acc 0.867, time 26.0 sec\n",
      "epoch 15, loss 0.3746, train acc 0.862, test acc 0.867, time 18.1 sec\n",
      "epoch 16, loss 0.3638, train acc 0.866, test acc 0.871, time 15.4 sec\n",
      "epoch 17, loss 0.3560, train acc 0.869, test acc 0.874, time 17.4 sec\n",
      "epoch 18, loss 0.3489, train acc 0.871, test acc 0.867, time 18.1 sec\n",
      "epoch 19, loss 0.3420, train acc 0.874, test acc 0.877, time 14.9 sec\n",
      "epoch 20, loss 0.3361, train acc 0.875, test acc 0.880, time 15.0 sec\n",
      "epoch 21, loss 0.3292, train acc 0.877, test acc 0.880, time 15.8 sec\n",
      "epoch 22, loss 0.3241, train acc 0.880, test acc 0.884, time 14.8 sec\n",
      "epoch 23, loss 0.3179, train acc 0.882, test acc 0.880, time 14.6 sec\n",
      "epoch 24, loss 0.3156, train acc 0.883, test acc 0.878, time 16.3 sec\n",
      "epoch 25, loss 0.3108, train acc 0.885, test acc 0.881, time 15.7 sec\n",
      "epoch 26, loss 0.3080, train acc 0.885, test acc 0.888, time 18.8 sec\n",
      "epoch 27, loss 0.3017, train acc 0.889, test acc 0.881, time 20.5 sec\n",
      "epoch 28, loss 0.2976, train acc 0.890, test acc 0.887, time 17.4 sec\n",
      "epoch 29, loss 0.2954, train acc 0.891, test acc 0.888, time 17.2 sec\n",
      "epoch 30, loss 0.2933, train acc 0.891, test acc 0.873, time 17.3 sec\n",
      "epoch 31, loss 0.2912, train acc 0.892, test acc 0.889, time 16.9 sec\n",
      "epoch 32, loss 0.2870, train acc 0.894, test acc 0.893, time 15.9 sec\n",
      "epoch 33, loss 0.2821, train acc 0.896, test acc 0.886, time 16.0 sec\n",
      "epoch 34, loss 0.2810, train acc 0.897, test acc 0.893, time 21.6 sec\n",
      "epoch 35, loss 0.2788, train acc 0.897, test acc 0.896, time 15.8 sec\n",
      "epoch 36, loss 0.2758, train acc 0.898, test acc 0.898, time 19.4 sec\n",
      "epoch 37, loss 0.2724, train acc 0.899, test acc 0.897, time 17.0 sec\n",
      "epoch 38, loss 0.2718, train acc 0.899, test acc 0.894, time 17.3 sec\n",
      "epoch 39, loss 0.2678, train acc 0.901, test acc 0.893, time 15.9 sec\n",
      "epoch 40, loss 0.2659, train acc 0.901, test acc 0.899, time 15.4 sec\n",
      "epoch 41, loss 0.2623, train acc 0.903, test acc 0.891, time 16.6 sec\n",
      "epoch 42, loss 0.2598, train acc 0.905, test acc 0.894, time 18.4 sec\n",
      "epoch 43, loss 0.2587, train acc 0.905, test acc 0.896, time 16.4 sec\n",
      "epoch 44, loss 0.2575, train acc 0.905, test acc 0.899, time 16.8 sec\n",
      "epoch 45, loss 0.2531, train acc 0.906, test acc 0.898, time 17.6 sec\n",
      "epoch 46, loss 0.2522, train acc 0.906, test acc 0.895, time 17.5 sec\n",
      "epoch 47, loss 0.2515, train acc 0.906, test acc 0.901, time 15.8 sec\n",
      "epoch 48, loss 0.2487, train acc 0.908, test acc 0.900, time 15.3 sec\n",
      "epoch 49, loss 0.2465, train acc 0.909, test acc 0.898, time 15.0 sec\n",
      "epoch 50, loss 0.2444, train acc 0.910, test acc 0.898, time 18.5 sec\n",
      "epoch 51, loss 0.2432, train acc 0.910, test acc 0.904, time 18.2 sec\n",
      "epoch 52, loss 0.2411, train acc 0.910, test acc 0.902, time 19.7 sec\n",
      "epoch 53, loss 0.2397, train acc 0.910, test acc 0.903, time 15.6 sec\n",
      "epoch 54, loss 0.2363, train acc 0.912, test acc 0.903, time 15.5 sec\n",
      "epoch 55, loss 0.2362, train acc 0.912, test acc 0.899, time 15.5 sec\n",
      "epoch 56, loss 0.2323, train acc 0.914, test acc 0.899, time 15.4 sec\n",
      "epoch 57, loss 0.2316, train acc 0.914, test acc 0.904, time 16.5 sec\n",
      "epoch 58, loss 0.2280, train acc 0.916, test acc 0.905, time 15.4 sec\n",
      "epoch 59, loss 0.2296, train acc 0.915, test acc 0.898, time 15.7 sec\n",
      "epoch 60, loss 0.2279, train acc 0.916, test acc 0.892, time 16.0 sec\n",
      "epoch 61, loss 0.2254, train acc 0.917, test acc 0.905, time 15.3 sec\n",
      "epoch 62, loss 0.2240, train acc 0.917, test acc 0.897, time 19.4 sec\n",
      "epoch 63, loss 0.2223, train acc 0.918, test acc 0.907, time 18.5 sec\n",
      "epoch 64, loss 0.2222, train acc 0.916, test acc 0.907, time 15.5 sec\n",
      "epoch 65, loss 0.2203, train acc 0.919, test acc 0.906, time 23.3 sec\n",
      "epoch 66, loss 0.2175, train acc 0.920, test acc 0.907, time 16.1 sec\n",
      "epoch 67, loss 0.2174, train acc 0.920, test acc 0.906, time 16.9 sec\n",
      "epoch 68, loss 0.2142, train acc 0.921, test acc 0.904, time 14.9 sec\n",
      "epoch 69, loss 0.2138, train acc 0.920, test acc 0.902, time 15.0 sec\n",
      "epoch 70, loss 0.2112, train acc 0.922, test acc 0.906, time 15.0 sec\n",
      "epoch 71, loss 0.2110, train acc 0.921, test acc 0.907, time 15.0 sec\n",
      "epoch 72, loss 0.2089, train acc 0.923, test acc 0.906, time 15.5 sec\n",
      "epoch 73, loss 0.2078, train acc 0.922, test acc 0.908, time 15.0 sec\n",
      "epoch 74, loss 0.2056, train acc 0.924, test acc 0.909, time 15.2 sec\n",
      "epoch 75, loss 0.2065, train acc 0.923, test acc 0.906, time 15.8 sec\n",
      "epoch 76, loss 0.2031, train acc 0.924, test acc 0.905, time 17.3 sec\n",
      "epoch 77, loss 0.2029, train acc 0.925, test acc 0.907, time 15.1 sec\n",
      "epoch 78, loss 0.2009, train acc 0.926, test acc 0.908, time 15.6 sec\n",
      "epoch 79, loss 0.2000, train acc 0.925, test acc 0.909, time 18.0 sec\n",
      "epoch 80, loss 0.1983, train acc 0.926, test acc 0.906, time 16.5 sec\n",
      "epoch 81, loss 0.1966, train acc 0.927, test acc 0.907, time 15.7 sec\n",
      "epoch 82, loss 0.1957, train acc 0.928, test acc 0.909, time 16.6 sec\n",
      "epoch 83, loss 0.1955, train acc 0.928, test acc 0.908, time 18.2 sec\n",
      "epoch 84, loss 0.1925, train acc 0.928, test acc 0.904, time 16.3 sec\n",
      "epoch 85, loss 0.1899, train acc 0.930, test acc 0.910, time 17.1 sec\n",
      "epoch 86, loss 0.1904, train acc 0.928, test acc 0.911, time 16.4 sec\n",
      "epoch 87, loss 0.1898, train acc 0.929, test acc 0.910, time 16.1 sec\n",
      "epoch 88, loss 0.1870, train acc 0.930, test acc 0.905, time 17.2 sec\n",
      "epoch 89, loss 0.1869, train acc 0.929, test acc 0.908, time 16.3 sec\n",
      "epoch 90, loss 0.1851, train acc 0.931, test acc 0.909, time 15.9 sec\n",
      "epoch 91, loss 0.1839, train acc 0.931, test acc 0.905, time 15.0 sec\n",
      "epoch 92, loss 0.1827, train acc 0.932, test acc 0.908, time 15.0 sec\n",
      "epoch 93, loss 0.1840, train acc 0.932, test acc 0.909, time 15.6 sec\n",
      "epoch 94, loss 0.1815, train acc 0.932, test acc 0.908, time 15.7 sec\n",
      "epoch 95, loss 0.1791, train acc 0.934, test acc 0.910, time 16.2 sec\n",
      "epoch 96, loss 0.1791, train acc 0.932, test acc 0.904, time 16.5 sec\n",
      "epoch 97, loss 0.1773, train acc 0.934, test acc 0.909, time 14.9 sec\n",
      "epoch 98, loss 0.1774, train acc 0.933, test acc 0.908, time 15.0 sec\n",
      "epoch 99, loss 0.1736, train acc 0.935, test acc 0.909, time 15.6 sec\n",
      "epoch 100, loss 0.1743, train acc 0.934, test acc 0.911, time 15.3 sec\n"
     ]
    }
   ],
   "source": [
    "context = get_ctx()\n",
    "lr, num_epochs = 0.9, 100\n",
    "lenet.initialize(force_reinit=True, ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(lenet.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_cnn(lenet, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = nn.Sequential()\n",
    "alexnet.add(nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "        nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。\n",
    "        # 前两个卷积层后不使用池化层来减小输入的高和宽\n",
    "        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
    "        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
    "        nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合\n",
    "        nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "        nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "        # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "        nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv4 output shape:\t (1, 96, 54, 54)\n",
      "pool4 output shape:\t (1, 96, 26, 26)\n",
      "conv5 output shape:\t (1, 256, 26, 26)\n",
      "pool5 output shape:\t (1, 256, 12, 12)\n",
      "conv6 output shape:\t (1, 384, 12, 12)\n",
      "conv7 output shape:\t (1, 384, 12, 12)\n",
      "conv8 output shape:\t (1, 256, 12, 12)\n",
      "pool6 output shape:\t (1, 256, 5, 5)\n",
      "dense6 output shape:\t (1, 4096)\n",
      "dropout0 output shape:\t (1, 4096)\n",
      "dense7 output shape:\t (1, 4096)\n",
      "dropout1 output shape:\t (1, 4096)\n",
      "dense8 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 224, 224))\n",
    "alexnet.initialize(force_reinit=True)\n",
    "for layer in alexnet:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-21:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# 如出现“out of memory”的报错信息，可减小batch_size或resize\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6c90964f7fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0malexnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_reinit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXavier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malexnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malexnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-58459d6bc941>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[0;34m(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtrain_acc_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Learning/EnvLearning/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2014\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Learning/EnvLearning/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1994\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1996\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1997\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "alexnet.initialize(force_reinit=True, ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(alexnet.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_cnn(alexnet, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG\n",
    "def vgg_block(num_convs, num_channels):\n",
    "    block = nn.Sequential()\n",
    "    for _ in range(num_convs):\n",
    "        block.add(nn.Conv2D(\n",
    "            num_channels, kernel_size=3, padding=1, activation='relu'))\n",
    "    block.add(nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    net = nn.Sequential()\n",
    "    # 卷积层部分\n",
    "    for (num_convs, num_channels) in conv_arch:\n",
    "        net.add(vgg_block(num_convs, num_channels))\n",
    "    # 全连接层部分\n",
    "    net.add(nn.Dense(4096, activation='relu'), nn.Dropout(0.5),\n",
    "            nn.Dense(4096, activation='relu'), nn.Dropout(0.5),\n",
    "            nn.Dense(10))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n",
    "vggnet = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential7 output shape:\t (1, 64, 112, 112)\n",
      "sequential8 output shape:\t (1, 128, 56, 56)\n",
      "sequential9 output shape:\t (1, 256, 28, 28)\n",
      "sequential10 output shape:\t (1, 512, 14, 14)\n",
      "sequential11 output shape:\t (1, 512, 7, 7)\n",
      "dense9 output shape:\t (1, 4096)\n",
      "dropout2 output shape:\t (1, 4096)\n",
      "dense10 output shape:\t (1, 4096)\n",
      "dropout3 output shape:\t (1, 4096)\n",
      "dense11 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "vggnet.initialize(force_reinit=True)\n",
    "X = nd.random.uniform(shape=(1, 1, 224, 224))\n",
    "for blk in vggnet:\n",
    "    X = blk(X)\n",
    "    print(blk.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(num_channels, kernel_size, strides, padding):\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.Conv2D(num_channels, kernel_size,\n",
    "                      strides, padding, activation='relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1, activation='relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1, activation='relu'))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "netinet = nn.Sequential()\n",
    "netinet.add(nin_block(96, kernel_size=11, strides=4, padding=0),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        nin_block(256, kernel_size=5, strides=1, padding=2),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        nin_block(384, kernel_size=3, strides=1, padding=1),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2), nn.Dropout(0.5),\n",
    "        # 标签类别数是10\n",
    "        nin_block(10, kernel_size=3, strides=1, padding=1),\n",
    "        # 全局平均池化层将窗口形状自动设置成输入的高和宽\n",
    "        nn.GlobalAvgPool2D(),\n",
    "        # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n",
    "        nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential13 output shape:\t (1, 96, 54, 54)\n",
      "pool12 output shape:\t (1, 96, 26, 26)\n",
      "sequential14 output shape:\t (1, 256, 26, 26)\n",
      "pool13 output shape:\t (1, 256, 12, 12)\n",
      "sequential15 output shape:\t (1, 384, 12, 12)\n",
      "pool14 output shape:\t (1, 384, 5, 5)\n",
      "dropout4 output shape:\t (1, 384, 5, 5)\n",
      "sequential16 output shape:\t (1, 10, 5, 5)\n",
      "pool15 output shape:\t (1, 10, 1, 1)\n",
      "flatten0 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 224, 224))\n",
    "netinet.initialize(force_reinit=True)\n",
    "for layer in netinet:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 5, 128\n",
    "net.initialize(force_reinit=True, ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "train_cnn(netinet, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Block):\n",
    "    \"\"\"\n",
    "    c1 - c4为每条线路里的层的输出通道数\n",
    "    这种forward 怎么反向计算\n",
    "    \"\"\"\n",
    "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # 线路1，单1 x 1卷积层\n",
    "        self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')\n",
    "        # 线路2，1 x 1卷积层后接3 x 3卷积层\n",
    "        self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')\n",
    "        self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1,\n",
    "                              activation='relu')\n",
    "        # 线路3，1 x 1卷积层后接5 x 5卷积层\n",
    "        self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')\n",
    "        self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2,\n",
    "                              activation='relu')\n",
    "        # 线路4，3 x 3最大池化层后接1 x 1卷积层\n",
    "        self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1_1(x)\n",
    "        p2 = self.p2_2(self.p2_1(x))\n",
    "        p3 = self.p3_2(self.p3_1(x))\n",
    "        p4 = self.p4_2(self.p4_1(x))\n",
    "        return nd.concat(p1, p2, p3, p4, dim=1)  # 在通道维上连结输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GoogleNet: 主体卷积部分有5个 模块\n",
    "block_1 = nn.Sequential()\n",
    "block_1.add(\n",
    "    nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_2 = nn.Sequential()\n",
    "block_2.add(\n",
    "    nn.Conv2D(64, kernel_size=1, activation='relu'),\n",
    "    nn.Conv2D(192, kernel_size=3, padding=1, activation='relu'),  # 通道数增大3倍\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三模块: 2个inception\n",
    "block_3 = nn.Sequential()\n",
    "block_3.add(\n",
    "    Inception(64, (96, 128), (16, 32), 32),\n",
    "    Inception(128, (128, 192), (32, 96), 64),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_4 = nn.Sequential()\n",
    "block_4.add(\n",
    "    Inception(192, (96, 208), (16, 48), 64),\n",
    "    Inception(160, (112, 224), (24, 64), 64),\n",
    "    Inception(128, (128, 256), (24, 64), 64),\n",
    "    Inception(112, (144, 288), (32, 64), 64),\n",
    "    Inception(256, (160, 320), (32, 128), 128),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_5 = nn.Sequential()\n",
    "block_5.add(\n",
    "    Inception(256, (160, 320), (32, 128), 128),\n",
    "    Inception(384, (192, 384), (48, 128), 128),\n",
    "    nn.GlobalAvgPool2D()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_net = nn.Sequential()\n",
    "google_net.add(\n",
    "    block_1,\n",
    "    block_2,\n",
    "    block_3,\n",
    "    block_4,\n",
    "    block_5,\n",
    "    nn.Dense(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential26 output shape:\t (1, 64, 24, 24)\n",
      "sequential27 output shape:\t (1, 192, 12, 12)\n",
      "sequential28 output shape:\t (1, 480, 6, 6)\n",
      "sequential29 output shape:\t (1, 832, 3, 3)\n",
      "sequential30 output shape:\t (1, 1024, 1, 1)\n",
      "dense14 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 96, 96))\n",
    "google_net.initialize(force_reinit=True)\n",
    "for layer in google_net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不敢执行, 悲哀\n",
    "lr, num_epochs, batch_size = 0.1, 5, 128\n",
    "google_net.initialize(force_reinit=True, ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(google_net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)\n",
    "train_cnn(net, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch normalization\n",
    "# 批量归一化 的 backward怎么算\n",
    "# BN from scratch, without mxnet BacthNorm Class\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    \"\"\"\n",
    "    通过autograd来判断当前模式是训练模式还是预测模式\n",
    "    在预测模式下, 使用移动平均和 移动方差, 保证模型对于任意输入都有稳定的输出\n",
    "    :param X: \n",
    "    :param gamma: 拉伸系数\n",
    "    :param beta: 偏移系数\n",
    "    :param moving_mean: 移动平均, 估算整个训练数据集的样本均值\n",
    "    :param moving_var: 移动方差\n",
    "    :param eps: 小数, 避免分母为0\n",
    "    :parma momentum: \n",
    "    \"\"\"\n",
    "    if not autograd.is_training():\n",
    "        # 在 预测 测试集时(预测模式), 直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / nd.sqrt(moving_var + eps)\n",
    "        return gamma * X_hat, moving_mean, moving_var   # 拉伸和偏移\n",
    "    if len(X.shape) == 2:\n",
    "        # 全连接层\n",
    "        mean = X.mean(axis=0)  # 列\n",
    "        var = ((X - mean) ** 2).mean(axis=0)\n",
    "    elif len(X.shape) == 4:\n",
    "        mean = X.mean(axis=(0,2,3), keepdims=True)\n",
    "        var = ((X - mean) ** 2).mean(axis=(0,2,3), keepdims=True)\n",
    "    else:\n",
    "        raise Exception(\"Unkown X shape: {}\".format(X.shape))\n",
    "    X_hat = (X - mean) / nd.sqrt(var + eps)\n",
    "    # 更新移动平均的均值和方差做标准化\n",
    "    moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "    moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    return gamma * X_hat, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Block):\n",
    "    def __init__(self, num_features, num_dims, **kwargs):\n",
    "        super(BatchNorm, self).__init__(**kwargs)\n",
    "        if num_dims == 2:  # 全连接层\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            # 卷积层\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0\n",
    "        self.gamma = self.params.get('gamma', shape=shape, init=init.One())\n",
    "        self.beta = self.params.get('beta', shape=shape, init=init.Zero())\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = nd.zeros(shape)\n",
    "        self.moving_var = nd.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.context != X.context:\n",
    "            self.moving_mean = self.moving_mean.copyto(X.context)\n",
    "            self.moving_var = self.moving_var.copyto(X.context)\n",
    "        # 保存更新过的moving_mean和moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma.data(), self.beta.data(), self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_with_bn = nn.Sequential()\n",
    "lenet_with_bn.add(\n",
    "    # 卷积层\n",
    "    nn.Conv2D(6, kernel_size=5),\n",
    "    BatchNorm(6, num_dims=4),  # 也可以使用 nn.BatchNorm()\n",
    "    nn.Activation('sigmoid'),\n",
    "    nn.MaxPool2D(pool_size=2, strides=2),\n",
    "    \n",
    "    nn.Conv2D(16, kernel_size=5),\n",
    "    BatchNorm(16, num_dims=4),\n",
    "    nn.Activation('sigmoid'),\n",
    "    nn.MaxPool2D(pool_size=2, strides=2),\n",
    "    # 全连接层\n",
    "    nn.Dense(120),\n",
    "    BatchNorm(120, num_dims=2),\n",
    "    nn.Activation('sigmoid'),\n",
    "    nn.Dense(84),\n",
    "    BatchNorm(84, num_dims=2),\n",
    "    nn.Activation('sigmoid'),\n",
    "    nn.Dense(10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 1.0, 5, 256\n",
    "net.initialize(ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "train_cnn(lenet_with_bn, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Block):\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,\n",
    "                               strides=strides)\n",
    "        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\n",
    "        self.conv3 = None\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2D(num_channels, kernel_size=1, strides=strides)\n",
    "        self.bn1 = nn.BatchNorm()\n",
    "        self.bn2 = nn.BatchNorm()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = nd.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return nd.relu(Y + X)\n",
    "\n",
    "    \n",
    "def resnet_block(num_channels, num_residuals, first_block=False):\n",
    "    blk = nn.Sequential()\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.add(Residual(num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_block = Residual(3)\n",
    "res_block.initialize()\n",
    "X = nd.random.uniform(shape=(4, 3, 6, 6))\n",
    "res_block(X).shape == X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6, 3, 3)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 增加输出通道数, 并且减半宽和高, 使用1x1卷积层resize\n",
    "res_block = Residual(6, use_1x1conv=True, strides=2)\n",
    "res_block.initialize()\n",
    "res_block(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet模型\n",
    "res_net = nn.Sequential()\n",
    "res_net.add(\n",
    "    nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n",
    "    nn.BatchNorm(), \n",
    "    nn.Activation('relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1),\n",
    "    resnet_block(64, 2, first_block=True),\n",
    "    resnet_block(128, 2),\n",
    "    resnet_block(256, 2),\n",
    "    resnet_block(512, 2),\n",
    "    nn.GlobalAvgPool2D(),  # 全局平均层\n",
    "    nn.Dense(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.05, 5, 256\n",
    "res_net.initialize(force_reinit=True, ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(res_net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "train_cnn(res_net, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 稠密连接网络\n",
    "def conv_block(num_channels):\n",
    "    \"\"\"\n",
    "    改良版的 \"卷积 + BN + 激活\"\n",
    "    \"\"\"\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.BatchNorm(), nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=3, padding=1))\n",
    "    return blk\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Block):\n",
    "    \"\"\"\n",
    "    稠密块\n",
    "    \"\"\"\n",
    "    def __init__(self, num_convs, num_channels, **kwargs):\n",
    "        super(DenseBlock, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "        for _ in range(num_convs):\n",
    "            self.net.add(conv_block(num_channels))\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            X = nd.concat(X, Y, dim=1)  # 在通道维上将输入和输出连结\n",
    "        return X\n",
    "\n",
    "    \n",
    "def transition_block(num_channels):\n",
    "    \"\"\"\n",
    "    过渡层\n",
    "    \"\"\"\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.BatchNorm(), nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1),\n",
    "            nn.AvgPool2D(pool_size=2, strides=2))\n",
    "    return blk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 23, 8, 8)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_block = DenseBlock(2, 10)\n",
    "dense_block.initialize()\n",
    "X = nd.random.uniform(shape=(4, 3, 8, 8))\n",
    "Y = dense_block(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10, 4, 4)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tran_block = transition_block(10)\n",
    "tran_block.initialize()\n",
    "tran_block(Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels, growth_rate = 64, 32  # num_channels为当前的通道数\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "\n",
    "dense_net = nn.Sequential()\n",
    "dense_net.add(\n",
    "    nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n",
    "    nn.BatchNorm(), \n",
    "    nn.Activation('relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    dense_net.add(DenseBlock(num_convs, growth_rate))\n",
    "    # 上一个稠密块的输出通道数\n",
    "    num_channels += num_convs * growth_rate\n",
    "    # 在稠密块之间加入通道数减半的过渡层\n",
    "    if i != len(num_convs_in_dense_blocks) - 1:\n",
    "        num_channels //= 2\n",
    "        dense_net.add(transition_block(num_channels))\n",
    "dense_net.add(\n",
    "    nn.BatchNorm(), \n",
    "    nn.Activation('relu'), \n",
    "    nn.GlobalAvgPool2D(),\n",
    "    nn.Dense(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 5, 256\n",
    "dense_net.initialize(ctx=context, init=init.Xavier())\n",
    "trainer = gluon.Trainer(dense_net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)\n",
    "train_cnn(dense_net, train_iter, test_iter, batch_size, trainer, context, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习\n",
    "1. 尝试基于LeNet构造更复杂的网络来提高分类准确率:\n",
    "    - 调整卷积窗口大小\n",
    "    - 输出通道数\n",
    "    - 激活函数和全连接层输出个数\n",
    "    - 在优化方面:\n",
    "        - 尝试不同的学习率、\n",
    "        - 初始化方法\n",
    "        - 增加迭代周期\n",
    "2. 尝试增加迭代周期。跟LeNet的结果相比，AlexNet的结果有什么区别？为什么？\n",
    "    - Mac跑不动\n",
    "3. AlexNet对Fashion-MNIST数据集来说可能过于复杂。试着简化模型来使训练更快，同时保证准确率不明显下降\n",
    "4. 修改批量大小, 观察准确率和内存或显存的变化\n",
    "5. 为什么NiN块里要有两个 1×1 卷积层？去除其中的一个，观察并分析实验现象\n",
    "6. 对比AlexNet、VGG和NiN、GoogLeNet的模型参数尺寸。为什么后两个网络可以显著减小模型参数尺寸？\n",
    "7. GoogLeNet有数个后续版本。尝试实现并运行它们，然后观察实验结果。后续版本:\n",
    "    - 加入批量归一化层\n",
    "    - 对Inception块做调整\n",
    "    - 加入残差连接\n",
    "8. 能否将批量归一化前的 全连接层/卷积层中的bias参数去掉? 为什么?\n",
    "    - 去掉bias, 会导致 均值减少bias, 方差不变\n",
    "9. 和LeNet 相比, 加了BN是不是可以调大learning rate?\n",
    "10. 尝试将BN层加到LeNet的其他地方, 看看有什么变化\n",
    "11. 如果不学习拉伸参数和偏移参数, 传参`grad_req='null'`可以避免计算梯度, 结果有什么变化?\n",
    "12. 如何在训练时使用基于全局平均的均值和方差?\n",
    "13. 对于比较深的网络， ResNet论文中介绍了一个“瓶颈”架构来降低模型复杂度。尝试实现它.\n",
    "14. 在ResNet的后续版本里，作者将残差块里的“卷积、批量归一化和激活”结构改成了“批量归一化、激活和卷积”，实现这个改进.\n",
    "15. DenseNet论文中提到的一个优点是模型参数比ResNet的更小，这是为什么 ?\n",
    "16. DenseNet被人诟病的一个问题是内存或显存消耗过多。真的会这样吗？可以把输入形状换成 224×224 ，来看看实际的消耗。\n",
    "17. 实现DenseNet论文中的表1提出的不同版本的DenseNet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
