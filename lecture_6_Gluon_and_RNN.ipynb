{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Gluon 实现RNN, 创作歌词\n",
    "### 1. 需要 想想怎么在 Colab 解决 代码复用的问题\n",
    "- mount google drive\n",
    "- clone the repo or change to the repo\n",
    "- pull code\n",
    "- open the ipynb file \n",
    "- run it\n",
    "\n",
    "### 2. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备 Google Colab 环境: 在Runtime中选择 GPU\n",
    "# 拉取数据集\n",
    "pwd = !pwd\n",
    "if 'dive_into_deep_learning' not in pwd[0]:\n",
    "    ! git clone https://github.com/chibinjiang/dive_into_deep_learning.git\n",
    "    # 进入到和开发环境相似的工作目录\n",
    "%cd /content/dive_into_deep_learning/\n",
    "# 安装依赖\n",
    "! pip install mxnet-cu101mkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import time\n",
    "import zipfile\n",
    "import traceback\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "with zipfile.ZipFile('DataResources/Chapter_6/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')\n",
    "corpus_chars = re.sub(r'\\s+', ' ', corpus_chars)\n",
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 RNN 模型\n",
    "def gen_net(num_hiddens, batch_size):\n",
    "    _rnn = rnn.RNN(num_hiddens)\n",
    "    _rnn.initialize()\n",
    "    _rnn.begin_state(batch_size=batch_size)\n",
    "    return _rnn\n",
    "    \n",
    "class RNN(nn.Block):\n",
    "    def __init__(self, num_hiddens, batch_size, vocab_size, **kwargs):\n",
    "        super(RNN, self).__init__(**kwargs)\n",
    "        self.rnn = gen_net(num_hiddens, batch_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dense = nn.Dense(vocab_size)  # 为什么需要一个全连接层\n",
    "    \n",
    "    def forward(self, inputs, state):\n",
    "        # 将输入转置成(num_steps, batch_size)后获取one-hot向量表示\n",
    "        X = nd.one_hot(inputs.T, self.vocab_size)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层会首先将Y的形状变成(num_steps * batch_size, num_hiddens)，它的输出\n",
    "        # 形状为(num_steps * batch_size, vocab_size)\n",
    "        output = self.dense(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_net = gen_net(100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 2, 100), [\n",
       "  [[[1.5278673  0.         1.0445228  0.         0.         1.6484566\n",
       "     1.8354766  0.         0.         0.         1.1338558  0.\n",
       "     0.42351055 0.         0.         0.         0.         1.3539939\n",
       "     0.31653637 0.         0.         0.4922253  0.         1.4564292\n",
       "     1.1331655  0.08769092 0.2364367  0.         0.5829359  3.5316272\n",
       "     0.         0.         0.08696572 0.         1.5529106  0.6949022\n",
       "     0.         0.57017994 0.         0.         0.19693512 0.\n",
       "     0.         0.         0.08501232 1.3938953  0.         0.5927223\n",
       "     0.         0.         0.         1.7381938  0.7154095  0.3993885\n",
       "     0.         0.         0.5789365  0.16283596 0.         0.\n",
       "     4.122617   0.         0.         0.7438283  0.         0.9080451\n",
       "     1.1469073  0.66469157 0.00958797 0.         0.7123618  0.\n",
       "     0.67446774 0.         0.08351076 2.5886292  0.         1.9853334\n",
       "     1.7640082  0.         0.15815529 0.22243357 0.         0.\n",
       "     0.7966601  0.         1.4879034  0.         0.43341446 0.\n",
       "     0.85194176 1.5169411  0.         0.4188936  0.         1.4793936\n",
       "     0.11655658 0.         0.         0.        ]\n",
       "    [0.49272102 0.         1.3639603  0.         0.13596529 0.85960716\n",
       "     1.7749538  0.         0.         0.         0.         0.\n",
       "     1.1927054  1.3133019  0.         0.         0.8974129  1.3199869\n",
       "     0.3085855  0.         0.         0.03820897 1.1344846  0.38984716\n",
       "     0.94526803 0.7019026  0.         0.         0.44088465 2.4963117\n",
       "     0.         0.         0.         0.         0.86436504 0.40091437\n",
       "     0.52114135 1.4102024  0.         0.         0.         0.\n",
       "     0.         0.         0.1185559  1.2798615  0.         0.15354332\n",
       "     0.         0.         0.         0.9478243  1.6855005  0.8527158\n",
       "     0.3331138  0.         0.5354203  0.20027965 0.         0.\n",
       "     1.9365169  0.         1.1820314  1.1050293  0.         0.5274993\n",
       "     0.         1.3106338  0.15993264 0.         0.82656825 0.\n",
       "     0.6445401  0.         0.         1.923662   0.         1.6844945\n",
       "     0.20763232 0.         0.04808267 0.38708332 0.1509037  0.\n",
       "     0.6003119  0.         2.3726835  0.         0.44051892 0.\n",
       "     0.         1.9628005  0.13584477 0.         0.         1.9289321\n",
       "     0.         0.         0.         0.        ]]]\n",
       "  <NDArray 1x2x100 @cpu(0)>])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_state = temp_net.begin_state(batch_size=2)\n",
    "X = nd.random.uniform(shape=(30, 2, vocab_size))\n",
    "Y, state_new = temp_net(X, init_state)\n",
    "(Y.shape, state_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn_gluon(prefix, num_chars, model, vocab_size, ctx, idx_to_char, char_to_idx):\n",
    "    \"\"\"\n",
    "    预测prefix 之后的歌词\n",
    "    \"\"\"\n",
    "    state = model.begin_state(batch_size=1, ctx=ctx)  # 使用model的成员函数来初始化隐藏状态\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        X = nd.array([output[-1]], ctx=ctx).reshape((1, 1))\n",
    "        (Y, state) = model(X, state)  # 前向计算不需要传入模型参数\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y.argmax(axis=1).asscalar()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(gpu_number=0):\n",
    "    \"\"\"\n",
    "        Return gpu(i) if exists, otherwise return cpu().\n",
    "    \"\"\"\n",
    "#     import traceback\n",
    "    try:\n",
    "        _ = mx.nd.array([1, 2, 3], ctx=mx.gpu(gpu_number))\n",
    "        print(\"Try GPU: {}\".format(gpu_number))\n",
    "    except mx.MXNetError:\n",
    "#         traceback.print_exc()\n",
    "        print(\"Try CPU: {}\".format(gpu_number))\n",
    "        return mx.cpu()\n",
    "    return mx.gpu(gpu_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try CPU: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'分开贪期隧砍術演墨币样阵'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = try_gpu()\n",
    "model = RNN(256, 2, vocab_size)\n",
    "model.initialize(force_reinit=True, ctx=ctx)\n",
    "predict_rnn_gluon('分开', 10, model, vocab_size, ctx, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    \"\"\"\n",
    "    相邻采样: 相邻 epoch 的 batch_size 样本是相邻的\n",
    "    Sample mini-batches in a consecutive order from sequential data.\n",
    "    \"\"\"\n",
    "    corpus_indices = nd.array(corpus_indices, ctx=ctx)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    indices = corpus_indices[0 : batch_size * batch_len].reshape((\n",
    "        batch_size, batch_len))  # 只要 前面的batch_size * batch_len 个 \n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i : i + num_steps]\n",
    "        Y = indices[:, i + 1 : i + num_steps + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剪裁梯度\n",
    "def grad_clipping(params, theta, ctx):\n",
    "    norm = nd.array([0], ctx)\n",
    "    for param in params:\n",
    "        norm += (param.grad ** 2).sum()\n",
    "    norm = norm.sqrt().asscalar()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,\n",
    "                                corpus_indices, idx_to_char, char_to_idx,\n",
    "                                num_epochs, num_steps, lr, clipping_theta,\n",
    "                                batch_size, pred_period, pred_len, prefixes):\n",
    "    perplexity_hist = list()\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    model.initialize(ctx=ctx, force_reinit=True, init=init.Normal(0.01))\n",
    "    trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                            {'learning_rate': lr, 'momentum': 0, 'wd': 0})\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_consecutive(\n",
    "            corpus_indices, batch_size, num_steps, ctx)\n",
    "        state = model.begin_state(batch_size=batch_size, ctx=ctx)\n",
    "        for X, Y in data_iter:\n",
    "            for s in state:\n",
    "                s.detach()\n",
    "            with autograd.record():\n",
    "                (output, state) = model(X, state)\n",
    "                y = Y.T.reshape((-1,))\n",
    "                l = loss(output, y).mean()\n",
    "            l.backward()\n",
    "            # 梯度裁剪\n",
    "            params = [p.data() for p in model.collect_params().values()]\n",
    "            grad_clipping(params, clipping_theta, ctx)\n",
    "            trainer.step(1)  # 因为已经误差取过均值，梯度不用再做平均\n",
    "            l_sum += l.asscalar() * y.size\n",
    "            n += y.size\n",
    "        perplexity = math.exp(l_sum / n)\n",
    "        perplexity_hist.append(perplexity)\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (epoch + 1, perplexity, time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn_gluon(prefix, pred_len, model, vocab_size, ctx, idx_to_char, char_to_idx))\n",
    "    return perplexity_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, perplexity 293.331648, time 8.80 sec\n",
      " - 分开 我们 我的 我 我的 我 我的 我 我的 我 我的 我 我的 我 我的 我 我的 我 我的 我 我\n",
      " - 不分开 我们的 我 我的 我 我的 我 我的 我 我的 我 我的 我 我的 我 我的 我 我的 我 我的 \n",
      " - 我静静地 我们 我的 我 我的 我 我的 我 我的 我 我的 我 我的 我 我的 我 我的 我 我的 我 我\n",
      "epoch 20, perplexity 166.335066, time 8.72 sec\n",
      " - 分开 我们的感 在一种 你的世界 在空的 我有不着 你的世界 你的爱界 你的爱界 你的爱界 你的爱界 你\n",
      " - 不分开 我们不起 你的世界 在空的 我有不着 你的世界 你的爱界 你的爱界 你的爱界 你的爱界 你的爱界 \n",
      " - 我静静地 你 不起 你的世界 你的爱界 你的爱界 你的爱界 你的爱界 你的爱界 你的爱界 你的爱界 你的爱界\n",
      "epoch 30, perplexity 93.325684, time 8.76 sec\n",
      " - 分开 你说不起 让我们乘着阳光 我用度坚尝 当敌 是谁 我用 不到 不了麻烦了 不用麻烦了 不用麻烦了 \n",
      " - 不分开 我要不到 你没有一人 我们势如龙 当敌人是空 我目势如龙 当敌人是空 我目势如龙 当敌人是空 我目\n",
      " - 我静静地 她不用 让我用 你不需 不要 不了 不需 不了 不需 不了 不需 不了 不需 不了 不需 不了 不\n",
      "epoch 40, perplexity 55.906913, time 8.77 sec\n",
      " - 分开 我们的时间 当目标的空寸 原来的很 你让我们热 每一天的 漫 一直 在谁在 的灵魂 我们 有你的 \n",
      " - 不分开 我知道如穷 当敌人是空 我左右如龙 当敌人尽空 不用法红豆 不用麻烦了 不用麻烦了 不用麻烦了 不\n",
      " - 我静静地 你就是虽 在飞了 的灵情 单纯一停 等一直人 我等一起热 让我们乘着阳光 看着远方 去支耿心 汹不\n",
      "epoch 50, perplexity 37.384783, time 9.95 sec\n",
      " - 分开 我们的时情 全在一遍秀 让我们 半兽人 的灵魂 我们无整恼 我目光 追绪 天 这些 Y飘 H硬 得\n",
      " - 不分开 她手的影匙 我们中个世色 未哼哈壁 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼的客栈 滴答中人好 \n",
      " - 我静静地 只在用琵琶弹奏 我说你的爱写在西元前 深埋你也手 有一条戏 生 哎 对a 我用一笔 我永我的手气 \n",
      "epoch 60, perplexity 27.176030, time 9.86 sec\n",
      " - 分开 我们 你不再 我手笑你的微笑每天都能看东 我想哑的情绪 我个你的指尖 缺席的答周 我在等待 在小村\n",
      " - 不分开 她手的路时 我在等待 在小村外的溪边 在我地盘这阳光 看着你方之不是 昨晚的方模 我想同个的色 是\n",
      " - 我静静地 你就用浏下的是雕 很我个略变 有世外 我却好 永干的 幸时呢 我错了 不是问来 那时光 让我不见那\n",
      "epoch 70, perplexity 21.351118, time 9.19 sec\n",
      " - 分开 我们 冲你 强字 我不能倒再 将炮 时空 我只能倒球 我目光如龙 当单人是空 我左右无弓 我攻势时\n",
      " - 不分开 戴手淋湿 让一代残 让一百热 动成牵 的日出调 你一转 重红堂 你哭开 面儿啦 啦儿啦 啦儿啦 啦\n",
      " - 我静静地 你就用虽我 受仔让没有 我马光如龙 当敌人是空 我战右无穷 我攻势时虹 将炮车下宫 再著 我一定字\n",
      "epoch 80, perplexity 17.652715, time 9.80 sec\n",
      " - 分开 我们的感动蔓延 安态与龙 那支地心 我不要 这样一口 你的都音 用南上间 我的感觉 你已听离见 你\n",
      " - 不分开 其我的Blu厚s状 燃烧的年的 不用麻烦了 你们一起世 我等着无龙 当敌人是空 接著的下過 我在等\n",
      " - 我静静地 你不用感意 我的乐器 被这太白 我等多 永不再化 因没生谣 我用一身世 将甚的思绪 全杂的手泪 看\n",
      "epoch 90, perplexity 15.090050, time 8.72 sec\n",
      " - 分开 我们 那些被敲长 我用前如龙 当敌势尽紧 接临城下 再逃都型 我不要等 释住感觉 我用一起世 拋甚\n",
      " - 不分开 寒手不用 你了 闭中 不同 他染 不同 烦了 不同麻烦了 不用麻烦了 不用麻烦出 不用麻烦热 不用\n",
      " - 我静静地 你在用窗还在风空感 我会不该尊 美笑里 等地那 你儿不公 没笑没有 我用第如力 拋闭远尽宫 兵临城\n",
      "epoch 100, perplexity 13.054898, time 9.35 sec\n",
      " - 分开 你的身子 微南极的地 将爱的红過 全个的风好 你的指美主义 太彻的消朵 升空着人在 情不该 你随生\n",
      " - 不分开 戴手的珊 再后一遍热粥 配上的客栈 滴在伤味 还生上 对子不回累 不要用露出胸膛 流一点汗 的答黄\n",
      " - 我静静地 你在那窗外来 缘身的客栈人多 牧草有间秀 我在等界 在每里外 等一直刮 页第回起 伤争忘 永吾没人\n",
      "epoch 110, perplexity 11.823232, time 9.32 sec\n",
      " - 分开 我用你的回色 未来的答干 快纸 我一路字何 我马儿如风 我一势如穷 将伤标是无季 我用眼光去 你无\n",
      " - 不分开 没有对 一句走 你想就有谁手 我加眼光怎么 竟会见你的牛爱在 檐麦 有飞完 我们在任腰 查化 我用\n",
      " - 我静静地 你才把浏览像 你看远退不住 蛋空眼睛 还天看见 我可以 忘不融 谁让我的地球变暗 街角的消防栓 老\n",
      "epoch 120, perplexity 10.555543, time 9.31 sec\n",
      " - 分开 如果 你的回刚 不该得别了 副歌 不要再 听我的我 出櫛最练一个忧我 释下的 被时光 我失还有敌样\n",
      " - 不分开 狼有了风 我也是那条龙 天事的影水 滴个的答祷 我爱好的特色 未来难预测 坚持了阻碍 情在这天池 \n",
      " - 我静静地 你才猜浏览 那脸术方 雷一百 的回出调整下时空 回到洪柔 去支配心操汹 我右拳打开了天 化身为龙 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-fd0a073c9b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mcorpus_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                             \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipping_theta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                             batch_size, pred_period, pred_len, prefixes)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-8eaa7d92028a>\u001b[0m in \u001b[0;36mtrain_and_predict_rnn_gluon\u001b[0;34m(model, num_hiddens, vocab_size, ctx, corpus_indices, idx_to_char, char_to_idx, num_epochs, num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# 梯度裁剪\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mgrad_clipping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipping_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 因为已经误差取过均值，梯度不用再做平均\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0ml_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-672af7c19696>\u001b[0m in \u001b[0;36mgrad_clipping\u001b[0;34m(params, theta, ctx)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mnorm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Learning/EnvLearning/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2034\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Learning/EnvLearning/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2014\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   2017\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_hiddens, num_steps = 256, 35\n",
    "num_epochs, batch_size, lr, clipping_theta = 300, 32, 100, 0.01\n",
    "pred_period, pred_len, prefixes = 10, 50, ['分开', '不分开', '我静静地']\n",
    "perplexities = train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,\n",
    "                            corpus_indices, idx_to_char, char_to_idx,\n",
    "                            num_epochs, num_steps, lr, clipping_theta,\n",
    "                            batch_size, pred_period, pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens, num_steps = 256, 35\n",
    "num_epochs, batch_size, lr, clipping_theta = 300, 32, 100, 0.01\n",
    "pred_period, pred_len, prefixes = 10, 50, ['分开', '不分开', '我静静地']\n",
    "perplexities = train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,\n",
    "                            corpus_indices, idx_to_char, char_to_idx,\n",
    "                            num_epochs, num_steps, lr, clipping_theta,\n",
    "                            batch_size, pred_period, pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
